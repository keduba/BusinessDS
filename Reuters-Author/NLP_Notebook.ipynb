{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text mining (nlp) with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Ties de Kok   ([Personal Website](https://www.tiesdekok.com))   <br>\n",
    "**Last updated:** June 2020  \n",
    "**Python version:** Python 3.7  \n",
    "**License:** MIT License  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Some features (like the ToC) will only work if you run the notebook or if you use nbviewer by clicking this link:  \n",
    "https://nbviewer.jupyter.org/github/TiesdeKok/Python_NLP_Tutorial/blob/master/NLP_Notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code examples to get you started with Python for Natural Language Processing (NLP) / Text Mining.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n",
    "\n",
    "This notebook only discusses step 3 and 4. If you want to learn more about step 2 see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: companion slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was designed to accompany a PhD course session on NLP techniques in Accounting Research.  \n",
    "The slides of this session are publically availabe here: [Slides](http://www.tiesdekok.com/AccountingNLP_Slides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Elements / topics that are discussed in this notebook:*\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Table of Contents*  <a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem) \n",
    "    * [Language modeling](#lang_model) \n",
    "        * [Part-of-Speech tagging](#pos_tagging) \n",
    "        * [Uni-Gram & N-Grams](#n_grams) \n",
    "        * [Stop words](#stop_words) \n",
    "* [Direct feature extraction](#feature_extract) \n",
    "    * [Feature search](#feature_search) \n",
    "        * [Entity recognition](#entity_recognition) \n",
    "        * [Pattern search](#pattern_search) \n",
    "    * [Text evaluation](#text_eval) \n",
    "        * [Language](#language) \n",
    "        * [Dictionary counting](#dict_counting) \n",
    "        * [Readability](#readability) \n",
    "* [Represent text numerically](#text_numerical) \n",
    "    * [Bag of Words](#bows) \n",
    "        * [TF-IDF](#tfidf) \n",
    "    * [Word Embeddings](#word_embed) \n",
    "        * [Spacy](#spacyEmbedding)\n",
    "        * [Word2Vec](#Word2Vec) \n",
    "* [Statistical models](#stat_models) \n",
    "    * [\"Traditional\" machine learning](#trad_ml) \n",
    "        * [Supervised](#trad_ml_supervised) \n",
    "            * [Naïve Bayes](#trad_ml_supervised_nb) \n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm) \n",
    "        * [Unsupervised](#trad_ml_unsupervised) \n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda) \n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis) \n",
    "* [Model Selection and Evaluation](#trad_ml_eval) \n",
    "* [Neural Networks](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` \n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `Gensim` for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments. \n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* for `tqdm` to work in JupyterLab you need to install the `@jupyter-widgets/jupyterlab-manager` using the puzzle icon in the left side bar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download and extract the zip file with the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the data into memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test'}\n",
    "text_dict = {'test' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2a58d118cd439692a0ce871b853852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label, folder in tqdm(folder_dict.items()):\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\\n Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\\n Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group\\'s shares up 11 pence to 248-1/2p by 1415 gmt.\\n \"It\\'s all quite encouraging. The way they are analysing and managing the business is very much more in line with what the market demands,\" said Richard Workman, an analyst at ABN-AMRO Hoare Govett.\\n The company said its recovery was strongly led by its British operations, while in mainland Europe, it started to realise the potential of businesses acquired earlier in the 1990\\'s.\\n UB\\'s British business increased profits before exceptional items by 12 percent to 113.7 million pounds as renewed consumer confidence encouraged shoppers to step up brands.\\n In an interview UB\\'s managing director Eric Nicoli said UB Has now entered a phase of consolidation after a hectic disposal programme.\\n \"The emphasis is on organic growth as we are now in a consolidation phase,\" said Nicoli.\\n Over the last two years UB has sold its American snack operation Keebler, pulled out of Portugal and exited from Spanish snacks. It has also sold one of its Italian snack businesses, withdrawn from Turkey and Brazil and sold Ross Vegetables Products business in Britain.\\n UB expects its margins in Europe to remain firm and to move up over the next two to three years, said Nicoli.\\n \"We expect margins to move to 10 percent in all our businesses over a two or three year period,\" he said.\\n (Corrects to make clear that company isn\\'t expecting European markets alone to move by 10 percent.)\\n UB margins are currently at around 10 percent in Britain and four percent in mainland Europe.\\n In 1997 the company plans to build on the turnaround \"all our markets continue to be very competitive but we expect another year of good progress,\" said Nicoli.\\n The total dividend payout was increased by 10 pence per share from 9.8p.\\n The company\\'s leading biscuit brands include McVities in Britain, Verkade in Holland and Oxford in Denmark. Biscuit sales growth came from the launch of Go Ahead! a range of low fat products. UB also owns snacks KP Nuts, Skips and Hula Hoops.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the text into a NLP representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** depending on the way that you installed the language models you will need to import it differently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from spacy.en import English\n",
    "nlp = English()\n",
    "```\n",
    "OR\n",
    "```\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `nlp.pipe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bcf54d3eaa47bcb8fcee23b8683476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in tqdm(text_dict['test'].items()):\n",
    "    spacy_text[author] = list(nlp.pipe(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on speed:*  This is slow because we didn't disable any compontents, see this note from the documentation:  \n",
    "> Only apply the pipeline components you need. Getting predictions from the model that you don’t actually need adds up and becomes very inefficient at scale. To prevent this, use the disable keyword argument to disable components you don’t need – either when loading a model, or during processing with nlp.pipe. See the section on disabling pipeline components for more details and examples. [link](https://spacy.io/usage/processing-pipelines#disabling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
    "\n",
    "This can imply many things, I will show a couple of options below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
      " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special characters can cause problems in our analyses (and can be hard to debug if you are using `print` statements to inspect the data).\n",
    "\n",
    "**So how do we remove them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts. Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, however, the problem arrises because of encoding / decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some π text that has to be cleaned…! it's difficult to deal with!\n",
      "b\"This is some  text that has to be cleaned! it's difficult to deal with!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\u03c0 text that has to be cleaned\\u2026! it\\u0027s difficult to deal with!'\n",
    "print(problem_sentence)\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative that is better at preserving the unicode characters would be to use `unidecode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王玉\n"
     ]
    }
   ],
   "source": [
    "print('\\u738b\\u7389')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wang Yu '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode.unidecode(u\"\\u738b\\u7389\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is some p text that has to be cleaned...! it's difficult to deal with!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unidecode.unidecode(problem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation refers to the task of splitting up the text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts\",\n",
       " '\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997',\n",
       " ' The shares fell 6p to 781p on the news',\n",
       " '\\n \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers',\n",
       " '  \\n Dermott Carr, an analyst at Nikko said, \"the mark']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain's Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\n",
       "  ,\n",
       " Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers Commission which is due to report before March 24, 1997.,\n",
       " The shares fell 6p to 781p on the news.\n",
       "  ,\n",
       " \"The stock is probably dead in the water until March,\" said John Wakley, analyst at Lehman Brothers.  \n",
       "  ,\n",
       " Dermott Carr, an analyst at Nikko said, \"the market is going to hang onto them for the moment]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned object is still a `spacy` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* `spacy` sentence segmentation relies on the text being capitalized, so make sure you didn't convert it to all lower case before running this operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to all texts (for use later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8469731143cc4184883d6ed6b3d25beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in tqdm(spacy_text.items()):\n",
    "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       "  ,\n",
       " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\n",
       "  ,\n",
       " Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\n",
       "  ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization means to split the sentence (or text) up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       " "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares,\n",
       " in,\n",
       " brewing,\n",
       " -,\n",
       " to,\n",
       " -,\n",
       " leisure,\n",
       " group,\n",
       " Bass,\n",
       " Plc,\n",
       " are,\n",
       " likely,\n",
       " to,\n",
       " be,\n",
       " held]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Space offers build-in functionality for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brewing',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisure',\n",
       " 'group',\n",
       " 'Bass',\n",
       " 'Plc',\n",
       " 'be',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'hold']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share',\n",
       " 'in',\n",
       " 'brew',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'leisur',\n",
       " 'group',\n",
       " 'bass',\n",
       " 'plc',\n",
       " 'are',\n",
       " 'like',\n",
       " 'to',\n",
       " 'be',\n",
       " 'held']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original  | Spacy Lemma  | NLTK Stemmer\n",
      "-----------------------------------------\n",
      "    Shares  |       share  |       share\n",
      "        in  |          in  |          in\n",
      "   brewing  |     brewing  |        brew\n",
      "         -  |           -  |           -\n",
      "        to  |          to  |          to\n",
      "         -  |           -  |           -\n",
      "   leisure  |     leisure  |      leisur\n",
      "     group  |       group  |       group\n",
      "      Bass  |        Bass  |        bass\n",
      "       Plc  |         Plc  |         plc\n",
      "       are  |          be  |         are\n",
      "    likely  |      likely  |        like\n",
      "        to  |          to  |          to\n",
      "        be  |          be  |          be\n",
      "      held  |        hold  |        held\n"
     ]
    }
   ],
   "source": [
    "print('  Original  | Spacy Lemma  | NLTK Stemmer')\n",
    "print('-' * 41)\n",
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(str(original).rjust(10, ' '), ' | ', str(lemma).rjust(10, ' '), ' | ', str(stem).rjust(10, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my experience it is usually best to use lemmatization instead of a stemmer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Shares, 'NOUN'),\n",
       " (in, 'ADP'),\n",
       " (brewing, 'NOUN'),\n",
       " (-, 'PUNCT'),\n",
       " (to, 'ADP'),\n",
       " (-, 'PUNCT'),\n",
       " (leisure, 'NOUN'),\n",
       " (group, 'NOUN'),\n",
       " (Bass, 'PROPN'),\n",
       " (Plc, 'PROPN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars  \n",
    "\n",
    "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are-likely', 'likely-to', 'to-be', 'be-held', 'held-back']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_without_punctuation(sen_obj):\n",
    "    return [token.text for token in sen_obj if token.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram(sen_obj, n, sep = '-'):\n",
    "    token_list = tokenize_without_punctuation(sen_obj)\n",
    "    number_of_tokens = len(token_list)\n",
    "    ngram_list = []\n",
    "    for i, token in enumerate(token_list[:-n+1]):\n",
    "        ngram_item = [token_list[i + ii] for ii in range(n)]\n",
    "        ngram_list.append(sep.join(ngram_item))\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United-Biscuits',\n",
       " 'Biscuits-Holdings',\n",
       " 'Holdings-Plc',\n",
       " 'Plc-more',\n",
       " 'more-than']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_ngram(example_sentence, 2)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shares-in-brewing',\n",
       " 'in-brewing-to',\n",
       " 'brewing-to-leisure',\n",
       " 'to-leisure-group',\n",
       " 'leisure-group-Bass']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_ngram(example_sentence, 3)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, -, -, leisure, group, Bass, Plc, likely, held]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, in, brewing, -, to, -, leisure, group, Bass, Plc]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note* we can also remove punctuation in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Shares, brewing, leisure, group, Bass, Plc, likely, held, Britain, Trade]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in example_sentence if not token.is_stop and token.is_alpha][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap everything into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic SpaCy text processing function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer, remove top words, remove punctuation\n",
    "3. Clean up the sentence using `textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_custom(text):\n",
    "    sentences = list(nlp(text, disable=['tagger', 'ner', 'entity_linker', 'textcat', 'entity_ruler']).sents)\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop and token.is_alpha])\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f2caac21dd4a08b916e38b4e3a6042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koollio/Projects/Python/BusinessDS/Reuters-Author/.venv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "spacy_text_clean = {}\n",
    "for author, text_list in tqdm(text_dict['test'].items()):\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        lst.append(process_text_custom(text))\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* that this would take quite a long time if we didn't disable some of the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 53986\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for author, texts in spacy_text_clean.items():\n",
    "    for text in texts:\n",
    "        count += len(text)\n",
    "print('Number of sentences:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united biscuits holdings plc doubled profits million pounds million tax exceptional items reflecting simpler slimmed portfolio products',\n",
       " 'united owns brands mcvities biscuits kp nuts exited keebler subsidiary said total exceptional charges mainly loss disposal businesses amounted million pounds compared million',\n",
       " 'sales rose percent billion trading profits grew percent million']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* the quality of the input text is not great, so the sentence segmentation is also not great (without further tweaking)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
    "\n",
    "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
    "\n",
    "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group's shares up 11 pence to 248-1/2p by 1415 gmt.\n",
       " "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11 pence, 'MONEY'), (248, 'CARDINAL'), (1415, 'DATE')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brewer to leisure group Whitbread Plc has turned in a \"sound\" business performance in the last three months, said chief executive Peter Jarvis in an interview on Friday.\n",
       " "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Whitbread Plc, 'ORG'),\n",
       " (the last three months, 'DATE'),\n",
       " (Peter Jarvis, 'PERSON'),\n",
       " (Friday, 'DATE')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-AZ\n",
      "663-BY\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company hotels excluding metropole london acquired november million stg million lonrho showed occupancy level percent weeks touch percent time\n",
      "group hotel occupancy pulled slightly period temporary closure stakis tyneside work underway million stg refurbishment\n",
      "turnover casinos rose percent million stg driven percent increase attendances\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    if re.search('million', sen, flags= re.IGNORECASE):\n",
    "        print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides feature search there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scottish based stakis plc wednesday reported surge visitors casinos sharp rise hotel room rates chief executive david michel confident mood future trends real terms room rates late room rates reached pre recession levels provinces michels told reuters company hotels excluding metropole london acquired november million stg million lonrho showed occupancy level percent weeks touch percent time average room rate rose stg period quarter percent think average percent year said michels forecast finish'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = ' '.join([x for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `spacy-langdetect` package it is easy to detect the language of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "detect() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from spacy_langdetect import LanguageDetector\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangdetect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m nlp.add_pipe(\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, name=\u001b[33m'\u001b[39m\u001b[33mlanguage_detector\u001b[39m\u001b[33m'\u001b[39m, last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: detect() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# from spacy_langdetect import LanguageDetector\n",
    "from langdetect import detect\n",
    "nlp.add_pipe(detect(), name='language_detector', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'language'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_paragraph\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlanguage\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/BusinessDS/Reuters-Author/.venv/lib/python3.12/site-packages/spacy/tokens/underscore.py:48\u001b[39m, in \u001b[36mUnderscore.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extensions:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors.E046.format(name=name))\n\u001b[32m     49\u001b[39m     default, method, getter, setter = \u001b[38;5;28mself\u001b[39m._extensions[name]\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: [E046] Can't retrieve unregistered extension attribute 'language'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "print(nlp(example_paragraph)._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally I'd recommend to calculate the readability metrics by yourself as they don't tend to be that difficult to compute. However, there are packages out there that can help, such as `spacy_readability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_readability import Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(Readability(), name='readability', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.412857142857145\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I am some really difficult text to read because I use obnoxiously large words.\")\n",
    "print(doc._.flesch_kincaid_grade_level)\n",
    "print(doc._.smog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual example:** FOG index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fog(document):\n",
    "    doc = nlp(document, disable=['tagger', 'ner', 'entity_linker', 'textcat', 'entity_ruler'])\n",
    "    sen_list = list(doc.sents)\n",
    "    num_sen = len(sen_list)\n",
    "\n",
    "    num_words = 0\n",
    "    num_complex_words = 0\n",
    "    for sen_obj in sen_list:\n",
    "        words_in_sen = [token.text for token in sen_obj if token.is_alpha]\n",
    "        num_words += len(words_in_sen)\n",
    "        num_complex  = 0\n",
    "        for word in words_in_sen:\n",
    "            num_syl = syllapy.count(word.lower())\n",
    "            if num_syl > 2:\n",
    "                num_complex += 1\n",
    "        num_complex_words += num_complex\n",
    "        \n",
    "    fog = 0.4 * ((num_words / num_sen) + ((num_complex_words / num_words)*100))\n",
    "    return {'fog' : fog, \n",
    "            'num_sen' : num_sen, \n",
    "            'num_words' : num_words, \n",
    "            'num_complex_words' : num_complex_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fog': 13.42327889849504,\n",
       " 'num_sen': 36,\n",
       " 'num_words': 347,\n",
       " 'num_complex_words': 83}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_fog(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `fuzzywuzzy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `spacy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy can provide a similary score based on the semantic similarity ([link](https://spacy.io/usage/vectors-similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000623731768"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1 = nlp(\"fuzzy wuzzy was a bear\")\n",
    "tokens_2 = nlp(\"wuzzy fuzzy was a bear\")\n",
    "\n",
    "tokens_1.similarity(tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8127869114665882"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_1 = nlp(\"Tom believes German cars are the best.\")\n",
    "tokens_2 = nlp(\"Sarah recently mentioned that she would like to go on holiday to Germany.\")\n",
    "\n",
    "tokens_1.similarity(tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common technique for basic NLP insights is to create simple metrics based on term counts. \n",
    "\n",
    "These are relatively easy to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 2\n",
      "first 0\n",
      "most 0\n",
      "be 7\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ['great', 'agree', 'increase']\n",
    "neg = ['bad', 'disagree', 'decrease']\n",
    "\n",
    "sentence = '''According to the president everything is great, great, \n",
    "and great even though some people might disagree with those statements.'''\n",
    "\n",
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)\n",
    "\n",
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)\n",
    "\n",
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the total number of words is also easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len([token for token in nlp(sentence) if token.is_alpha])\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'agree': 1, 'increase': 0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* `.lower()` is actually quite slow, if you have a lot of words / sentences it is recommend to minimize the amount of `.lower()` operations that you have to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "*Note 1:* these functions also provide a variety of built-in preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
    "\n",
    "*Note 2:* example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = \"The sky is blue.\"\n",
    "doc_2 = \"The sun is bright today.\"\n",
    "doc_3 = \"The sun in the sky is bright.\"\n",
    "doc_4 = \"We can see the shining sun, the bright sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue' 'bright' 'shining' 'sky' 'sun' 'today'] \n",
      "\n",
      "[1 0 0 1 0 0]\n",
      "[0 1 0 0 1 1]\n",
      "[0 1 0 1 1 0]\n",
      "[0 1 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out(), '\\n')\n",
    "for doc_tf_vector in tf.toarray():\n",
    "    print(doc_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
      "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
      "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
      "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 27743\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x21978 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 410121 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Spacy</span><a id='spacyEmbedding'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `en_core_web_lg` language model comes with GloVe vectors trained on the Common Crawl dataset ([link](https://spacy.io/models/en#en_core_web_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True 7.2950425 True\n",
      "Dutch True 7.227342 True\n",
      "word True 5.537156 True\n",
      "for True 7.144757 True\n",
      "peanut True 6.721452 True\n",
      "butter True 5.5611167 True\n",
      "is True 6.0367823 True\n",
      "pindakaas True 6.0864863 True\n",
      "did True 7.466853 True\n",
      "you True 8.403958 True\n",
      "know True 6.753204 True\n",
      "that True 7.6419835 True\n",
      "This True 7.5847964 True\n",
      "is True 6.569338 True\n",
      "a True 7.379784 True\n",
      "typpo True 6.361052 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(\"The Dutch word for peanut butter is 'pindakaas', did you know that? This is a typpo.\")\n",
    "\n",
    "for token in tokens:\n",
    "    if token.is_alpha:\n",
    "        print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token: \"Car\" has the following vector (dimension: 96)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.3298006 , -1.0853987 ,  0.7887242 ,  0.56536686, -0.03679049,\n",
       "        0.17918521,  1.0721457 ,  0.4140611 , -0.45138526, -0.4259355 ,\n",
       "       -0.32376927, -0.24423355, -1.1884332 ,  0.38261512,  0.15325083,\n",
       "        0.888623  , -1.0993898 , -0.36031944, -0.0155575 , -0.48386702,\n",
       "       -0.65094745,  1.1043994 , -1.2378284 ,  0.16953191, -0.19734624,\n",
       "       -0.11411573,  0.655138  ,  0.71806145,  0.1673866 ,  1.1834166 ,\n",
       "       -0.5743222 ,  1.0206122 ,  0.2183578 , -0.8829129 , -0.37797755,\n",
       "       -0.8775984 , -0.8520317 ,  0.5326886 ,  0.44445798, -0.02371767,\n",
       "       -0.45813775,  0.1717524 ,  0.3198011 ,  0.56773376,  0.15410456,\n",
       "       -0.26940504, -1.2045121 , -1.0995429 ,  0.20882471, -0.5321012 ,\n",
       "        0.33936197,  0.8772712 ,  0.7056221 , -0.4283748 ,  0.673675  ,\n",
       "       -1.0647851 ,  0.76150036, -0.8680595 , -0.11669695, -0.03319389,\n",
       "       -1.2372603 ,  0.29322624,  0.12529306, -0.27613178,  0.4557415 ,\n",
       "       -0.4610034 , -0.09175343,  0.7253711 , -0.12498236,  0.11990362,\n",
       "       -0.3470809 ,  0.69323444,  0.72016764,  0.02434444,  0.3535809 ,\n",
       "        0.19183424,  0.1180287 , -0.9591374 , -0.4796641 , -0.08612835,\n",
       "       -0.12538305,  0.48527142, -0.22504804, -0.4204605 ,  0.24330547,\n",
       "        1.1860456 ,  0.2604432 ,  0.15190706, -0.70859885,  0.37568298,\n",
       "       -0.22914791, -0.17419972,  1.2612746 ,  0.43828404, -0.6273713 ,\n",
       "        0.34612507], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = nlp('Car')\n",
    "print('The token: \"{}\" has the following vector (dimension: {})'.format(token.text, len(token.vector)))\n",
    "token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* you might have to run `nltk.download('brown')` to install the NLTK corpus files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find words most similar to 'mother':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9791275262832642), ('husband', 0.96830815076828), ('wife', 0.9470910429954529), ('son', 0.9341433644294739), ('friend', 0.9211450815200806), ('nickname', 0.9091935157775879), ('voice', 0.9058724045753479), ('brother', 0.8998143672943115), ('patient', 0.8902835249900818), ('uncle', 0.8770355582237244)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the odd one out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve vector representation of the word \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5970706 ,  0.377084  ,  0.7432498 ,  0.18828899, -0.2765691 ,\n",
       "       -0.63277966,  1.4230691 ,  1.3836328 , -0.5716369 , -0.35936382,\n",
       "       -0.09947161, -0.70819783,  0.7136255 , -0.4917133 ,  0.09956642,\n",
       "       -0.63992554,  0.4826114 , -0.26998937, -0.4830221 , -1.0307387 ,\n",
       "        0.5938822 ,  0.04562862,  0.5864228 ,  0.74978435, -0.36310643,\n",
       "       -0.35855043,  0.21060367,  0.17792048, -0.73211503, -0.10176354,\n",
       "        0.43670663, -0.6949803 ,  0.78549016, -0.32429957,  0.31239748,\n",
       "        0.88337296, -0.37561104, -0.4772151 , -0.59918207,  0.01508404,\n",
       "       -0.08576139, -0.33897203,  0.5327704 ,  0.00574782,  0.6405279 ,\n",
       "        0.14520381, -0.322308  , -0.16603571,  0.07882246,  0.20980729,\n",
       "        1.1841923 , -0.6082804 , -0.4157628 , -0.09768971, -0.8226214 ,\n",
       "       -0.49966252,  0.97380173,  0.14174512, -0.6766032 , -0.1139592 ,\n",
       "        0.06136736,  0.27670348,  0.01457378, -0.6659458 , -1.0047694 ,\n",
       "        1.0267179 ,  0.06506062,  0.30315375, -0.83636177,  0.18053932,\n",
       "        0.14185588,  0.15356416,  0.7875036 ,  0.22756448,  1.2897012 ,\n",
       "        0.36193126,  1.5107402 ,  0.57190007, -0.14368013, -0.70084137,\n",
       "       -1.1185637 , -0.08633357, -0.41646263,  0.29357567, -0.39580432,\n",
       "        0.05883495,  0.4236732 , -0.10028225,  0.2033633 ,  0.15544872,\n",
       "        0.6197253 ,  0.12965065,  0.0816185 ,  0.02426882,  0.45462304,\n",
       "       -0.2127042 ,  0.19140436, -1.1411533 , -0.14560054, -0.31240144],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>BernardHickey</td>\n",
       "      <td>treasurer peter costello wednesday announced b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>PatriciaCommins</td>\n",
       "      <td>summit medical systems said monday revenues su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>BenjaminKangLim</td>\n",
       "      <td>selling motorcycles joy ride giant military fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>GrahamEarnshaw</td>\n",
       "      <td>china announced tuesday preferential tax rate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>FumikoFujisaki</td>\n",
       "      <td>year formed japanese financial megamerger worl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               text\n",
       "2204    BernardHickey  treasurer peter costello wednesday announced b...\n",
       "289   PatriciaCommins  summit medical systems said monday revenues su...\n",
       "151   BenjaminKangLim  selling motorcycles joy ride giant military fo...\n",
       "1388   GrahamEarnshaw  china announced tuesday preferential tax rate ...\n",
       "2383   FumikoFujisaki  year formed japanese financial megamerger worl..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the sample into a training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Naïve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.853\n",
      "Accuracy on testing set:\n",
      "0.73\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.64      0.88      0.74         8\n",
      "       AlanCrosby       0.53      1.00      0.70         8\n",
      "   AlexanderSmith       0.83      0.91      0.87        11\n",
      "  BenjaminKangLim       0.43      0.27      0.33        11\n",
      "    BernardHickey       0.58      0.70      0.64        10\n",
      "      BradDorfman       0.86      0.67      0.75         9\n",
      " DarrenSchuettler       0.71      0.71      0.71        14\n",
      "      DavidLawder       1.00      0.50      0.67        10\n",
      "    EdnaFernandes       0.60      1.00      0.75         3\n",
      "      EricAuchard       0.71      0.56      0.62         9\n",
      "   FumikoFujisaki       0.93      1.00      0.96        13\n",
      "   GrahamEarnshaw       0.62      0.91      0.74        11\n",
      " HeatherScoffield       0.83      0.50      0.62        10\n",
      "       JanLopatka       0.83      0.50      0.62        10\n",
      "    JaneMacartney       0.45      0.50      0.48        10\n",
      "     JimGilchrist       0.77      1.00      0.87        10\n",
      "   JoWinterbottom       1.00      1.00      1.00         8\n",
      "         JoeOrtiz       1.00      0.70      0.82        10\n",
      "     JohnMastrini       0.75      0.50      0.60         6\n",
      "     JonathanBirt       0.80      0.67      0.73        12\n",
      "      KarlPenhaul       0.93      1.00      0.97        14\n",
      "        KeithWeir       0.70      1.00      0.82         7\n",
      "   KevinDrawbaugh       0.86      0.60      0.71        10\n",
      "    KevinMorrison       0.75      0.71      0.73        17\n",
      "    KirstinRidley       0.56      0.50      0.53        10\n",
      "KouroshKarimkhany       0.89      0.89      0.89         9\n",
      "        LydiaZajc       0.75      1.00      0.86         9\n",
      "   LynneO'Donnell       1.00      0.78      0.88         9\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       0.86      0.67      0.75         9\n",
      "     MarkBendeich       1.00      0.64      0.78        11\n",
      "       MartinWolk       1.00      0.70      0.82        10\n",
      "     MatthewBunce       1.00      0.92      0.96        13\n",
      "    MichaelConnor       0.82      1.00      0.90         9\n",
      "       MureDickie       0.45      0.36      0.40        14\n",
      "        NickLouth       0.69      1.00      0.82         9\n",
      "  PatriciaCommins       0.54      0.88      0.67         8\n",
      "    PeterHumphrey       0.39      0.69      0.50        13\n",
      "       PierreTran       0.86      0.67      0.75         9\n",
      "       RobinSidel       1.00      0.83      0.91        12\n",
      "     RogerFillion       1.00      0.92      0.96        12\n",
      "      SamuelPerry       0.86      0.60      0.71        10\n",
      "     SarahDavison       1.00      0.38      0.55         8\n",
      "      ScottHillis       0.30      0.33      0.32         9\n",
      "      SimonCowell       1.00      0.69      0.82        13\n",
      "         TanEeLyn       0.50      0.44      0.47         9\n",
      "   TheresePoletti       0.75      1.00      0.86         9\n",
      "       TimFarrand       0.56      0.90      0.69        10\n",
      "       ToddNissen       0.42      1.00      0.59         5\n",
      "     WilliamKazer       0.60      0.30      0.40        10\n",
      "\n",
      "         accuracy                           0.73       500\n",
      "        macro avg       0.76      0.74      0.72       500\n",
      "     weighted avg       0.77      0.73      0.73       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: MarcelMichelson\n",
      "Predicted author: MarcelMichelson\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "        \n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.9985\n",
      "Accuracy on testing set:\n",
      "0.84\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.73      1.00      0.84         8\n",
      "       AlanCrosby       0.89      1.00      0.94         8\n",
      "   AlexanderSmith       0.92      1.00      0.96        11\n",
      "  BenjaminKangLim       0.53      0.73      0.62        11\n",
      "    BernardHickey       0.86      0.60      0.71        10\n",
      "      BradDorfman       0.73      0.89      0.80         9\n",
      " DarrenSchuettler       1.00      0.93      0.96        14\n",
      "      DavidLawder       0.80      0.80      0.80        10\n",
      "    EdnaFernandes       0.60      1.00      0.75         3\n",
      "      EricAuchard       0.57      0.89      0.70         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.83      0.91      0.87        11\n",
      " HeatherScoffield       1.00      1.00      1.00        10\n",
      "       JanLopatka       1.00      0.70      0.82        10\n",
      "    JaneMacartney       0.50      0.50      0.50        10\n",
      "     JimGilchrist       0.91      1.00      0.95        10\n",
      "   JoWinterbottom       1.00      1.00      1.00         8\n",
      "         JoeOrtiz       1.00      0.90      0.95        10\n",
      "     JohnMastrini       0.75      1.00      0.86         6\n",
      "     JonathanBirt       0.89      0.67      0.76        12\n",
      "      KarlPenhaul       1.00      1.00      1.00        14\n",
      "        KeithWeir       1.00      1.00      1.00         7\n",
      "   KevinDrawbaugh       0.80      0.80      0.80        10\n",
      "    KevinMorrison       0.88      0.82      0.85        17\n",
      "    KirstinRidley       1.00      0.50      0.67        10\n",
      "KouroshKarimkhany       1.00      0.89      0.94         9\n",
      "        LydiaZajc       0.90      1.00      0.95         9\n",
      "   LynneO'Donnell       1.00      0.78      0.88         9\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       0.88      0.78      0.82         9\n",
      "     MarkBendeich       0.71      0.91      0.80        11\n",
      "       MartinWolk       0.86      0.60      0.71        10\n",
      "     MatthewBunce       1.00      1.00      1.00        13\n",
      "    MichaelConnor       0.90      1.00      0.95         9\n",
      "       MureDickie       0.64      0.64      0.64        14\n",
      "        NickLouth       0.89      0.89      0.89         9\n",
      "  PatriciaCommins       0.89      1.00      0.94         8\n",
      "    PeterHumphrey       0.69      0.85      0.76        13\n",
      "       PierreTran       0.78      0.78      0.78         9\n",
      "       RobinSidel       1.00      0.92      0.96        12\n",
      "     RogerFillion       0.92      1.00      0.96        12\n",
      "      SamuelPerry       0.86      0.60      0.71        10\n",
      "     SarahDavison       1.00      0.50      0.67         8\n",
      "      ScottHillis       0.56      0.56      0.56         9\n",
      "      SimonCowell       1.00      0.92      0.96        13\n",
      "         TanEeLyn       0.78      0.78      0.78         9\n",
      "   TheresePoletti       0.90      1.00      0.95         9\n",
      "       TimFarrand       0.82      0.90      0.86        10\n",
      "       ToddNissen       0.71      1.00      0.83         5\n",
      "     WilliamKazer       0.50      0.30      0.38        10\n",
      "\n",
      "         accuracy                           0.84       500\n",
      "        macro avg       0.85      0.84      0.83       500\n",
      "     weighted avg       0.85      0.84      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: MarcelMichelson\n",
      "Predicted author: MarcelMichelson\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the options that should be tried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = { 'vect__stop_words': ['english'],\n",
    "                'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "             'clf__gamma' : [0.2, 0.3, 0.4], \n",
    "             'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;clf&#x27;, SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;clf__C&#x27;: [8, 10, 12], &#x27;clf__gamma&#x27;: [0.2, 0.3, 0.4],\n",
       "                         &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;], &#x27;vect__max_features&#x27;: [1500],\n",
       "                         &#x27;vect__ngram_range&#x27;: [(1, 1), (2, 2)],\n",
       "                         &#x27;vect__stop_words&#x27;: [&#x27;english&#x27;],\n",
       "                         &#x27;vect__strip_accents&#x27;: [&#x27;unicode&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('estimator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=estimator,-estimator%20object\">\n",
       "            estimator\n",
       "            <span class=\"param-doc-description\">estimator: estimator object<br><br>This is assumed to implement the scikit-learn estimator interface.<br>Either estimator needs to provide a ``score`` function,<br>or ``scoring`` must be passed.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">Pipeline(step...clf&#x27;, SVC())])</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('param_grid',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=param_grid,-dict%20or%20list%20of%20dictionaries\">\n",
       "            param_grid\n",
       "            <span class=\"param-doc-description\">param_grid: dict or list of dictionaries<br><br>Dictionary with parameters names (`str`) as keys and lists of<br>parameter settings to try as values, or a list of such<br>dictionaries, in which case the grids spanned by each dictionary<br>in the list are explored. This enables searching over any sequence<br>of parameter settings.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">{&#x27;clf__C&#x27;: [8, 10, ...], &#x27;clf__gamma&#x27;: [0.2, 0.3, ...], &#x27;clf__kernel&#x27;: [&#x27;rbf&#x27;], &#x27;vect__max_features&#x27;: [1500], ...}</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scoring',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=scoring,-str%2C%20callable%2C%20list%2C%20tuple%20or%20dict%2C%20default%3DNone\">\n",
       "            scoring\n",
       "            <span class=\"param-doc-description\">scoring: str, callable, list, tuple or dict, default=None<br><br>Strategy to evaluate the performance of the cross-validated model on<br>the test set.<br><br>If `scoring` represents a single score, one can use:<br><br>- a single string (see :ref:`scoring_string_names`);<br>- a callable (see :ref:`scoring_callable`) that returns a single value;<br>- `None`, the `estimator`'s<br>  :ref:`default evaluation criterion <scoring_api_overview>` is used.<br><br>If `scoring` represents multiple scores, one can use:<br><br>- a list or tuple of unique strings;<br>- a callable returning a dictionary where the keys are the metric<br>  names and the values are the metric scores;<br>- a dictionary with metric names as keys and callables as values.<br><br>See :ref:`multimetric_grid_search` for an example.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Number of jobs to run in parallel.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.<br><br>.. versionchanged:: v0.20<br>   `n_jobs` default changed from 1 to None</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('refit',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=refit,-bool%2C%20str%2C%20or%20callable%2C%20default%3DTrue\">\n",
       "            refit\n",
       "            <span class=\"param-doc-description\">refit: bool, str, or callable, default=True<br><br>Refit an estimator using the best found parameters on the whole<br>dataset.<br><br>For multiple metric evaluation, this needs to be a `str` denoting the<br>scorer that would be used to find the best parameters for refitting<br>the estimator at the end.<br><br>Where there are considerations other than maximum score in<br>choosing a best estimator, ``refit`` can be set to a function which<br>returns the selected ``best_index_`` given ``cv_results_``. In that<br>case, the ``best_estimator_`` and ``best_params_`` will be set<br>according to the returned ``best_index_`` while the ``best_score_``<br>attribute will not be available.<br><br>The refitted estimator is made available at the ``best_estimator_``<br>attribute and permits using ``predict`` directly on this<br>``GridSearchCV`` instance.<br><br>Also for multiple metric evaluation, the attributes ``best_index_``,<br>``best_score_`` and ``best_params_`` will only be available if<br>``refit`` is set and all of them will be determined w.r.t this specific<br>scorer.<br><br>See ``scoring`` parameter to know more about multiple metric<br>evaluation.<br><br>See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`<br>to see how to design a custom selection strategy using a callable<br>via `refit`.<br><br>See :ref:`this example<br><sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`<br>for an example of how to use ``refit=callable`` to balance model<br>complexity and cross-validated score.<br><br>.. versionchanged:: 0.20<br>    Support for callable added.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cv',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=cv,-int%2C%20cross-validation%20generator%20or%20an%20iterable%2C%20default%3DNone\">\n",
       "            cv\n",
       "            <span class=\"param-doc-description\">cv: int, cross-validation generator or an iterable, default=None<br><br>Determines the cross-validation splitting strategy.<br>Possible inputs for cv are:<br><br>- None, to use the default 5-fold cross validation,<br>- integer, to specify the number of folds in a `(Stratified)KFold`,<br>- :term:`CV splitter`,<br>- An iterable yielding (train, test) splits as arrays of indices.<br><br>For integer/None inputs, if the estimator is a classifier and ``y`` is<br>either binary or multiclass, :class:`StratifiedKFold` is used. In all<br>other cases, :class:`KFold` is used. These splitters are instantiated<br>with `shuffle=False` so the splits will be the same across calls.<br><br>Refer :ref:`User Guide <cross_validation>` for the various<br>cross-validation strategies that can be used here.<br><br>.. versionchanged:: 0.22<br>    ``cv`` default value if None changed from 3-fold to 5-fold.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=verbose,-int\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int<br><br>Controls the verbosity: the higher, the more messages.<br><br>- >1 : the computation time for each fold and parameter candidate is<br>  displayed;<br>- >2 : the score is also displayed;<br>- >3 : the fold and candidate parameter indexes are also displayed<br>  together with the starting time of the computation.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('pre_dispatch',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=pre_dispatch,-int%2C%20or%20str%2C%20default%3D%272%2An_jobs%27\">\n",
       "            pre_dispatch\n",
       "            <span class=\"param-doc-description\">pre_dispatch: int, or str, default='2*n_jobs'<br><br>Controls the number of jobs that get dispatched during parallel<br>execution. Reducing this number can be useful to avoid an<br>explosion of memory consumption when more jobs get dispatched<br>than CPUs can process. This parameter can be:<br><br>- None, in which case all the jobs are immediately created and spawned. Use<br>  this for lightweight and fast-running jobs, to avoid delays due to on-demand<br>  spawning of the jobs<br>- An int, giving the exact number of total jobs that are spawned<br>- A str, giving an expression as a function of n_jobs, as in '2*n_jobs'</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;2*n_jobs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('error_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=error_score,-%27raise%27%20or%20numeric%2C%20default%3Dnp.nan\">\n",
       "            error_score\n",
       "            <span class=\"param-doc-description\">error_score: 'raise' or numeric, default=np.nan<br><br>Value to assign to the score if an error occurs in estimator fitting.<br>If set to 'raise', the error is raised. If a numeric value is given,<br>FitFailedWarning is raised. This parameter does not affect the refit<br>step, which will always raise the error.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('return_train_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.model_selection.GridSearchCV.html#:~:text=return_train_score,-bool%2C%20default%3DFalse\">\n",
       "            return_train_score\n",
       "            <span class=\"param-doc-description\">return_train_score: bool, default=False<br><br>If ``False``, the ``cv_results_`` attribute will not include training<br>scores.<br>Computing training scores is used to get insights on how different<br>parameter settings impact the overfitting/underfitting trade-off.<br>However computing the scores on the training set can be computationally<br>expensive and is not strictly required to select the parameters that<br>yield the best generalization performance.<br><br>.. versionadded:: 0.19<br><br>.. versionchanged:: 0.21<br>    Default value was changed from ``True`` to ``False``</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___\"></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___vect__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=input,-%7B%27filename%27%2C%20%27file%27%2C%20%27content%27%7D%2C%20default%3D%27content%27\">\n",
       "            input\n",
       "            <span class=\"param-doc-description\">input: {'filename', 'file', 'content'}, default='content'<br><br>- If `'filename'`, the sequence passed as an argument to fit is<br>  expected to be a list of filenames that need reading to fetch<br>  the raw content to analyze.<br><br>- If `'file'`, the sequence items must have a 'read' method (file-like<br>  object) that is called to fetch the bytes in memory.<br><br>- If `'content'`, the input is expected to be a sequence of items that<br>  can be of type string or byte.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;content&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('encoding',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=encoding,-str%2C%20default%3D%27utf-8%27\">\n",
       "            encoding\n",
       "            <span class=\"param-doc-description\">encoding: str, default='utf-8'<br><br>If bytes or files are given to analyze, this encoding is used to<br>decode.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;utf-8&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decode_error',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=decode_error,-%7B%27strict%27%2C%20%27ignore%27%2C%20%27replace%27%7D%2C%20default%3D%27strict%27\">\n",
       "            decode_error\n",
       "            <span class=\"param-doc-description\">decode_error: {'strict', 'ignore', 'replace'}, default='strict'<br><br>Instruction on what to do if a byte sequence is given to analyze that<br>contains characters not of the given `encoding`. By default, it is<br>'strict', meaning that a UnicodeDecodeError will be raised. Other<br>values are 'ignore' and 'replace'.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;strict&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strip_accents',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=strip_accents,-%7B%27ascii%27%2C%20%27unicode%27%7D%20or%20callable%2C%20default%3DNone\">\n",
       "            strip_accents\n",
       "            <span class=\"param-doc-description\">strip_accents: {'ascii', 'unicode'} or callable, default=None<br><br>Remove accents and perform other character normalization<br>during the preprocessing step.<br>'ascii' is a fast method that only works on characters that have<br>a direct ASCII mapping.<br>'unicode' is a slightly slower method that works on any characters.<br>None (default) means no character normalization is performed.<br><br>Both 'ascii' and 'unicode' use NFKD normalization from<br>:func:`unicodedata.normalize`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;unicode&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('lowercase',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=lowercase,-bool%2C%20default%3DTrue\">\n",
       "            lowercase\n",
       "            <span class=\"param-doc-description\">lowercase: bool, default=True<br><br>Convert all characters to lowercase before tokenizing.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('preprocessor',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=preprocessor,-callable%2C%20default%3DNone\">\n",
       "            preprocessor\n",
       "            <span class=\"param-doc-description\">preprocessor: callable, default=None<br><br>Override the preprocessing (string transformation) stage while<br>preserving the tokenizing and n-grams generation steps.<br>Only applies if ``analyzer`` is not callable.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tokenizer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=tokenizer,-callable%2C%20default%3DNone\">\n",
       "            tokenizer\n",
       "            <span class=\"param-doc-description\">tokenizer: callable, default=None<br><br>Override the string tokenization step while preserving the<br>preprocessing and n-grams generation steps.<br>Only applies if ``analyzer == 'word'``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('analyzer',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=analyzer,-%7B%27word%27%2C%20%27char%27%2C%20%27char_wb%27%7D%20or%20callable%2C%20default%3D%27word%27\">\n",
       "            analyzer\n",
       "            <span class=\"param-doc-description\">analyzer: {'word', 'char', 'char_wb'} or callable, default='word'<br><br>Whether the feature should be made of word or character n-grams.<br>Option 'char_wb' creates character n-grams only from text inside<br>word boundaries; n-grams at the edges of words are padded with space.<br><br>If a callable is passed it is used to extract the sequence of features<br>out of the raw, unprocessed input.<br><br>.. versionchanged:: 0.21<br>    Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data<br>    is first read from the file and then passed to the given callable<br>    analyzer.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;word&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('stop_words',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=stop_words,-%7B%27english%27%7D%2C%20list%2C%20default%3DNone\">\n",
       "            stop_words\n",
       "            <span class=\"param-doc-description\">stop_words: {'english'}, list, default=None<br><br>If a string, it is passed to _check_stop_list and the appropriate stop<br>list is returned. 'english' is currently the only supported string<br>value.<br>There are several known issues with 'english' and you should<br>consider an alternative (see :ref:`stop_words`).<br><br>If a list, that list is assumed to contain stop words, all of which<br>will be removed from the resulting tokens.<br>Only applies if ``analyzer == 'word'``.<br><br>If None, no stop words will be used. In this case, setting `max_df`<br>to a higher value, such as in the range (0.7, 1.0), can automatically detect<br>and filter stop words based on intra corpus document frequency of terms.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;english&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('token_pattern',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=token_pattern,-str%2C%20default%3Dr%22%28%3Fu%29%5C%5Cb%5C%5Cw%5C%5Cw%2B%5C%5Cb%22\">\n",
       "            token_pattern\n",
       "            <span class=\"param-doc-description\">token_pattern: str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"<br><br>Regular expression denoting what constitutes a \"token\", only used<br>if ``analyzer == 'word'``. The default regexp selects tokens of 2<br>or more alphanumeric characters (punctuation is completely ignored<br>and always treated as a token separator).<br><br>If there is a capturing group in token_pattern then the<br>captured group content, not the entire match, becomes the token.<br>At most one capturing group is permitted.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;(?u)\\\\b\\\\w\\\\w+\\\\b&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ngram_range',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=ngram_range,-tuple%20%28min_n%2C%20max_n%29%2C%20default%3D%281%2C%201%29\">\n",
       "            ngram_range\n",
       "            <span class=\"param-doc-description\">ngram_range: tuple (min_n, max_n), default=(1, 1)<br><br>The lower and upper boundary of the range of n-values for different<br>n-grams to be extracted. All values of n such that min_n <= n <= max_n<br>will be used. For example an ``ngram_range`` of ``(1, 1)`` means only<br>unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means<br>only bigrams.<br>Only applies if ``analyzer`` is not callable.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">(1, ...)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=max_df,-float%20or%20int%2C%20default%3D1.0\">\n",
       "            max_df\n",
       "            <span class=\"param-doc-description\">max_df: float or int, default=1.0<br><br>When building the vocabulary ignore terms that have a document<br>frequency strictly higher than the given threshold (corpus-specific<br>stop words).<br>If float in range [0.0, 1.0], the parameter represents a proportion of<br>documents, integer absolute counts.<br>This parameter is ignored if vocabulary is not None.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_df',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=min_df,-float%20or%20int%2C%20default%3D1\">\n",
       "            min_df\n",
       "            <span class=\"param-doc-description\">min_df: float or int, default=1<br><br>When building the vocabulary ignore terms that have a document<br>frequency strictly lower than the given threshold. This value is also<br>called cut-off in the literature.<br>If float in range of [0.0, 1.0], the parameter represents a proportion<br>of documents, integer absolute counts.<br>This parameter is ignored if vocabulary is not None.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=max_features,-int%2C%20default%3DNone\">\n",
       "            max_features\n",
       "            <span class=\"param-doc-description\">max_features: int, default=None<br><br>If not None, build a vocabulary that only consider the top<br>`max_features` ordered by term frequency across the corpus.<br>Otherwise, all features are used.<br><br>This parameter is ignored if vocabulary is not None.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1500</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('vocabulary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=vocabulary,-Mapping%20or%20iterable%2C%20default%3DNone\">\n",
       "            vocabulary\n",
       "            <span class=\"param-doc-description\">vocabulary: Mapping or iterable, default=None<br><br>Either a Mapping (e.g., a dict) where keys are terms and values are<br>indices in the feature matrix, or an iterable over terms. If not<br>given, a vocabulary is determined from the input documents.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('binary',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=binary,-bool%2C%20default%3DFalse\">\n",
       "            binary\n",
       "            <span class=\"param-doc-description\">binary: bool, default=False<br><br>If True, all non-zero term counts are set to 1. This does not mean<br>outputs will have only 0/1 values, only that the tf term in tf-idf<br>is binary. (Set `binary` to True, `use_idf` to False and<br>`norm` to None to get 0/1 outputs).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=dtype,-dtype%2C%20default%3Dfloat64\">\n",
       "            dtype\n",
       "            <span class=\"param-doc-description\">dtype: dtype, default=float64<br><br>Type of the matrix returned by fit_transform() or transform().</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('norm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=norm,-%7B%27l1%27%2C%20%27l2%27%7D%20or%20None%2C%20default%3D%27l2%27\">\n",
       "            norm\n",
       "            <span class=\"param-doc-description\">norm: {'l1', 'l2'} or None, default='l2'<br><br>Each output row will have unit norm, either:<br><br>- 'l2': Sum of squares of vector elements is 1. The cosine<br>  similarity between two vectors is their dot product when l2 norm has<br>  been applied.<br>- 'l1': Sum of absolute values of vector elements is 1.<br>  See :func:`~sklearn.preprocessing.normalize`.<br>- None: No normalization.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('use_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=use_idf,-bool%2C%20default%3DTrue\">\n",
       "            use_idf\n",
       "            <span class=\"param-doc-description\">use_idf: bool, default=True<br><br>Enable inverse-document-frequency reweighting. If False, idf(t) = 1.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('smooth_idf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=smooth_idf,-bool%2C%20default%3DTrue\">\n",
       "            smooth_idf\n",
       "            <span class=\"param-doc-description\">smooth_idf: bool, default=True<br><br>Smooth idf weights by adding one to document frequencies, as if an<br>extra document was seen containing every term in the collection<br>exactly once. Prevents zero divisions.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sublinear_tf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#:~:text=sublinear_tf,-bool%2C%20default%3DFalse\">\n",
       "            sublinear_tf\n",
       "            <span class=\"param-doc-description\">sublinear_tf: bool, default=False<br><br>Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"best_estimator___clf__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=C,-float%2C%20default%3D1.0\">\n",
       "            C\n",
       "            <span class=\"param-doc-description\">C: float, default=1.0<br><br>Regularization parameter. The strength of the regularization is<br>inversely proportional to C. Must be strictly positive. The penalty<br>is a squared l2 penalty. For an intuitive visualization of the effects<br>of scaling the regularization parameter C, see<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=kernel,-%7B%27linear%27%2C%20%27poly%27%2C%20%27rbf%27%2C%20%27sigmoid%27%2C%20%27precomputed%27%7D%20or%20callable%2C%20%20%20%20%20%20%20%20%20%20default%3D%27rbf%27\">\n",
       "            kernel\n",
       "            <span class=\"param-doc-description\">kernel: {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'<br><br>Specifies the kernel type to be used in the algorithm. If<br>none is given, 'rbf' will be used. If a callable is given it is used to<br>pre-compute the kernel matrix from data matrices; that matrix should be<br>an array of shape ``(n_samples, n_samples)``. For an intuitive<br>visualization of different kernel types see<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=degree,-int%2C%20default%3D3\">\n",
       "            degree\n",
       "            <span class=\"param-doc-description\">degree: int, default=3<br><br>Degree of the polynomial kernel function ('poly').<br>Must be non-negative. Ignored by all other kernels.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=gamma,-%7B%27scale%27%2C%20%27auto%27%7D%20or%20float%2C%20default%3D%27scale%27\">\n",
       "            gamma\n",
       "            <span class=\"param-doc-description\">gamma: {'scale', 'auto'} or float, default='scale'<br><br>Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.<br><br>- if ``gamma='scale'`` (default) is passed then it uses<br>  1 / (n_features * X.var()) as value of gamma,<br>- if 'auto', uses 1 / n_features<br>- if float, must be non-negative.<br><br>.. versionchanged:: 0.22<br>   The default value of ``gamma`` changed from 'auto' to 'scale'.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=coef0,-float%2C%20default%3D0.0\">\n",
       "            coef0\n",
       "            <span class=\"param-doc-description\">coef0: float, default=0.0<br><br>Independent term in kernel function.<br>It is only significant in 'poly' and 'sigmoid'.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=shrinking,-bool%2C%20default%3DTrue\">\n",
       "            shrinking\n",
       "            <span class=\"param-doc-description\">shrinking: bool, default=True<br><br>Whether to use the shrinking heuristic.<br>See the :ref:`User Guide <shrinking_svm>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('probability',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=probability,-bool%2C%20default%3DFalse\">\n",
       "            probability\n",
       "            <span class=\"param-doc-description\">probability: bool, default=False<br><br>Whether to enable probability estimates. This must be enabled prior<br>to calling `fit`, will slow down that method as it internally uses<br>5-fold cross-validation, and `predict_proba` may be inconsistent with<br>`predict`. Read more in the :ref:`User Guide <scores_probabilities>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=tol,-float%2C%20default%3D1e-3\">\n",
       "            tol\n",
       "            <span class=\"param-doc-description\">tol: float, default=1e-3<br><br>Tolerance for stopping criterion.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=cache_size,-float%2C%20default%3D200\">\n",
       "            cache_size\n",
       "            <span class=\"param-doc-description\">cache_size: float, default=200<br><br>Specify the size of the kernel cache (in MB).</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=class_weight,-dict%20or%20%27balanced%27%2C%20default%3DNone\">\n",
       "            class_weight\n",
       "            <span class=\"param-doc-description\">class_weight: dict or 'balanced', default=None<br><br>Set the parameter C of class i to class_weight[i]*C for<br>SVC. If not given, all classes are supposed to have<br>weight one.<br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=verbose,-bool%2C%20default%3DFalse\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: bool, default=False<br><br>Enable verbose output. Note that this setting takes advantage of a<br>per-process runtime setting in libsvm that, if enabled, may not work<br>properly in a multithreaded context.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=max_iter,-int%2C%20default%3D-1\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=-1<br><br>Hard limit on iterations within solver, or -1 for no limit.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decision_function_shape',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=decision_function_shape,-%7B%27ovo%27%2C%20%27ovr%27%7D%2C%20default%3D%27ovr%27\">\n",
       "            decision_function_shape\n",
       "            <span class=\"param-doc-description\">decision_function_shape: {'ovo', 'ovr'}, default='ovr'<br><br>Whether to return a one-vs-rest ('ovr') decision function of shape<br>(n_samples, n_classes) as all other classifiers, or the original<br>one-vs-one ('ovo') decision function of libsvm which has shape<br>(n_samples, n_classes * (n_classes - 1) / 2). However, note that<br>internally, one-vs-one ('ovo') is always used as a multi-class strategy<br>to train models; an ovr matrix is only constructed from the ovo matrix.<br>The parameter is ignored for binary classification.<br><br>.. versionchanged:: 0.19<br>    decision_function_shape is 'ovr' by default.<br><br>.. versionadded:: 0.17<br>   *decision_function_shape='ovr'* is recommended.<br><br>.. versionchanged:: 0.17<br>   Deprecated *decision_function_shape='ovo' and None*.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;ovr&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('break_ties',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=break_ties,-bool%2C%20default%3DFalse\">\n",
       "            break_ties\n",
       "            <span class=\"param-doc-description\">break_ties: bool, default=False<br><br>If true, ``decision_function_shape='ovr'``, and number of classes > 2,<br>:term:`predict` will break ties according to the confidence values of<br>:term:`decision_function`; otherwise the first class among the tied<br>classes is returned. Please note that breaking ties comes at a<br>relatively high computational cost compared to a simple predict. See<br>:ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an<br>example of its usage with ``decision_function_shape='ovr'``.<br><br>.. versionadded:: 0.22</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.svm.SVC.html#:~:text=random_state,-int%2C%20RandomState%20instance%20or%20None%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance or None, default=None<br><br>Controls the pseudo random number generation for shuffling the data for<br>probability estimates. Ignored when `probability` is False.<br>Pass an int for reproducible output across multiple function calls.<br>See :term:`Glossary <random_state>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf', SVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__C': [8, 10, 12], 'clf__gamma': [0.2, 0.3, 0.4],\n",
       "                         'clf__kernel': ['rbf'], 'vect__max_features': [1500],\n",
       "                         'vect__ngram_range': [(1, 1), (2, 2)],\n",
       "                         'vect__stop_words': ['english'],\n",
       "                         'vect__strip_accents': ['unicode']})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(clf_search, \n",
    "                    param_grid=parameters, \n",
    "                    # scoring=make_scorer(f1_score, average='micro'), \n",
    "                    n_jobs=-1\n",
    "                   )\n",
    "grid.fit(X_train, y_train)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* if you are on a powerful (preferably unix system) you can set n_jobs to the number of available threads to speed up the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'clf__C': 8, 'clf__gamma': 0.3, 'clf__kernel': 'rbf', 'vect__max_features': 1500, 'vect__ngram_range': (1, 1), 'vect__stop_words': 'english', 'vect__strip_accents': 'unicode'} with a score of 0.79\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.73      1.00      0.84         8\n",
      "       AlanCrosby       0.89      1.00      0.94         8\n",
      "   AlexanderSmith       0.92      1.00      0.96        11\n",
      "  BenjaminKangLim       0.53      0.73      0.62        11\n",
      "    BernardHickey       0.86      0.60      0.71        10\n",
      "      BradDorfman       0.73      0.89      0.80         9\n",
      " DarrenSchuettler       1.00      0.93      0.96        14\n",
      "      DavidLawder       0.80      0.80      0.80        10\n",
      "    EdnaFernandes       0.60      1.00      0.75         3\n",
      "      EricAuchard       0.57      0.89      0.70         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.83      0.91      0.87        11\n",
      " HeatherScoffield       1.00      1.00      1.00        10\n",
      "       JanLopatka       1.00      0.70      0.82        10\n",
      "    JaneMacartney       0.50      0.50      0.50        10\n",
      "     JimGilchrist       0.91      1.00      0.95        10\n",
      "   JoWinterbottom       1.00      1.00      1.00         8\n",
      "         JoeOrtiz       1.00      0.90      0.95        10\n",
      "     JohnMastrini       0.75      1.00      0.86         6\n",
      "     JonathanBirt       0.89      0.67      0.76        12\n",
      "      KarlPenhaul       1.00      1.00      1.00        14\n",
      "        KeithWeir       1.00      1.00      1.00         7\n",
      "   KevinDrawbaugh       0.80      0.80      0.80        10\n",
      "    KevinMorrison       0.88      0.82      0.85        17\n",
      "    KirstinRidley       1.00      0.50      0.67        10\n",
      "KouroshKarimkhany       1.00      0.89      0.94         9\n",
      "        LydiaZajc       0.90      1.00      0.95         9\n",
      "   LynneO'Donnell       1.00      0.78      0.88         9\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       0.89      0.89      0.89         9\n",
      "     MarkBendeich       0.71      0.91      0.80        11\n",
      "       MartinWolk       0.86      0.60      0.71        10\n",
      "     MatthewBunce       1.00      1.00      1.00        13\n",
      "    MichaelConnor       0.90      1.00      0.95         9\n",
      "       MureDickie       0.64      0.64      0.64        14\n",
      "        NickLouth       0.89      0.89      0.89         9\n",
      "  PatriciaCommins       0.89      1.00      0.94         8\n",
      "    PeterHumphrey       0.69      0.85      0.76        13\n",
      "       PierreTran       0.88      0.78      0.82         9\n",
      "       RobinSidel       1.00      0.92      0.96        12\n",
      "     RogerFillion       0.92      1.00      0.96        12\n",
      "      SamuelPerry       0.86      0.60      0.71        10\n",
      "     SarahDavison       1.00      0.50      0.67         8\n",
      "      ScottHillis       0.56      0.56      0.56         9\n",
      "      SimonCowell       1.00      0.92      0.96        13\n",
      "         TanEeLyn       0.78      0.78      0.78         9\n",
      "   TheresePoletti       0.90      1.00      0.95         9\n",
      "       TimFarrand       0.82      0.90      0.86        10\n",
      "       ToddNissen       0.71      1.00      0.83         5\n",
      "     WilliamKazer       0.50      0.30      0.38        10\n",
      "\n",
      "         accuracy                           0.84       500\n",
      "        macro avg       0.85      0.85      0.84       500\n",
      "     weighted avg       0.85      0.84      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizer (using countvectorizer for the sake of example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english', max_df=0.8)\n",
    "tf_large = vectorizer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                n_jobs=-1)\n",
    "lda_fitted = lda.fit_transform(tf_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words):\n",
    "    out_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = save_top_words(lda, vectorizer.get_feature_names_out(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>conrail new merger long services companies com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>china kong hong chinese beijing people deng pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>percent million year quarter sales share billi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>bank market banks financial percent securities...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>new company ford gm tobacco car industry chrys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>company new internet computer software microso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>year percent tonnes prices oil market million ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>thomson aol online new service america french ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>pounds million british bre shares gold company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>government czech minister percent told deal po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                          top_words\n",
       "0         1  conrail new merger long services companies com...\n",
       "1         2  china kong hong chinese beijing people deng pa...\n",
       "2         3  percent million year quarter sales share billi...\n",
       "3         4  bank market banks financial percent securities...\n",
       "4         5  new company ford gm tobacco car industry chrys...\n",
       "5         6  company new internet computer software microso...\n",
       "6         7  year percent tonnes prices oil market million ...\n",
       "7         8  thomson aol online new service america french ...\n",
       "8         9  pounds million british bre shares gold company...\n",
       "9        10  government czech minister percent told deal po..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pyLDAvis.lda_model\n",
    "# import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el9843381322139795939368380334490\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el9843381322139795939368380334490_data = {\"mdsDat\": {\"x\": [0.195590465420789, -0.18621323500037965, 0.09009758262919164, -0.13208915261611417, 0.00034391759819114804, 0.012645150022609039, 0.1537661592058128, -0.061683059946498454, -0.050851059625657474, -0.02160676768794411], \"y\": [-0.0012573254017963899, -0.16141459817338683, -0.03353694313077179, -0.08217404324980881, 0.025032104668227383, 0.07847514236117857, -0.1650630770812215, 0.13063749537566482, 0.023378770465930718, 0.18592247416598373], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.585693794073634, 15.989689918721837, 13.097525141116645, 10.528011761976499, 9.44292571905304, 9.412560291190907, 8.086042619817412, 6.364484520008082, 5.929084484821582, 4.563981749220364]}, \"tinfo\": {\"Term\": [\"china\", \"kong\", \"hong\", \"bank\", \"pounds\", \"chinese\", \"quarter\", \"beijing\", \"million\", \"percent\", \"tonnes\", \"government\", \"banks\", \"company\", \"new\", \"sales\", \"bre\", \"internet\", \"gold\", \"earnings\", \"british\", \"service\", \"deng\", \"profit\", \"software\", \"year\", \"computer\", \"merger\", \"ford\", \"shares\", \"quarter\", \"quaker\", \"fourth\", \"snapple\", \"earnings\", \"jones\", \"items\", \"margins\", \"expectations\", \"profit\", \"posted\", \"earned\", \"consensus\", \"quarters\", \"decline\", \"fiscal\", \"grew\", \"income\", \"sales\", \"net\", \"margin\", \"gains\", \"slightly\", \"cereal\", \"outlook\", \"rose\", \"revenues\", \"results\", \"estimates\", \"pretax\", \"growth\", \"half\", \"profits\", \"share\", \"cents\", \"revenue\", \"percent\", \"million\", \"reported\", \"year\", \"increase\", \"analysts\", \"fell\", \"costs\", \"analyst\", \"forecast\", \"billion\", \"strong\", \"company\", \"expected\", \"business\", \"market\", \"stock\", \"shares\", \"hong\", \"kong\", \"beijing\", \"deng\", \"tung\", \"taiwan\", \"handover\", \"territory\", \"jiang\", \"democracy\", \"hwang\", \"colonial\", \"xiaoping\", \"legislature\", \"seoul\", \"sino\", \"xinjiang\", \"colony\", \"li\", \"midnight\", \"pyongyang\", \"diplomat\", \"patten\", \"apec\", \"diplomatic\", \"zemin\", \"xinhua\", \"china\", \"diplomats\", \"provisional\", \"chinese\", \"human\", \"mainland\", \"korean\", \"police\", \"rights\", \"death\", \"political\", \"communist\", \"people\", \"party\", \"leader\", \"civil\", \"korea\", \"official\", \"officials\", \"trade\", \"foreign\", \"south\", \"economic\", \"july\", \"state\", \"years\", \"united\", \"government\", \"british\", \"year\", \"told\", \"states\", \"new\", \"banks\", \"lending\", \"mortgage\", \"nomura\", \"bank\", \"banking\", \"bankers\", \"institutions\", \"loan\", \"bond\", \"index\", \"loans\", \"natwest\", \"mutual\", \"securities\", \"yen\", \"trust\", \"asset\", \"zealand\", \"debt\", \"funds\", \"australia\", \"estate\", \"deposits\", \"credit\", \"brokerage\", \"insurance\", \"brokers\", \"listed\", \"prudential\", \"points\", \"financial\", \"markets\", \"investment\", \"investors\", \"life\", \"japan\", \"firms\", \"market\", \"finance\", \"australian\", \"foreign\", \"trading\", \"stock\", \"business\", \"exchange\", \"capital\", \"billion\", \"percent\", \"analysts\", \"year\", \"shares\", \"major\", \"companies\", \"new\", \"investigation\", \"suez\", \"unions\", \"probe\", \"tender\", \"klaus\", \"budget\", \"debate\", \"labor\", \"opposition\", \"suspended\", \"prime\", \"labour\", \"election\", \"bids\", \"proposal\", \"czech\", \"rejected\", \"electricity\", \"minister\", \"vote\", \"original\", \"strike\", \"scandal\", \"auction\", \"compensation\", \"prague\", \"reform\", \"won\", \"parliament\", \"newspapers\", \"government\", \"ministry\", \"power\", \"workers\", \"union\", \"electric\", \"board\", \"committee\", \"decision\", \"house\", \"meeting\", \"tuesday\", \"deal\", \"told\", \"plan\", \"president\", \"country\", \"statement\", \"party\", \"bid\", \"pay\", \"chairman\", \"percent\", \"national\", \"company\", \"billion\", \"public\", \"friday\", \"week\", \"year\", \"bre\", \"busang\", \"gold\", \"itt\", \"pence\", \"freeport\", \"pounds\", \"lloyd\", \"indonesian\", \"walsh\", \"barrick\", \"hilton\", \"sears\", \"minerals\", \"mgam\", \"strathcona\", \"pound\", \"ce\", \"mining\", \"uk\", \"bzw\", \"toronto\", \"hotels\", \"indonesia\", \"deutsche\", \"england\", \"hotel\", \"northern\", \"deposit\", \"names\", \"plc\", \"london\", \"british\", \"britain\", \"bid\", \"shares\", \"canada\", \"million\", \"canadian\", \"group\", \"company\", \"offer\", \"executive\", \"chief\", \"stock\", \"percent\", \"billion\", \"market\", \"share\", \"year\", \"microsoft\", \"software\", \"netscape\", \"hp\", \"devices\", \"computers\", \"apple\", \"pcs\", \"pc\", \"computer\", \"machines\", \"windows\", \"web\", \"sun\", \"ibm\", \"internet\", \"encryption\", \"san\", \"music\", \"technology\", \"version\", \"intel\", \"technologies\", \"electronic\", \"mainframe\", \"robotics\", \"video\", \"applications\", \"film\", \"game\", \"personal\", \"systems\", \"product\", \"users\", \"products\", \"network\", \"networks\", \"new\", \"company\", \"use\", \"information\", \"data\", \"business\", \"industry\", \"companies\", \"market\", \"based\", \"world\", \"year\", \"international\", \"years\", \"executive\", \"president\", \"tonnes\", \"crop\", \"exports\", \"cocoa\", \"tonne\", \"output\", \"russia\", \"corn\", \"producers\", \"exporters\", \"grain\", \"zinc\", \"lme\", \"rains\", \"gazprom\", \"imports\", \"norilsk\", \"nickel\", \"coffee\", \"sugar\", \"ivory\", \"import\", \"metal\", \"steel\", \"export\", \"russian\", \"metals\", \"oil\", \"skoda\", \"reserves\", \"inflation\", \"coast\", \"traders\", \"prices\", \"production\", \"gas\", \"crown\", \"crowns\", \"domestic\", \"demand\", \"year\", \"percent\", \"figures\", \"copper\", \"market\", \"world\", \"million\", \"trade\", \"total\", \"price\", \"billion\", \"industry\", \"thomson\", \"aol\", \"boeing\", \"csf\", \"alcatel\", \"lagardere\", \"airbus\", \"aerospatiale\", \"plane\", \"aviation\", \"gec\", \"cathay\", \"satellite\", \"sa\", \"tv\", \"aircraft\", \"subscribers\", \"airline\", \"airlines\", \"france\", \"cable\", \"online\", \"electronics\", \"air\", \"paris\", \"french\", \"broadcasting\", \"digital\", \"francs\", \"channels\", \"defence\", \"america\", \"television\", \"sale\", \"service\", \"stake\", \"network\", \"european\", \"new\", \"group\", \"company\", \"state\", \"deal\", \"billion\", \"million\", \"ford\", \"chrysler\", \"gm\", \"tobacco\", \"motor\", \"auto\", \"uaw\", \"drug\", \"lawsuit\", \"automotive\", \"automaker\", \"cigarette\", \"wmx\", \"florida\", \"lawsuits\", \"philip\", \"trucks\", \"sport\", \"car\", \"morris\", \"vehicles\", \"truck\", \"lawyers\", \"vehicle\", \"waste\", \"attorney\", \"mercury\", \"cars\", \"trial\", \"filed\", \"class\", \"general\", \"plant\", \"new\", \"company\", \"workers\", \"industry\", \"american\", \"court\", \"case\", \"states\", \"production\", \"million\", \"thursday\", \"action\", \"york\", \"united\", \"years\", \"companies\", \"stock\", \"sales\", \"mci\", \"distance\", \"conrail\", \"fcc\", \"nynex\", \"csx\", \"concert\", \"bt\", \"phone\", \"atlantic\", \"wireless\", \"bell\", \"licenses\", \"telecommunications\", \"carriers\", \"sprint\", \"optus\", \"provision\", \"telecoms\", \"norfolk\", \"calling\", \"communications\", \"carrier\", \"merger\", \"telecom\", \"calls\", \"radio\", \"operators\", \"regulators\", \"telephone\", \"local\", \"southern\", \"services\", \"customers\", \"service\", \"long\", \"offer\", \"companies\", \"new\", \"deal\", \"billion\", \"corp\", \"shareholders\", \"expected\", \"industry\", \"access\", \"company\"], \"Freq\": [3383.0, 2288.0, 2272.0, 1772.0, 1286.0, 1553.0, 1471.0, 1307.0, 5071.0, 5580.0, 791.0, 1906.0, 891.0, 3714.0, 3367.0, 1597.0, 660.0, 755.0, 631.0, 1002.0, 1260.0, 860.0, 771.0, 1018.0, 552.0, 5144.0, 606.0, 628.0, 446.0, 1582.0, 1450.7752522226135, 87.88645385809133, 468.69655609877685, 84.25629369029085, 930.8507955923151, 94.81382878999709, 103.06159588256287, 272.55176270936914, 208.10869348551913, 921.374245014726, 150.98455983599663, 106.01226661050016, 80.82468399379, 79.88533661821614, 125.57232365353839, 195.54787621778763, 99.34365799497289, 234.57188850565052, 1315.4687810381308, 465.57446725756733, 113.04212999519517, 123.86253592118413, 149.58156249910505, 74.04691105829578, 96.73801680674934, 496.4250174626758, 448.53770351587406, 541.9792392187483, 188.17904646645798, 85.89235385676369, 806.9040429079258, 613.2449099444906, 632.0110476623216, 1211.007378548922, 420.4704840905696, 276.7782397228719, 2953.852068100408, 2636.327107535679, 411.71989001869173, 2377.543281828957, 367.8062948928888, 1115.4134849946702, 338.19262269796025, 370.9886976342112, 705.0215684321554, 227.15682586604083, 1171.0430707705107, 397.3068080239445, 1010.0529386058497, 627.9948376089555, 598.2234577690394, 719.1187143800598, 506.0125590523411, 442.03994777590486, 2271.154211219157, 2287.6684247227936, 1306.5315135471153, 770.1091674119037, 379.200622516457, 336.2344618391303, 284.9435564079234, 327.6705773125578, 261.2232326664239, 218.30935125866125, 186.60050475124476, 185.25384259229835, 178.25176921416025, 176.07378791683624, 158.85072647828935, 151.16481637664432, 145.71607071173156, 140.32866038855929, 137.4565948871274, 131.62229649235314, 123.87849001084912, 117.97402074224594, 109.27499979725489, 106.05612612639683, 103.00508197246162, 97.39161475733427, 97.11966085594734, 3350.875968704314, 92.9844182489941, 107.68998400670158, 1504.0922335189898, 315.8825394706228, 101.89966146526007, 212.98322698713255, 304.45667274493997, 470.03263131151397, 254.39247911604267, 490.2533289101342, 334.62955265075965, 898.9218255757177, 517.8662413843352, 364.226925920671, 204.1862909063015, 200.58988487048484, 426.1385058203755, 476.1700338621028, 439.7472149020003, 400.098759157884, 291.1311578829163, 312.06760180228446, 261.8225939081778, 402.50104772968376, 415.1308488961849, 332.8847931706213, 405.61614384842983, 358.0076974537651, 450.0102507809414, 353.6957220605524, 313.66411372636696, 334.79709925124064, 887.1762217826753, 99.56817197350922, 82.73659165603887, 465.28583765258327, 1724.3216138134608, 534.7355014921415, 110.72562285080053, 129.0758926141985, 129.16585689229206, 111.33690191450971, 213.80169275317118, 228.0053222299632, 208.81644318801747, 122.94562213549719, 602.6583388264345, 181.08261049330656, 147.83975592033303, 162.9747436373066, 86.11494786106657, 418.79845043902003, 277.57963213069763, 375.6694576304022, 98.3112691996245, 79.70423966853676, 253.6603383269637, 168.77464593906313, 279.8178165650396, 157.0439794225127, 96.45894491176935, 80.6019960460112, 194.13308331605995, 713.3662950264506, 585.6537945068075, 525.3003508980356, 431.72607088860957, 238.61708886029956, 298.0363990276769, 277.4789745634255, 1068.7548342422097, 254.17724776628452, 208.09112794428557, 388.1667804503089, 375.500330671382, 537.8149342121993, 532.8254715359401, 309.2632244976839, 300.2145357450546, 535.074485113514, 659.3467277975104, 427.91083096987666, 498.99290328597465, 360.0428318348563, 303.5370137354003, 305.57259074022636, 312.00702948267235, 207.88796791726628, 91.47928297664144, 100.46915948680966, 106.41812525907991, 91.83324548300858, 133.50583734942762, 239.3657672016532, 94.9683273900596, 72.64205186319673, 126.39427781618107, 112.04585494898346, 173.03151667111686, 222.3528618454786, 112.76704382116507, 69.63535015419691, 104.59645755644321, 471.14139779509463, 82.06210034090677, 141.58184167559438, 344.1406420780642, 133.23925791686958, 93.1096208275617, 240.93946993171306, 128.38067119295903, 101.13576657969574, 72.93786162447448, 185.5416617296525, 109.77916857861543, 99.78331607674843, 95.3998899248273, 97.70278750723308, 1090.1221527827636, 224.30297520476785, 245.34854753353926, 233.8311910916695, 205.040822924589, 160.61902380287705, 193.5765000734015, 152.19677897363496, 191.01248754765018, 148.78658737281773, 197.4649280856511, 243.20141937925348, 256.1358189124656, 287.86155193879245, 211.14825538769043, 222.58165174637784, 176.3618060421309, 176.86968007731693, 195.39272709896954, 185.8758876847214, 178.60352426459698, 197.981264073345, 296.29024422694295, 184.71852773395491, 245.30807207586204, 227.9828023887207, 179.12251730496945, 182.79737773896585, 180.5832946611466, 181.44634440212067, 658.9120852658774, 380.9040212695098, 630.1282672740087, 207.66317401450146, 505.5342265317516, 199.73435107127264, 1278.3583779214257, 158.37154338306954, 158.59808610424454, 117.44908682099108, 119.79577019488843, 130.59076394691158, 98.98016691805772, 110.17531550604288, 118.05640461399616, 119.56628490820627, 372.46904864192425, 167.47856347539644, 279.01838652671137, 189.71635684056372, 99.11298160828409, 358.5413358465541, 94.81334534147815, 134.66423796549498, 80.24728986402496, 78.62777006180872, 76.49230752687353, 262.69999912353313, 108.50577139072324, 87.67467877227551, 325.65321905334065, 345.9381329971157, 678.9644874248162, 401.4530389560473, 364.08580899209073, 642.8713402727258, 269.37597137690045, 1059.3528218649417, 190.16830534181642, 514.8798811864958, 621.9088557304115, 248.62441683557958, 265.09034803844696, 262.8135853974983, 296.21482043241065, 395.50063756340563, 304.810291249516, 250.24940755760684, 220.3925546670837, 202.38325259914194, 472.82040095515333, 551.0924646890164, 181.63866463866356, 182.86547119394623, 89.69589163953665, 251.17548255580783, 368.3631305198817, 127.98581522976708, 116.36983585953116, 583.2268928750302, 174.8053803608192, 150.07540865971055, 145.02220239074452, 293.5553387648509, 403.50447259392627, 699.1789021195225, 75.64393365879836, 151.06705065368902, 83.21142334428654, 362.6720960984496, 82.03367994549386, 291.4874513024054, 94.80766986830349, 186.1163513705211, 101.39739852225928, 61.521790725057485, 76.25561558549887, 72.6798090003485, 75.24373916100821, 71.40546253967209, 214.553894880227, 265.4793019590804, 229.1320462461806, 147.4536558048844, 344.5921416551861, 245.40571251397083, 137.5698696887562, 706.8391999970694, 729.2888694742214, 220.7731298525872, 182.51283827256924, 167.65241084417053, 419.6470737533726, 359.8403213824507, 360.3841521059603, 470.97984923448814, 299.0477293761475, 255.20317360980462, 334.53066671492627, 197.70670663667093, 187.41230092999595, 185.44867536249413, 184.19617709335643, 789.9520545506102, 318.85502720356817, 360.59236613025644, 223.34826621417866, 199.60416631734893, 250.72393860537392, 319.2326434078632, 142.99132479382737, 124.27326926763907, 113.32258807409626, 124.1237583114423, 138.24193222587812, 105.19376821548413, 100.14225316906463, 95.97875537109418, 219.7296183393428, 159.8366036628024, 123.64722113880889, 207.37281738314454, 101.94025905573902, 182.08651922947172, 116.04742907163003, 128.6916206580771, 101.91321946535865, 312.03294312170505, 192.98291492016824, 125.42931488283511, 423.69956248633235, 93.61407979427152, 122.45635300831347, 231.85542931330562, 207.72755425026924, 219.5195553595661, 513.4310881805051, 304.4760073917062, 190.0338424844453, 189.21032530415536, 167.63330648295127, 240.9252110090298, 226.49815754548123, 806.7451828084783, 796.3075137395399, 188.0060104416851, 169.37144732079915, 414.05759471274683, 274.6233717624024, 367.45535864405997, 229.8469747632184, 210.64134376862322, 223.50155018341158, 248.10348854695962, 202.46797218974913, 429.630948152994, 382.92995391371284, 293.9193362393137, 254.38063046016015, 142.59950135482003, 135.5659544336537, 131.44895504471492, 119.11873777869269, 96.9530495153776, 95.48905714977651, 82.89332863138969, 87.6695645691057, 196.06587271247207, 95.37747604961471, 249.8353823275543, 194.3737108414281, 135.99610949134268, 148.9228324591197, 179.86504815311483, 294.7100879137715, 290.5702941851188, 382.02191871966807, 205.4698484488927, 276.2844999308823, 65.77250341740316, 299.46816351280637, 82.12956939101821, 244.80284487837272, 182.70336448851228, 76.4438017640805, 222.5687773615339, 302.6827664488153, 282.18225060222903, 273.7170862545366, 350.99447468109975, 208.7865018118659, 213.81445165016825, 186.39748270375435, 374.7641834834629, 271.8926788731244, 288.3799217531781, 210.04233585682613, 177.29748937698565, 183.09018171104526, 180.32597049492233, 445.55999431923885, 239.34524948176343, 322.68621837649334, 304.9184058283066, 126.00437160134243, 143.18506900161557, 129.67276148634267, 201.29591151636725, 116.50169059186399, 98.65902992433331, 102.2248055880729, 108.58938765223347, 81.29560707919487, 87.37377751412778, 108.24059489657759, 133.0543235665029, 107.80264509060713, 85.45737186114809, 260.1709300835631, 106.20509605174439, 119.5533753367159, 125.84623986668716, 88.44805091391228, 129.0984617384842, 89.34507288616703, 105.43373755377456, 163.56058432872015, 124.27795072943675, 113.9814029031562, 106.00854179636471, 116.2543023543706, 237.81863634496565, 148.7915012374058, 475.7326039612106, 462.65408003634303, 175.78612495488346, 254.2936935777338, 168.01984365191544, 135.76039341960663, 143.7386793822553, 174.80221404800216, 154.5664445032696, 210.50317657881354, 151.64757348537543, 129.9959848092093, 136.15570549345307, 143.09694098356928, 143.3020827343525, 138.96696283671568, 133.08125176309633, 132.61976826408892, 262.68176154151666, 283.04882680157397, 381.0543851933318, 224.31003876341583, 90.85032076763324, 232.69586083910622, 116.70983633668227, 183.97706663957305, 271.2960921877631, 105.28429102358577, 211.7449565566653, 155.77196069977, 121.30449594130417, 255.45266204731277, 89.48477682613786, 60.48422285320639, 59.43434055761938, 63.258442004256686, 72.10658278096852, 121.05560887584838, 75.22491971083738, 295.065089535521, 54.09331199341331, 352.3363940039115, 132.85696394762928, 83.86046497775371, 95.54116799730389, 39.61930838513282, 73.72049203900022, 160.34121027944488, 293.8795807572458, 144.2187355561438, 316.2678497058775, 250.0491510426806, 292.9365550983844, 350.183578808984, 218.78998527972306, 300.06139700342777, 369.7369654475547, 217.1319979698714, 292.4614385393989, 145.21866861020646, 136.34190772002947, 157.14178385196138, 147.98623406834608, 129.41742453909114, 133.9785402787909], \"Total\": [3383.0, 2288.0, 2272.0, 1772.0, 1286.0, 1553.0, 1471.0, 1307.0, 5071.0, 5580.0, 791.0, 1906.0, 891.0, 3714.0, 3367.0, 1597.0, 660.0, 755.0, 631.0, 1002.0, 1260.0, 860.0, 771.0, 1018.0, 552.0, 5144.0, 606.0, 628.0, 446.0, 1582.0, 1471.671288306797, 90.70119462526893, 492.8917607809365, 89.08567744852276, 1002.0077701778774, 103.4159212726381, 113.12184622707403, 299.67716103691185, 229.4368635920514, 1018.2112830027021, 170.06027013058429, 120.26226558622099, 92.81700476420643, 93.13475899874011, 148.75684264993274, 234.20100222934013, 119.14240344273279, 281.80224039106895, 1597.0596020148978, 567.2524315057905, 142.20761431414164, 156.63434915090392, 189.9709190392631, 94.69727260657784, 124.31699843675763, 638.1579512117553, 577.235910114744, 697.9135347776515, 249.97002544355297, 114.70931031789297, 1090.093326668659, 837.010787297363, 901.1338844813517, 1913.1020717991194, 610.8705772172189, 384.1950037770988, 5580.5602616563865, 5071.989881905495, 629.094716460366, 5144.784738050827, 566.2713043690295, 2387.1865997497484, 535.9185521440919, 611.2796160792883, 1474.9700694139083, 323.4643248850345, 3182.3925616980355, 753.1599239961716, 3714.255646018388, 1739.1405059452127, 1987.2793965386445, 3219.8152941754747, 1785.7310324012637, 1582.9109372797054, 2272.1400811050776, 2288.6816675553428, 1307.457042278385, 771.0223046789122, 380.11026757500395, 337.2064773502361, 285.8544491168654, 328.76070339246854, 262.13675262639686, 219.2213387453352, 187.5146137040242, 186.16538122704043, 179.16249560455117, 176.98368531208956, 159.7742558304148, 152.07597746449272, 146.63420803829177, 141.23996421709353, 138.37649357044322, 132.53834106855268, 124.79238107149489, 118.88720171504661, 110.18465311822607, 106.9667828811488, 103.91879717865089, 98.30253295267289, 98.0319709216656, 3383.088863413749, 93.89656737046822, 108.7629753294273, 1553.8702974657767, 323.322101707299, 103.01225085751129, 224.55686298866743, 329.17341501708984, 539.5779865934094, 286.3833825076059, 632.5440352139525, 407.58925765922464, 1426.7227520637734, 722.551364898798, 487.9521588028597, 235.2843022223103, 231.4262199489648, 703.5695445125806, 952.3077075134916, 861.2047410631554, 955.8122057558829, 508.5050479174235, 607.9378797520947, 419.6539505147305, 1272.3323397507213, 1511.7404776524497, 881.7391397180522, 1906.921842391643, 1260.7446585570026, 5144.784738050827, 1532.2486541064036, 821.2214247066221, 3367.985836652164, 891.571663015008, 100.48794184439164, 83.67258484892058, 471.4761790338921, 1772.6266795081265, 550.661986786692, 114.49540396357065, 133.5589677762806, 133.73978412150308, 115.61726696178218, 222.37404385703587, 242.50547298273938, 223.28560883097455, 133.37865970743874, 691.2736112551424, 208.6398741160282, 171.61616076656918, 201.43737162159888, 106.73149123983336, 522.5854577163614, 347.3322698802248, 486.72280759273514, 127.43205148631523, 104.03138353309679, 338.9058136903274, 225.95355438470426, 378.6441607973127, 212.6640896156938, 133.33173484485437, 111.63773432441124, 284.85341719486917, 1241.9129941512665, 1021.0602664717563, 906.9804303729168, 722.0124527228388, 372.2978295513899, 507.399408349865, 463.2176690914673, 3219.8152941754747, 441.6559898863318, 322.8945271242935, 955.8122057558829, 912.5434675356977, 1785.7310324012637, 1987.2793965386445, 735.426371057528, 765.6723778441336, 3182.3925616980355, 5580.5602616563865, 2387.1865997497484, 5144.784738050827, 1582.9109372797054, 901.2313143100444, 1602.5274741622695, 3367.985836652164, 208.9152067575528, 92.71589031694667, 103.82114767472778, 110.33073441831604, 95.95058803539754, 144.68940508014788, 262.3935226500709, 113.66281562575392, 89.62353023921266, 158.5092715560153, 140.7731450307833, 218.30739344179977, 292.2611106326283, 148.31610020731628, 93.14293039273147, 143.13161979003172, 654.3761223803848, 114.6999861770725, 199.250259845625, 485.630221579698, 189.5648974828912, 132.9398346859591, 350.0716618224063, 186.74114965674733, 149.28948674843707, 108.47912305591632, 277.4932766443114, 165.40723465611262, 156.39630022321418, 153.51084546514213, 157.22382263463132, 1906.921842391643, 395.1305646486907, 460.28697551956634, 443.154694052515, 399.4069720830534, 297.1093263775518, 405.48648510736626, 283.9694292460415, 474.69738671345607, 300.9721550235213, 583.5281845921573, 944.4192102762386, 1122.5584310278, 1532.2486541064036, 722.8029385659257, 837.3940318202956, 508.42796199137786, 520.1459938199849, 722.551364898798, 673.3009851235711, 580.365225594632, 864.3450475199335, 5580.5602616563865, 682.7652670012378, 3714.255646018388, 3182.3925616980355, 617.8339485281056, 835.4598618132458, 1137.6806917804306, 5144.784738050827, 660.0411910779992, 381.90106618962045, 631.9672411604558, 208.6225518903027, 507.937704084528, 200.68416624246282, 1286.585737362483, 159.39589366475016, 159.63462854881746, 118.36452813620518, 120.73127271062982, 131.6513674436606, 99.89126827465509, 111.25877683520558, 119.26563892761357, 121.16196941620645, 380.16577073634215, 174.20297821311416, 297.4315704272702, 204.42039867051668, 107.45051319105202, 392.4433082429077, 105.34751470189401, 177.16882265110178, 108.03133998775338, 108.28417410755523, 106.39830355184147, 376.36274852499724, 164.79646435520223, 137.91825420114978, 513.7975722888416, 557.3859210831628, 1260.7446585570026, 726.8509185862725, 673.3009851235711, 1582.9109372797054, 489.921945374927, 5071.989881905495, 307.9885470310326, 1776.134923967446, 3714.255646018388, 760.5504290199848, 940.5418807441691, 1078.5814295032608, 1785.7310324012637, 5580.5602616563865, 3182.3925616980355, 3219.8152941754747, 1913.1020717991194, 5144.784738050827, 473.76492781653405, 552.4528983667916, 182.63022620385033, 184.13010945875627, 90.75307117390528, 256.62191997136347, 377.12803685072515, 131.25290533755503, 119.61210733344222, 606.6439217065956, 182.0332342999425, 158.58463497180415, 153.38885375611426, 314.0895732933326, 434.5937286319717, 755.855282111409, 83.90057764985102, 169.41701362140213, 95.6459181427665, 423.11096558217747, 96.81212512972354, 344.9634670680808, 113.34887848572315, 223.63009695047788, 122.54061950734491, 76.70499277372488, 95.14325468861706, 90.86647849313671, 94.71667530423375, 94.22736083389256, 312.42155585184275, 420.1236479848365, 406.8709675443197, 230.3787125529228, 727.425967002491, 564.3351581482513, 222.94406683419956, 3367.985836652164, 3714.255646018388, 491.80599636881385, 363.60569053831176, 311.25622109727925, 1987.2793965386445, 1594.9993470877287, 1602.5274741622695, 3219.8152941754747, 1166.8049862948408, 1134.7561352509993, 5144.784738050827, 818.7069269611031, 1511.7404776524497, 940.5418807441691, 837.3940318202956, 791.0914640975433, 319.7714853028144, 361.71983067798277, 224.2748065391568, 200.51081379861992, 251.909823097397, 321.1938263769854, 143.91425077882388, 125.24367736827635, 114.2293876974436, 125.13417948016551, 139.42071364839305, 106.13334620904007, 101.05599916176793, 96.88572444843115, 221.83234311465995, 161.69281454713786, 125.53674575886747, 211.66828080449972, 104.77863226726255, 187.36372518020855, 119.9572891247922, 135.9025069325222, 108.99175473112525, 335.94453332898127, 207.86132204037142, 143.53160917854711, 492.4893687291544, 108.93549022476354, 145.22591738049698, 275.1722068309464, 252.2653408067203, 290.6304547127871, 921.5163956430297, 532.9942096113285, 266.6998129421819, 271.6295771185291, 227.09559613932393, 417.20743787727616, 421.0281628897465, 5144.784738050827, 5580.5602616563865, 325.02750935311394, 258.3240425017405, 3219.8152941754747, 1134.7561352509993, 5071.989881905495, 861.2047410631554, 671.1170374954801, 1107.9078476588331, 3182.3925616980355, 1594.9993470877287, 430.63909508083856, 383.8415630177434, 294.8464417933001, 255.31428061554635, 143.55938756869173, 136.49035153331607, 132.35240426647454, 120.02432133966293, 97.85644454044478, 96.41643922025922, 83.9548881417573, 90.92372475024492, 205.20744257933148, 100.79580277049769, 264.10169229557033, 206.10284861631322, 144.69397307550574, 158.90190463484635, 192.08982259335758, 320.24850214218225, 337.4344047214824, 444.7201811089788, 243.73010350126654, 340.8513192633727, 83.56036829555019, 384.0045117986892, 106.36645802247396, 320.28060584260146, 239.22580238317764, 101.15856355367083, 341.8971461912801, 521.1380188929041, 502.37991040520683, 502.0370831686962, 860.3180795046298, 520.7192743591153, 564.3351581482513, 479.5972600945532, 3367.985836652164, 1776.134923967446, 3714.255646018388, 1272.3323397507213, 1122.5584310278, 3182.3925616980355, 5071.989881905495, 446.4777905915869, 240.2605777321946, 324.23538329378204, 306.5727672584074, 126.91608784175656, 144.24258364158746, 130.69262453621997, 202.91601322126178, 117.44070184167838, 99.56848011875509, 103.16839435585983, 109.70654026709848, 82.21615137166181, 88.42826443559882, 110.93500908753259, 136.72132082893305, 113.96541638683827, 90.50276506985712, 276.85168928562734, 115.09673632671392, 130.34123218075558, 139.56854959647788, 100.18429962561856, 147.02374718828233, 101.94478316413657, 120.66836948213049, 189.67755339625165, 152.77958228079007, 140.41306354935261, 132.04729745089017, 163.33456078659617, 645.1760420276028, 271.69490096958083, 3367.985836652164, 3714.255646018388, 443.154694052515, 1594.9993470877287, 509.35845992133363, 273.60859734269496, 341.4064309767135, 821.2214247066221, 532.9942096113285, 5071.989881905495, 991.8969627008057, 377.5735213994527, 546.9627999956462, 881.7391397180522, 1511.7404776524497, 1602.5274741622695, 1785.7310324012637, 1597.0596020148978, 263.917774956707, 284.44057270465277, 383.6739660760199, 225.85638817769183, 91.9364713849308, 235.6956321343069, 118.4005482437842, 194.88892276263616, 293.218793890446, 115.03326821189935, 232.99005944440395, 176.6816479608971, 141.3439160794858, 331.6594299491828, 128.6344308593304, 87.98122463776548, 89.75630288011352, 99.2746507454012, 114.32300061348718, 202.0385578680893, 127.13006133839403, 518.0195298269748, 94.97641144287176, 628.6514831269905, 245.63835602969203, 163.5457217617997, 200.11568748091565, 89.16378049838877, 174.78150394579262, 382.9488933987449, 718.7776378788285, 347.36337287514004, 903.5755756508954, 698.4413500249486, 860.3180795046298, 1093.312528409118, 760.5504290199848, 1602.5274741622695, 3367.985836652164, 1122.5584310278, 3182.3925616980355, 690.1661120378996, 629.0669263342555, 1739.1405059452127, 1594.9993470877287, 460.53092746859585, 3714.255646018388], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9128, -6.7166, -5.0427, -6.7588, -4.3565, -6.6407, -6.5573, -5.5848, -5.8546, -4.3668, -6.1754, -6.5291, -6.8003, -6.812, -6.3597, -5.9168, -6.594, -5.7349, -4.0107, -5.0493, -6.4649, -6.3734, -6.1848, -6.8879, -6.6206, -4.9852, -5.0866, -4.8974, -5.9552, -6.7395, -4.4994, -4.7739, -4.7437, -4.0934, -5.1512, -5.5694, -3.2018, -3.3155, -5.1723, -3.4188, -5.2851, -4.1756, -5.369, -5.2764, -4.6344, -5.767, -4.127, -5.2079, -4.2749, -4.7501, -4.7987, -4.6146, -4.9661, -5.1012, -3.428, -3.4207, -3.9809, -4.5095, -5.218, -5.3382, -5.5037, -5.364, -5.5906, -5.7701, -5.9271, -5.9343, -5.9728, -5.9851, -6.0881, -6.1377, -6.1744, -6.212, -6.2327, -6.2761, -6.3367, -6.3856, -6.4622, -6.4921, -6.5212, -6.5773, -6.5801, -3.039, -6.6236, -6.4768, -3.8401, -5.4007, -6.532, -5.7948, -5.4375, -5.0032, -5.6171, -4.9611, -5.343, -4.3548, -4.9063, -5.2582, -5.837, -5.8548, -5.1013, -4.9902, -5.0698, -5.1643, -5.4823, -5.4128, -5.5884, -5.1583, -5.1274, -5.3482, -5.1506, -5.2755, -5.0468, -5.2876, -5.4077, -5.3425, -4.1685, -6.3557, -6.5408, -4.8139, -3.5039, -4.6747, -6.2494, -6.0961, -6.0954, -6.2439, -5.5915, -5.5271, -5.615, -6.1448, -4.5552, -5.7575, -5.9604, -5.8629, -6.5008, -4.9191, -5.3304, -5.0278, -6.3684, -6.5782, -5.4205, -5.8279, -5.3224, -5.9, -6.3874, -6.567, -5.688, -4.3865, -4.5838, -4.6925, -4.8887, -5.4816, -5.2593, -5.3308, -3.9823, -5.4185, -5.6185, -4.9951, -5.0282, -4.669, -4.6783, -5.2223, -5.252, -4.6741, -4.4653, -4.8976, -4.7439, -5.0703, -5.241, -5.2343, -5.2135, -5.4011, -6.222, -6.1283, -6.0707, -6.2181, -5.844, -5.2601, -6.1846, -6.4526, -5.8987, -6.0192, -5.5846, -5.3339, -6.0128, -6.4948, -6.088, -4.583, -6.3306, -5.7852, -4.8971, -5.846, -6.2043, -5.2536, -5.8831, -6.1217, -6.4485, -5.5148, -6.0396, -6.1351, -6.18, -6.1562, -3.7441, -5.3251, -5.2354, -5.2835, -5.4149, -5.6591, -5.4724, -5.7129, -5.4858, -5.7356, -5.4526, -5.2442, -5.1924, -5.0756, -5.3856, -5.3328, -5.5656, -5.5627, -5.4631, -5.513, -5.553, -5.4499, -5.0468, -5.5193, -5.2356, -5.3088, -5.55, -5.5297, -5.5419, -5.5372, -4.1388, -4.6868, -4.1834, -5.2934, -4.4037, -5.3324, -3.476, -5.5644, -5.563, -5.8633, -5.8436, -5.7573, -6.0344, -5.9273, -5.8582, -5.8455, -4.7092, -5.5085, -4.9981, -5.3838, -6.0331, -4.7473, -6.0774, -5.7266, -6.2442, -6.2646, -6.2922, -5.0583, -5.9425, -6.1557, -4.8435, -4.7831, -4.1088, -4.6343, -4.732, -4.1634, -5.0332, -3.6639, -5.3814, -4.3854, -4.1966, -5.1134, -5.0493, -5.0579, -4.9383, -4.6492, -4.9097, -5.1069, -5.2339, -5.3192, -4.4674, -4.3142, -5.4241, -5.4174, -6.1297, -5.1, -4.7171, -5.7742, -5.8694, -4.2575, -5.4625, -5.615, -5.6492, -4.9441, -4.6259, -4.0762, -6.3001, -5.6084, -6.2047, -4.7326, -6.219, -4.9511, -6.0743, -5.3998, -6.0071, -6.5067, -6.292, -6.3401, -6.3054, -6.3578, -5.2576, -5.0446, -5.1918, -5.6326, -4.7838, -5.1232, -5.702, -4.0653, -4.0341, -5.229, -5.4193, -5.5042, -4.5867, -4.7405, -4.739, -4.4713, -4.9255, -5.0841, -4.8134, -5.3393, -5.3928, -5.4033, -5.4101, -3.8022, -4.7095, -4.5865, -5.0655, -5.1779, -4.9499, -4.7083, -5.5114, -5.6517, -5.744, -5.6529, -5.5452, -5.8184, -5.8676, -5.9101, -5.0818, -5.4001, -5.6568, -5.1397, -5.8498, -5.2697, -5.7202, -5.6168, -5.8501, -4.7311, -5.2116, -5.6425, -4.4252, -5.935, -5.6665, -5.0281, -5.138, -5.0828, -4.2331, -4.7556, -5.227, -5.2314, -5.3524, -4.9897, -5.0515, -3.7812, -3.7942, -5.2377, -5.3421, -4.4482, -4.8588, -4.5676, -5.0368, -5.1241, -5.0648, -4.9604, -5.1636, -4.1719, -4.287, -4.5515, -4.696, -5.2748, -5.3254, -5.3562, -5.4547, -5.6606, -5.6758, -5.8173, -5.7612, -4.9564, -5.677, -4.714, -4.965, -5.3222, -5.2314, -5.0426, -4.5488, -4.563, -4.2893, -4.9095, -4.6134, -6.0486, -4.5328, -5.8265, -4.7344, -5.0269, -5.8983, -4.8296, -4.5221, -4.5923, -4.6227, -4.374, -4.8935, -4.8697, -5.0069, -4.3085, -4.6294, -4.5705, -4.8875, -5.057, -5.0248, -5.04, -4.0646, -4.686, -4.3873, -4.4439, -5.3276, -5.1998, -5.2989, -4.8592, -5.406, -5.5723, -5.5368, -5.4764, -5.7659, -5.6938, -5.4796, -5.2732, -5.4836, -5.7159, -4.6026, -5.4986, -5.3802, -5.3289, -5.6815, -5.3034, -5.6714, -5.5059, -5.0668, -5.3414, -5.4279, -5.5004, -5.4082, -4.6924, -5.1614, -3.9991, -4.027, -4.9947, -4.6255, -5.0399, -5.2531, -5.196, -5.0003, -5.1233, -4.8144, -5.1424, -5.2964, -5.2502, -5.2004, -5.199, -5.2297, -5.273, -5.2765, -4.3313, -4.2567, -3.9593, -4.4892, -5.3931, -4.4525, -5.1426, -4.6875, -4.2991, -5.2456, -4.5469, -4.8539, -5.104, -4.3592, -5.4082, -5.7999, -5.8174, -5.755, -5.6241, -5.106, -5.5818, -4.2151, -5.9116, -4.0377, -5.013, -5.4731, -5.3427, -6.223, -5.602, -4.825, -4.2191, -4.9309, -4.1457, -4.3806, -4.2223, -4.0438, -4.5142, -4.1983, -3.9895, -4.5218, -4.2239, -4.924, -4.9871, -4.8451, -4.9052, -5.0392, -5.0046], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.7823, 1.7651, 1.7463, 1.7409, 1.723, 1.7098, 1.7035, 1.7018, 1.6991, 1.6967, 1.6777, 1.6705, 1.6583, 1.6432, 1.6272, 1.6163, 1.6149, 1.6132, 1.6027, 1.5991, 1.5671, 1.5619, 1.5576, 1.5506, 1.5458, 1.5455, 1.5444, 1.5438, 1.5127, 1.5073, 1.4958, 1.4856, 1.4419, 1.3394, 1.4231, 1.4687, 1.1605, 1.1423, 1.3727, 1.0247, 1.3651, 1.0357, 1.3363, 1.2972, 1.0585, 1.4432, 0.7969, 1.1571, 0.4945, 0.778, 0.5961, 0.2976, 0.5356, 0.521, 1.8328, 1.8328, 1.8325, 1.832, 1.8308, 1.8303, 1.83, 1.8299, 1.8297, 1.8291, 1.8283, 1.8283, 1.8281, 1.8281, 1.8274, 1.8272, 1.8269, 1.8268, 1.8266, 1.8263, 1.8259, 1.8255, 1.8249, 1.8247, 1.8244, 1.8239, 1.8239, 1.8237, 1.8235, 1.8233, 1.8007, 1.8099, 1.8224, 1.7803, 1.7552, 1.6952, 1.7148, 1.5784, 1.636, 1.3713, 1.5002, 1.5408, 1.6915, 1.6902, 1.3318, 1.1401, 1.1611, 0.9624, 1.2755, 1.1664, 1.3615, 0.6823, 0.5408, 0.8591, 0.2854, 0.5743, -0.6032, 0.3672, 0.8708, -0.4753, 2.0278, 2.0236, 2.0215, 2.0195, 2.0051, 2.0034, 1.9993, 1.9986, 1.9979, 1.995, 1.9934, 1.9711, 1.9658, 1.9513, 1.8956, 1.8911, 1.8836, 1.8209, 1.8181, 1.8113, 1.8086, 1.7738, 1.7733, 1.7664, 1.743, 1.741, 1.7303, 1.7296, 1.709, 1.707, 1.6493, 1.4783, 1.4769, 1.4866, 1.5185, 1.5879, 1.5007, 1.5203, 0.9299, 1.4802, 1.5934, 1.1316, 1.1448, 0.8327, 0.7164, 1.1665, 1.0965, 0.2498, -0.103, 0.3138, -0.3004, 0.5519, 0.9445, 0.3756, -0.3463, 2.2462, 2.2377, 2.2183, 2.215, 2.2073, 2.1707, 2.1593, 2.0714, 2.0411, 2.0247, 2.0229, 2.0187, 1.9777, 1.9771, 1.9603, 1.9375, 1.9226, 1.9163, 1.9094, 1.9067, 1.8985, 1.895, 1.8775, 1.8764, 1.8617, 1.8542, 1.8486, 1.8412, 1.8017, 1.7754, 1.7754, 1.6919, 1.6849, 1.622, 1.6118, 1.5844, 1.6361, 1.5117, 1.6274, 1.3408, 1.5466, 1.1676, 0.8945, 0.7735, 0.5791, 1.0206, 0.9261, 1.1923, 1.1724, 0.9434, 0.964, 1.0726, 0.7773, -0.6846, 0.9438, -0.4663, -0.385, 1.013, 0.7315, 0.4106, -1.0936, 2.3582, 2.3573, 2.357, 2.3553, 2.3552, 2.3552, 2.3535, 2.3535, 2.3534, 2.3521, 2.3521, 2.3518, 2.3507, 2.3501, 2.3497, 2.3466, 2.3395, 2.3205, 2.296, 2.2853, 2.2791, 2.2696, 2.2545, 2.0856, 2.0626, 2.0399, 2.0299, 2.0004, 1.942, 1.9069, 1.9039, 1.8829, 1.741, 1.7663, 1.7451, 1.4588, 1.7618, 0.7938, 1.8778, 1.1216, 0.5728, 1.2418, 1.0935, 0.9479, 0.5634, -0.287, 0.0142, -0.1947, 0.1988, -0.8757, 2.3611, 2.3607, 2.3577, 2.3562, 2.3514, 2.3417, 2.3396, 2.3379, 2.3356, 2.3238, 2.3226, 2.308, 2.307, 2.2955, 2.2889, 2.2852, 2.2595, 2.2485, 2.2239, 2.209, 2.1975, 2.1947, 2.1845, 2.1795, 2.1737, 2.1425, 2.1418, 2.1398, 2.133, 2.0858, 1.9873, 1.9041, 1.7889, 1.9169, 1.616, 1.5304, 1.8803, 0.8019, 0.7353, 1.5622, 1.6739, 1.7444, 0.808, 0.8742, 0.871, 0.4409, 1.0017, 0.871, -0.3699, 0.9422, 0.2754, 0.7394, 0.8488, 2.5136, 2.5122, 2.5119, 2.5109, 2.5105, 2.5103, 2.5089, 2.5086, 2.5073, 2.5071, 2.5069, 2.5065, 2.5061, 2.5059, 2.5056, 2.5055, 2.5035, 2.4999, 2.4945, 2.4876, 2.4865, 2.4819, 2.4605, 2.4479, 2.4412, 2.4408, 2.3802, 2.3646, 2.3635, 2.3445, 2.3437, 2.3208, 2.2344, 1.9301, 1.9551, 2.1761, 2.1535, 2.2114, 1.9659, 1.8951, 0.6623, 0.568, 1.9676, 2.0929, 0.464, 1.0963, -0.1099, 1.1941, 1.3562, 0.9142, -0.0365, 0.451, 2.7521, 2.7521, 2.7513, 2.7508, 2.7477, 2.7476, 2.7476, 2.7469, 2.7452, 2.7448, 2.7417, 2.718, 2.7089, 2.6992, 2.6989, 2.6958, 2.6924, 2.6896, 2.6887, 2.6713, 2.6049, 2.6025, 2.5837, 2.5444, 2.5151, 2.5058, 2.4958, 2.4857, 2.4849, 2.4743, 2.3252, 2.2111, 2.1776, 2.1479, 1.8579, 1.8405, 1.7839, 1.8094, 0.5587, 0.8776, 0.1988, 0.9531, 0.9089, -0.101, -0.5823, 2.8232, 2.8215, 2.8205, 2.8199, 2.8181, 2.8179, 2.8175, 2.8173, 2.8173, 2.8161, 2.8161, 2.8151, 2.814, 2.8133, 2.8007, 2.7981, 2.7697, 2.7679, 2.7632, 2.7449, 2.7389, 2.7218, 2.7007, 2.6953, 2.6934, 2.6903, 2.6772, 2.6188, 2.6167, 2.6057, 2.4853, 1.8273, 2.2232, 0.8681, 0.7423, 1.9006, 0.9892, 1.7162, 2.1245, 1.9602, 1.2782, 1.5874, -0.3567, 0.9472, 1.759, 1.4347, 1.0069, 0.4692, 0.3802, 0.2287, 0.3369, 3.0823, 3.0821, 3.0801, 3.0801, 3.0751, 3.0742, 3.0726, 3.0294, 3.0093, 2.9984, 2.9914, 2.961, 2.9341, 2.8259, 2.7241, 2.7122, 2.6747, 2.6363, 2.6261, 2.5748, 2.5622, 2.5242, 2.5241, 2.508, 2.4724, 2.419, 2.3476, 2.2758, 2.2237, 2.2164, 2.1926, 2.2079, 2.0372, 2.0598, 2.0096, 1.9485, 1.841, 1.4116, 0.8777, 1.4441, 0.6999, 1.5283, 1.5579, 0.683, 0.7095, 1.8176, -0.2353]}, \"token.table\": {\"Topic\": [1, 2, 3, 6, 7, 8, 9, 10, 2, 3, 4, 5, 8, 9, 10, 8, 2, 5, 7, 8, 9, 8, 2, 8, 9, 1, 7, 8, 9, 2, 6, 8, 9, 8, 1, 2, 3, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 2, 1, 2, 6, 1, 2, 5, 6, 1, 3, 4, 5, 8, 8, 10, 9, 10, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 3, 4, 5, 6, 9, 9, 9, 9, 8, 1, 2, 3, 4, 5, 7, 3, 8, 3, 5, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 8, 10, 1, 2, 3, 4, 5, 6, 8, 10, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 8, 2, 3, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 8, 10, 5, 8, 1, 3, 4, 5, 6, 9, 1, 3, 4, 5, 6, 8, 10, 1, 8, 10, 1, 2, 4, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 5, 8, 10, 2, 4, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 6, 9, 10, 1, 2, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 1, 2, 8, 10, 2, 8, 10, 2, 7, 9, 2, 3, 4, 5, 6, 8, 9, 10, 2, 8, 4, 5, 1, 3, 4, 6, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 7, 2, 7, 9, 9, 2, 4, 7, 8, 2, 5, 6, 8, 9, 7, 10, 7, 6, 7, 2, 2, 2, 3, 4, 6, 2, 6, 8, 10, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 9, 1, 6, 8, 10, 6, 8, 8, 10, 4, 10, 1, 2, 3, 5, 7, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 1, 3, 4, 5, 6, 7, 8, 7, 3, 4, 7, 3, 4, 7, 8, 4, 10, 1, 3, 5, 6, 7, 8, 9, 10, 4, 7, 1, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 2, 4, 1, 3, 4, 5, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 6, 9, 2, 4, 5, 8, 9, 1, 2, 3, 6, 7, 8, 9, 10, 2, 2, 3, 4, 5, 7, 3, 7, 3, 5, 6, 8, 6, 1, 5, 6, 8, 2, 2, 2, 8, 10, 1, 2, 3, 5, 7, 8, 10, 6, 9, 1, 2, 3, 4, 9, 1, 3, 6, 9, 10, 1, 2, 3, 4, 5, 7, 2, 3, 4, 10, 4, 5, 6, 8, 4, 5, 1, 2, 3, 6, 8, 1, 6, 7, 8, 9, 2, 6, 8, 1, 3, 4, 5, 9, 1, 3, 6, 9, 1, 3, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 7, 8, 7, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 2, 4, 9, 10, 5, 6, 1, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 6, 7, 9, 9, 1, 3, 5, 7, 1, 2, 3, 4, 5, 7, 8, 10, 1, 2, 4, 9, 1, 4, 5, 7, 8, 1, 4, 7, 8, 5, 1, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 7, 10, 1, 3, 4, 7, 2, 5, 6, 8, 4, 5, 7, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 4, 5, 2, 3, 4, 5, 7, 8, 9, 10, 7, 1, 2, 4, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 5, 2, 2, 3, 4, 5, 6, 2, 5, 6, 2, 3, 4, 5, 6, 9, 6, 2, 9, 2, 1, 6, 2, 7, 2, 7, 1, 2, 3, 5, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 2, 3, 5, 6, 7, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 1, 3, 4, 5, 6, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 10, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 6, 10, 5, 7, 10, 1, 2, 3, 4, 6, 7, 8, 2, 1, 3, 6, 1, 2, 3, 4, 5, 6, 7, 10, 3, 4, 7, 2, 1, 2, 8, 2, 8, 4, 6, 9, 2, 4, 9, 10, 8, 9, 2, 9, 2, 5, 9, 1, 2, 3, 4, 5, 6, 8, 9, 2, 3, 2, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 5, 6, 8, 5, 7, 3, 4, 3, 4, 7, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 9, 1, 6, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 9, 10, 1, 3, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 5, 6, 8, 9, 10, 1, 3, 4, 5, 6, 8, 10, 6, 7, 5, 7, 5, 6, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 4, 5, 7, 2, 4, 7, 2, 3, 4, 7, 8, 4, 5, 9, 3, 9, 2, 5, 6, 2, 3, 5, 2, 3, 4, 5, 6, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 6, 1, 3, 4, 5, 6, 8, 10, 2, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 8, 5, 7, 3, 4, 6, 4, 10, 4, 7, 1, 3, 4, 5, 10, 1, 3, 4, 5, 6, 8, 9, 10, 2, 3, 4, 5, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 7, 6, 8, 6, 8, 9, 10, 2, 4, 10, 5, 8, 10, 1, 3, 4, 6, 7, 10, 1, 3, 6, 7, 1, 4, 5, 8, 2, 4, 1, 2, 4, 6, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 6, 8, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 3, 9, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 4, 7, 8, 9, 1, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 7, 2, 4, 9, 2, 3, 4, 10, 1, 2, 3, 8, 1, 3, 5, 1, 3, 5, 1, 2, 3, 4, 6, 10, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 10, 2, 4, 6, 7, 8, 4, 9, 7, 1, 2, 3, 6, 7, 9, 1, 4, 5, 6, 7, 9, 1, 3, 5, 6, 7, 8, 9, 1, 3, 4, 5, 7, 8, 9, 1, 3, 4, 5, 6, 7, 9, 2, 3, 4, 5, 9, 3, 4, 5, 10, 2, 1, 3, 4, 5, 2, 3, 4, 5, 6, 8, 9, 10, 2, 1, 3, 9, 1, 7, 9, 1, 2, 4, 2, 4, 5, 8, 10, 7, 2, 4, 9, 10, 3, 4, 5, 8, 10, 2, 4, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 9, 1, 3, 4, 6, 8, 10, 1, 3, 6, 8, 9, 10, 2, 4, 8, 9, 6, 8, 1, 3, 5, 6, 7, 9, 10, 4, 7, 4, 7, 1, 4, 8, 1, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 2, 6, 10, 5, 8, 10, 2, 3, 4, 6, 5, 1, 3, 4, 6, 9, 2, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 10, 1, 3, 4, 5, 6, 8, 9, 10, 2, 4, 7, 1, 2, 3, 7, 8, 1, 3, 9, 6, 1, 2, 3, 4, 5, 7, 8, 9, 10, 2, 4, 5, 7, 10, 5, 6, 9, 8, 10, 1, 3, 4, 5, 6, 7, 8, 10, 2, 3, 4, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 5, 6, 1, 2, 4, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 8, 10, 4, 3, 7, 2, 5, 6, 4, 5, 9, 1, 2, 3, 6, 8, 9, 10, 2, 6, 7, 8, 9, 1, 2, 3, 5, 6, 7, 8, 10, 3, 4, 8, 10, 8, 10, 3, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 6, 8, 10, 3, 4, 8, 2, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 7, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 6, 9, 2, 7, 9, 2, 7, 9, 2, 3, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 8, 9, 3, 5, 8, 10, 2, 3, 4, 5, 9, 10, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 8, 10, 4, 8, 9, 10, 2, 7, 9, 2, 6, 8, 9, 1, 6, 8, 9, 2, 4, 10, 5, 2, 4, 9, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 8, 10, 9, 2, 4, 5, 6, 8, 9, 10, 1, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 7, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 3, 6, 8, 2, 7], \"Freq\": [0.002171406827108677, 0.16936973251447682, 0.056456577504825606, 0.24319756463617184, 0.04342813654217354, 0.20411224174821566, 0.002171406827108677, 0.28011148069701935, 0.1880428472230863, 0.06356377934301509, 0.34960078638658304, 0.05032132531322028, 0.002648490805958962, 0.3443038047746651, 0.002648490805958962, 0.9914657185457925, 0.15842684756714903, 0.00293383051050276, 0.02053681357351932, 0.8097372208987618, 0.00586766102100552, 0.989781792979358, 0.03396362567036322, 0.9412776257214949, 0.01940778609735041, 0.006293190772621521, 0.012586381545243042, 0.9376854251206067, 0.04405233540835065, 0.031235387273492534, 0.015617693636746267, 0.9370616182047761, 0.010411795757830845, 0.9961034413829324, 0.18613111399176938, 0.02494540702982476, 0.026864284493657433, 0.007675509855330696, 0.0729173436256416, 0.5814198715413001, 0.07099846616180894, 0.02878316195749011, 0.03730182473642314, 0.10601571240878158, 0.005889761800487865, 0.05889761800487865, 0.06675063373886247, 0.06871388767235842, 0.013742777534471685, 0.10208920454178966, 0.32982666082732043, 0.21006817088406718, 0.4779758007429518, 0.031865053382863454, 0.18237658212745253, 0.004067879155259164, 0.07457778451308468, 0.10237495874068897, 0.03728889225654234, 0.01762747633945638, 0.03254303324207331, 0.03932283183417192, 0.46707701866158546, 0.0477549597555343, 0.17929055066112878, 0.016337223074261734, 0.05655192602629062, 0.07163243963330145, 0.05948424811654272, 0.030579930369771965, 0.03267444614852347, 0.03895799348477798, 0.9978075250342171, 0.9909618401609498, 0.007954858050470165, 0.013258096750783608, 0.9757959208576735, 0.03301547556095282, 0.06603095112190564, 0.09904642668285846, 0.8033765719831852, 0.049643221213117546, 0.809184505773816, 0.009928644242623508, 0.12410805303279386, 0.004964322121311754, 0.07823823611984464, 0.9127794213981875, 0.87015346648526, 0.11602046219803466, 0.6765379277523531, 0.31482458024119403, 0.09656420300592777, 0.02260013261840863, 0.7725136240474222, 0.05547305279063936, 0.02260013261840863, 0.014381902575350946, 0.014381902575350946, 0.18891617811942332, 0.644173197521968, 0.1331704206415607, 0.01858191915928754, 0.0061939730530958465, 0.0061939730530958465, 0.9913854590633581, 0.9886748808765049, 0.9942905614499984, 0.9853091523425437, 0.0005641345758586252, 0.0005641345758586252, 0.9725680087802697, 0.010154422365455253, 0.0005641345758586252, 0.015795768124041504, 0.9694712290400513, 0.026201925109190574, 0.9715578936579857, 0.02723994094368184, 0.9948723549607352, 0.004486459323385502, 0.9939429719060235, 0.10970127956554308, 0.037709814850655436, 0.1611237543618914, 0.05485063978277154, 0.16798008433473785, 0.2562553327351358, 0.037709814850655436, 0.01114153620587547, 0.0959886196198502, 0.06770625848185863, 0.9996504341912538, 0.10753805060843133, 0.005659897400443754, 0.8829439944692257, 0.004455659602888312, 0.04158615629362424, 0.01039653907340606, 0.2762508953790753, 0.5406200318171152, 0.013366978808664935, 0.10693583046931948, 0.007426099338147187, 0.7515331513068064, 0.1610428181371728, 0.08588950300649216, 0.36796214712593067, 0.0062845797971977915, 0.16811250957504092, 0.07164420968805482, 0.09583984190726633, 0.02985175403668951, 0.07792878948525261, 0.05750390514435979, 0.0329940439352884, 0.09175486503908775, 0.04439112192662572, 0.007398520321104287, 0.019729387522944765, 0.47843764743141054, 0.11591015169730048, 0.03699260160552143, 0.07151902977067477, 0.1603012736239262, 0.06412050944957048, 0.9971292114357837, 0.025947681335450213, 0.9600642094116579, 0.9984225362112648, 0.011006383558763365, 0.2902933663623838, 0.012382181503608787, 0.02201276711752673, 0.5516949758830137, 0.004127393834536262, 0.06741409929742562, 0.006878989724227104, 0.0330191506762901, 0.005552274167881003, 0.28395916458591414, 0.003172728095932001, 0.021415914647541008, 0.5385705942844572, 0.09994093502185804, 0.04759092143898002, 0.21623357990486414, 0.77091971966082, 0.15932477848393162, 0.7479413212162345, 0.008851376582440645, 0.05310825949464387, 0.030979818038542255, 0.004425688291220323, 0.14576979148675376, 0.7382534601103335, 0.014106754014847138, 0.03291575936797665, 0.00470225133828238, 0.00470225133828238, 0.06112926739767093, 0.025655639782512018, 0.025655639782512018, 0.9441275439964423, 0.04573283623316895, 0.026677487802681886, 0.9108456549772815, 0.011433209058292237, 0.9976405769205864, 0.30091390321943157, 0.04981685019853466, 0.2682058702607977, 0.012076812169341734, 0.0538424542549819, 0.21134421296348035, 0.0015096015211677168, 0.04025604056447245, 0.021134421296348033, 0.04025604056447245, 0.03722643923429024, 0.03722643923429024, 0.9213543710486835, 0.002963538945666782, 0.8623898331890334, 0.13335925255500516, 0.19664900446681255, 0.20451496464548505, 0.5899470134004376, 0.07337397683491655, 0.00611449806957638, 0.11006096525237483, 0.018343494208729137, 0.16509144787856225, 0.018343494208729137, 0.05503048262618741, 0.042801486487034655, 0.5136178378444158, 0.130633054110328, 0.06123424411421625, 0.5490670555574724, 0.23269012763402175, 0.00408228294094775, 0.02041141470473875, 0.12013433732090018, 0.009740621944937852, 0.042209361761397356, 0.6169060565127306, 0.15584995111900563, 0.05519685768798116, 0.02350874933046654, 0.12929812131756596, 0.39181248884110903, 0.13452228783544742, 0.1776216616079694, 0.013060416294703633, 0.04440541540199235, 0.0783624977682218, 0.0013060416294703634, 0.005224166517881454, 0.03612042254755043, 0.02167225352853026, 0.9391309862363112, 0.03158679038746897, 0.042115720516625293, 0.3474546942621587, 0.5685622269744415, 0.10883555752899358, 0.1943492098732028, 0.6918831871486021, 0.16363443090224633, 0.01963613170826956, 0.8116267772751418, 0.08787180696677101, 0.0351487227867084, 0.3251256857770527, 0.0205034216255799, 0.0146453011611285, 0.0702974455734168, 0.42178467344050086, 0.023432481857805602, 0.021996459180414436, 0.9678442039382351, 0.03444257992340291, 0.9586518078680476, 0.6875433449639736, 0.054021262818597926, 0.0016370079641999373, 0.04747323096179818, 0.0032740159283998747, 0.07202835042479724, 0.13423465306439486, 0.7814375003959705, 0.2111993244313434, 0.18742514978806998, 0.06825977677466746, 0.03355141570280265, 0.22907518307430777, 0.13073482670402412, 0.1399903896565214, 0.11222370079902956, 0.02660974348842969, 0.07057366751279179, 0.10874017595320859, 0.03954188216480312, 0.09885470541200782, 0.7512957611312594, 0.2308587865402769, 0.17986588188278602, 0.0324500302365851, 0.01576144325776991, 0.24383879863491095, 0.15854157629874438, 0.0018542874420905775, 0.1158929651306611, 0.020397161862996352, 0.9905149215083374, 0.0008867635823709377, 0.008572047962919065, 0.9679057527857308, 0.0315341634883649, 0.9947532893490346, 0.9935597251961616, 0.8670361688951477, 0.051002127582067514, 0.004250177298505626, 0.07225301407459564, 0.006122402969611216, 0.024489611878444863, 0.18979449205794768, 0.06122402969611215, 0.7101987444749011, 0.8245286464436058, 0.17441952136307046, 0.9943158727508066, 0.014173120264395444, 0.9779452982432856, 0.9937400755212422, 0.9912208685129116, 0.44018822847192973, 0.010564517483326314, 0.5352688858218666, 0.014086023311101752, 0.003860858297500918, 0.22392978125505325, 0.2026950606187982, 0.5694765988813855, 0.8219058616115081, 0.14720701999012084, 0.02944140399802417, 0.016224370826211046, 0.003744085575279472, 0.19094836433925308, 0.10608242463291837, 0.08673798249397444, 0.22464513451676832, 0.033696770177515246, 0.0642734690422976, 0.08673798249397444, 0.1872042787639736, 0.2719252782405274, 0.05357735680184649, 0.06596207244448438, 0.1674628941243644, 0.19627081964093512, 0.006461590770071938, 0.07753908924086327, 0.12465485527263781, 0.03607721513290166, 0.6729405432450964, 0.05531018163658327, 0.2581141809707219, 0.004945240350485132, 0.9610250414442774, 0.009890480700970265, 0.02142937485210224, 0.9780925964080122, 0.01948391626310781, 0.00844590683770333, 0.9881711000112896, 0.0052127592092180855, 0.9930306293560452, 0.8726849159351079, 0.04309555140420286, 0.08619110280840572, 0.3406574128670469, 0.6542170769833061, 0.9936472533201111, 0.08983344577282773, 0.12026090321201133, 0.010142485813061196, 0.08983344577282773, 0.23617502678985355, 0.018836045081399362, 0.1927072304481627, 0.03187638398390662, 0.21009434898483906, 0.6069235587791595, 0.09324701576930483, 0.04253372649126185, 0.06707241485160523, 0.024538688360343376, 0.07852380275309881, 0.022902775802987153, 0.045805551605974305, 0.017995038130918475, 0.007867387907488522, 0.28715965862333104, 0.11211027768171143, 0.346165067929495, 0.011801081861232783, 0.029502704653081958, 0.12981190047356062, 0.013767928838104914, 0.007867387907488522, 0.05310486837554752, 0.0950262537526735, 0.40568900640564454, 0.4970604042447537, 0.050161429262271726, 0.7494707666245305, 0.005901344619090792, 0.02655605078590856, 0.02655605078590856, 0.002950672309545396, 0.141632270858179, 0.9975873855603984, 0.23193350543158683, 0.06994820005079602, 0.6958005162947605, 0.017613727734050757, 0.23778532440968522, 0.7397765648301318, 0.9948523027682677, 0.00848551999835252, 0.9885630798080686, 0.0916326045096183, 0.0916326045096183, 0.0071587972273139296, 0.2433991057286736, 0.010022316118239501, 0.1632205767827576, 0.03436222669110686, 0.3579398613656965, 0.7197695390942315, 0.278127507675478, 0.09638361570490145, 0.04176623347212396, 0.006425574380326763, 0.539748247947448, 0.1895544442196395, 0.067468530993431, 0.06104295661310424, 0.008017400031248009, 0.06592084470137252, 0.11313442266316635, 0.22805048977772116, 0.08730057811803388, 0.06770248915276096, 0.003563288902776893, 0.1576755339478775, 0.07482906695831475, 0.19330842297564646, 0.8869229693983873, 0.013967290856667517, 0.09427921328250573, 0.15836313662391385, 0.8358054432928788, 0.1339493837159036, 0.8017827396709086, 0.017222063620616176, 0.032530564616719444, 0.005740687873538725, 0.007654250498051634, 0.014746236646601648, 0.20644731305242306, 0.008426420940915228, 0.40236159992870213, 0.07373118323300824, 0.023172657587516875, 0.002106605235228807, 0.14114255076033005, 0.07373118323300824, 0.05266513088072017, 0.847019859762108, 0.02688951935752724, 0.10755807743010896, 0.01344475967876362, 0.00672237983938181, 0.14916766801986464, 0.12576881813439567, 0.06434683718503965, 0.6522429405574474, 0.00877456870705086, 0.15913424779031968, 0.019001104213769514, 0.07125414080163568, 0.11875690133605947, 0.5367811940389888, 0.0665038647481933, 0.007125414080163568, 0.019001104213769514, 0.9944287415069843, 0.9986740919520637, 0.29126838483950007, 0.030340456754114592, 0.6614219572396981, 0.018204274052468754, 0.7689987125332098, 0.22108712985329784, 0.02776971941975435, 0.7405258511934494, 0.02776971941975435, 0.18513146279836235, 0.991701975876252, 0.0031222621094061484, 0.009366786328218446, 0.22168060976783655, 0.7649542168045064, 0.9925374497654248, 0.9911585083392435, 0.990451542632748, 0.003515672853880604, 0.9949354176482109, 0.04074711631819144, 0.10066934619788473, 0.19175113561501853, 0.004793778390375463, 0.5776502960402433, 0.02876267034225278, 0.050334673098942365, 0.004928147286777162, 0.9905576046422095, 0.8814069773532099, 0.016630320327419054, 0.016630320327419054, 0.07483644147338575, 0.008315160163709527, 0.9291345114367007, 0.06187576767892099, 0.001995992505771645, 0.005987977517314934, 0.0009979962528858224, 0.07237581579542691, 0.5132103301857545, 0.13817201197308773, 0.13817201197308773, 0.0016449049044415208, 0.13652710706864624, 0.1685589087432517, 0.04719649444811048, 0.7618862675194977, 0.020227069049190204, 0.5418880718520737, 0.42745208152306435, 0.013463057685765806, 0.013463057685765806, 0.7126715925490821, 0.28607239982604, 0.04471670019538768, 0.013415010058616304, 0.04024503017584891, 0.8317306236342109, 0.06707505029308151, 0.08616087917876289, 0.03692609107661267, 0.012308697025537555, 0.8410942967450663, 0.016411596034050072, 0.0834320834978474, 0.9058340494052004, 0.011918869071121059, 0.046174799237362785, 0.055409759084835336, 0.046174799237362785, 0.729561827950332, 0.12005447801714324, 0.11770978984522454, 0.769037293655467, 0.054931235261104785, 0.04708391593808982, 0.7520901742775286, 0.012001438951237158, 0.1120134302115468, 0.0880105523090725, 0.008000959300824772, 0.0280033575528867, 0.12301988545215642, 0.029191159259833725, 0.0667226497367628, 0.11884971984360874, 0.0500419873025721, 0.022935910847012214, 0.12927513386497794, 0.3878254015949338, 0.054212152911119776, 0.020850828042738377, 0.17268893936639315, 0.04895119541094609, 0.4201644272772873, 0.016317065136982032, 0.13733529823626542, 0.03535364113012773, 0.06798777140409179, 0.06662801597600995, 0.031274374845882226, 0.16586183262415782, 0.10100560320060893, 0.0712355306783242, 0.041465458156039456, 0.28175247208590914, 0.19669512202223846, 0.02126433751591767, 0.11270098883436366, 0.007442518130571185, 0.9065674832873977, 0.017433990063219187, 0.03922647764224317, 0.004358497515804797, 0.008716995031609593, 0.021792487579023984, 0.36109790891143995, 0.05002471031097974, 0.1224742907613642, 0.06842460375869643, 0.0643996270670084, 0.057499667024114645, 0.09774943394099489, 0.0569246703538735, 0.03047482352278076, 0.09027447722785999, 0.06548700102958961, 0.9287247418741801, 0.002976681864981346, 0.9892375532932046, 0.9980099772892363, 0.9917806700413924, 0.6306928518293248, 0.026123372560977947, 0.09516371432927681, 0.05784461067073689, 0.08396798323171484, 0.013061686280488973, 0.05411270030488289, 0.0018659551829269963, 0.011195731097561978, 0.026123372560977947, 0.28920628960632755, 0.03076662655386463, 0.046149939830796946, 0.043073277175410485, 0.009229987966159389, 0.5784125792126551, 0.04543826428732073, 0.09087652857464146, 0.8027426690759994, 0.0605843523830943, 0.20059825726537847, 0.7918352260475465, 0.0679261703384144, 0.5751082421985753, 0.2332131848285561, 0.011321028389735734, 0.043019907880995786, 0.0633977589825201, 0.004528411355894294, 0.14252206139526163, 0.00644167509131126, 0.5741142925131161, 0.08454698557346028, 0.0853521949598742, 0.05233861011690399, 0.02898753791090067, 0.015298978341864243, 0.00966251263696689, 0.5979910061360448, 0.11873467630860095, 0.028064559854760224, 0.053970307413000435, 0.0626055565990805, 0.10362299023296083, 0.03238218444780026, 0.8368879643310322, 0.04269836552709348, 0.008539673105418696, 0.098206240712315, 0.012809509658128044, 0.9838483267231938, 0.9989298670579922, 0.7017775455784195, 0.006183062075580789, 0.009274593113371183, 0.2813293244389259, 0.0031386918705725423, 0.41849224940967233, 0.40593748192738216, 0.11717782983470824, 0.006277383741145085, 0.03871053307039469, 0.008369844988193447, 0.0010462306235241808, 0.951527368314937, 0.02231727303084074, 0.010144215014018519, 0.01623074402242963, 0.018735450626201997, 0.024980600834935997, 0.021858025730568997, 0.012490300417467999, 0.9211596557882649, 0.13794498616475562, 0.08778317301393539, 0.00836030219180337, 0.7649676505500084, 0.9965908309794793, 0.028645496763763673, 0.1874977969991804, 0.0026041360694330612, 0.7786366847604853, 0.0969525930595889, 0.16637543747262787, 0.20228380527247564, 0.21904104357907123, 0.111315940179528, 0.021545020679908645, 0.05625644288642813, 0.03590836779984775, 0.06702895322638246, 0.023938911866565164, 0.03742813762879839, 0.800386327754304, 0.07197718774768921, 0.07197718774768921, 0.0057581750198151364, 0.011516350039630273, 0.7916526654095297, 0.17876027928602284, 0.01276859137757306, 0.01276859137757306, 0.11673891641082045, 0.09551365888158037, 0.7534966422880228, 0.02122525752924008, 0.1649794932909788, 0.11998508602980276, 0.7124114483019539, 0.990858050001963, 0.988626175760666, 0.10074769638953687, 0.09609780271001978, 0.08679801535098561, 0.09454783815018075, 0.007749822799195143, 0.06044861783372212, 0.021699503837746402, 0.16119631422325897, 0.3688915652416888, 0.0015499645598390287, 0.0030841791227144495, 0.9961898566367672, 0.0015823605004647717, 0.9968871152928062, 0.21290856865470623, 0.04614767005323682, 0.5716018222503196, 0.015732160245421643, 0.0776119905440801, 0.07027031576288334, 0.0026220267075702736, 0.0031464320490843284, 0.9909362934661246, 0.830938416040814, 0.041966586668727976, 0.008393317333745596, 0.09232649067120155, 0.016786634667491192, 0.20325032469583748, 0.06024317103173022, 0.06362129277182724, 0.08501606379244171, 0.2899554493583277, 0.06925149567198895, 0.001689060870048511, 0.1531415188843983, 0.048982765231406815, 0.024209872470695323, 0.7403035870939635, 0.0926526174677699, 0.0018347052963914832, 0.03485940063143818, 0.11833849161725066, 0.004586763240978708, 0.007338821185565933, 0.7323681000328862, 0.06212584209088105, 0.057346931160813276, 0.009557821860135547, 0.04301019837060996, 0.028673465580406638, 0.05376274796326245, 0.002389455465033887, 0.009557821860135547, 0.997010894462181, 0.9950523305886713, 0.9994982346755121, 0.19737156795707714, 0.0187972921863883, 0.0375945843727766, 0.7142971030827554, 0.028195938279582448, 0.07593914315527911, 0.9017773249689394, 0.009492392894409889, 0.2591601870741306, 0.14619292604181724, 0.4950624086416084, 0.023257965506652743, 0.046515931013305485, 0.026580532007603134, 0.993862440737812, 0.9773535379467264, 0.021650236600085714, 0.9972556074758179, 0.069029988293745, 0.929603842355766, 0.02500890126717589, 0.9670108489974677, 0.004507908927793831, 0.9917399641146428, 0.833918139450845, 0.0035485878274504044, 0.14194351309801617, 0.007097175654900809, 0.010645763482351214, 0.6498651744503383, 0.10242440249489028, 0.024723131636697655, 0.00706375189619933, 0.01412750379239866, 0.1306794100796876, 0.047680325299345475, 0.00706375189619933, 0.017659379740498325, 0.03597542168699866, 0.9623425301272143, 0.06208767341453907, 0.05079900552098651, 0.7619850828147977, 0.022577335787105118, 0.022577335787105118, 0.08466500920164419, 0.9960244932156221, 0.08213169506256525, 0.005642635538649521, 0.11285271077299042, 0.07523514051532694, 0.01943574463312613, 0.22570542154598083, 0.12664581986746704, 0.09905960167851381, 0.15924771409077537, 0.0927900066355699, 0.014536351785183825, 0.0036340879462959563, 0.12355899017406251, 0.01817043973147978, 0.8431084035406619, 0.09075765550078187, 0.0632553356520601, 0.16501391909233068, 0.07975672756129315, 0.5032924532316085, 0.027502319848721778, 0.027502319848721778, 0.03850324778821049, 0.029949318017343792, 0.9658655060593373, 0.005282004074190901, 0.7394805703867261, 0.01848701425966815, 0.22448517315311328, 0.013205010185477251, 0.15363945768071754, 0.843567588397902, 0.13924396660859834, 0.18810149875196616, 0.08183636634014113, 0.08550068125089372, 0.05618616196487301, 0.24184478410967078, 0.05252184705412043, 0.08427924294730951, 0.021985889464515525, 0.046414655536199445, 0.9247802013732189, 0.02249107785886226, 0.0515971786173899, 0.9956192429849546, 0.03197422902286467, 0.014333275079215195, 0.5788438012759982, 0.037487027130255125, 0.15656346624988904, 0.08930732933972545, 0.035281907887298944, 0.014333275079215195, 0.015435834700693287, 0.025358871293996114, 0.0401655122299287, 0.006925088315504948, 0.5983276304596276, 0.0678658654919485, 0.14265681929940194, 0.10249130706947324, 0.0013850176631009896, 0.002770035326201979, 0.03878049456682771, 0.910522621715738, 0.02652007636065256, 0.03536010181420341, 0.017680050907101705, 0.9970158936094787, 0.9713726593819073, 0.021348849656745213, 0.00788333595620178, 0.1143083713649258, 0.5873085287370325, 0.00394166797810089, 0.059125019671513346, 0.1892000629488427, 0.0394166797810089, 0.9956635129755461, 0.9186206420725974, 0.04834845484592618, 0.01933938193837047, 0.14297494382313436, 0.62432392136102, 0.0023829157303855725, 0.06195580899002488, 0.05718997752925374, 0.004765831460771145, 0.06433872472041045, 0.04050956741655473, 0.006911356083370924, 0.9261217151717039, 0.06911356083370924, 0.9997021571129764, 0.00864206311817671, 0.8685273433767594, 0.12098888365447394, 0.948534803902873, 0.048985365459772785, 0.8145182387388327, 0.011157784092312776, 0.16736676138469164, 0.1642366988071017, 0.7595947319828453, 0.0547455662690339, 0.017107989459073093, 0.9964074271345372, 0.996247452248093, 0.018028573814979472, 0.9735429860088916, 0.04990801970652723, 0.05988962364783268, 0.8783811468348792, 0.040987624789011974, 0.745974771160018, 0.004098762478901198, 0.08402463081747455, 0.002049381239450599, 0.08607401205692515, 0.01844443115505539, 0.01639504991560479, 0.9944419435590633, 0.9951442746718085, 0.990052547691256, 0.028299767764680937, 0.0636744774705321, 0.028299767764680937, 0.021224825823510703, 0.8560679748815984, 0.0214881725462644, 0.1719053803701152, 0.641959154819649, 0.07789462548020845, 0.04566236666081185, 0.029546237251113553, 0.013430107841415252, 0.052500629412384404, 0.0975011689087139, 0.7200086319412718, 0.03000035966421966, 0.01500017983210983, 0.06750080924449424, 0.9912425995886314, 0.9893214880193466, 0.9645596547606435, 0.029908826504206, 0.9401849665315726, 0.03298894619409026, 0.0247417096455677, 0.1850363631129286, 0.19477511906624062, 0.06121503742081848, 0.016695010205677766, 0.016695010205677766, 0.08347505102838883, 0.011130006803785179, 0.022260013607570357, 0.4090277500391053, 0.02691133635175184, 0.09508672177618983, 0.06817538542443799, 0.012558623630817525, 0.620754825180409, 0.0017940890901167893, 0.12558623630817525, 0.02511724726163505, 0.02511724726163505, 0.042073971352852534, 0.17652774937175084, 0.15274680904187768, 0.06676956323387467, 0.041159319801703564, 0.041159319801703564, 0.07774538184766229, 0.04481792600629944, 0.037500713597107695, 0.32012804290213887, 0.9613629108608068, 0.016480507043328117, 0.016480507043328117, 0.1632111872814649, 0.8242164957713977, 0.9901734905403489, 0.07545232718867381, 0.10984971164233394, 0.3373162862552477, 0.11206889773611847, 0.04216453578190596, 0.12427442125193335, 0.0876578507044887, 0.05215087320393632, 0.035506977500552386, 0.02219186093784524, 0.7946128661604506, 0.11954352853741292, 0.014063944533813284, 0.06328775040215978, 0.9109803331538302, 0.06340156164806877, 0.0033369242972667774, 0.01334769718906711, 0.010010772891800333, 0.2233047346848262, 0.012733649683001216, 0.33200662222264143, 0.00559038278765907, 0.07764420538415376, 0.14628168294374566, 0.12857880411615863, 0.032921143082881195, 0.00683269007380553, 0.034163450369027654, 0.12829800972734612, 0.024484353001401928, 0.5739132343528612, 0.0019587482401121543, 0.0048968706002803855, 0.06659744016381325, 0.0812880519646544, 0.028401849481626235, 0.003917496480224309, 0.08618492256493479, 0.9965224966114634, 0.02913312578360158, 0.2519158523640842, 0.051411398441649846, 0.3376015164335006, 0.11139136329024132, 0.018850846095271608, 0.006854853125553313, 0.09425423047635804, 0.09596794375774638, 0.026360525589206638, 0.02108842047136531, 0.07908157676761991, 0.8646252393259777, 0.005272105117841328, 0.004772119497877628, 0.21315467090520074, 0.0031814129985850855, 0.04135836898160611, 0.0015907064992925427, 0.17338700842288715, 0.559928687750975, 0.04414929595801421, 0.9492098630973055, 0.11844080963972706, 0.8708883061744637, 0.9893880673512199, 0.9983854275155837, 0.9959382238813881, 0.5197171251078445, 0.021687740425592905, 0.03962941659585613, 0.023659353191555897, 0.2087937919154808, 0.03371457829796715, 0.07235818851084179, 0.03548902978733385, 0.04160102936181912, 0.0031545804255407863, 0.9886860446338535, 0.013448471506416987, 0.9380308875725848, 0.0437075323958552, 0.24092405044194481, 0.7083578918976839, 0.04736113812106608, 0.24548847565422427, 0.10123236109452548, 0.5669012221293427, 0.05314698957462587, 0.030369708328357642, 0.017376687331279266, 0.0521300619938378, 0.9209644285578011, 0.9919617058546117, 0.9927819407504999, 0.08364183391557545, 0.041820916957787724, 0.8677840268740952, 0.05997961006316685, 0.9221865047211903, 0.014994902515791712, 0.04350403095480874, 0.07975739008381602, 0.029002687303205824, 0.6380591206705282, 0.18851746747083786, 0.007250671825801456, 0.007250671825801456, 0.004393896621566956, 0.16843270382673334, 0.2563106362580725, 0.2709569583299623, 0.0717669781522603, 0.03222190855815768, 0.07762550698101624, 0.09520109346728406, 0.0205048509006458, 0.002929264414377971, 0.9360209155181665, 0.004478568973771132, 0.05822139665902471, 0.8215037505665467, 0.008814417924533764, 0.051123623962295826, 0.014103068679254021, 0.0017628835849067526, 0.04230920603776206, 0.022917486603787784, 0.0370205552830418, 0.9965491681363474, 0.04252791918680208, 0.00354399326556684, 0.00177199663278342, 0.00708798653113368, 0.4341391750319379, 0.37920727941565185, 0.12935575419318965, 0.008970859948864988, 0.004485429974432494, 0.6189893364716842, 0.04485429974432494, 0.3229509581591396, 0.1077795506292355, 0.09946597647601624, 0.09263696913587184, 0.045427744480091, 0.0510690983697755, 0.20991774736878652, 0.031472816437187225, 0.11134251098061519, 0.14133076060472755, 0.10985794416754033, 0.0699639521267883, 0.15900898210633704, 0.6233152098568412, 0.09540538926380222, 0.025441437137013925, 0.025441437137013925, 0.007965795145915383, 0.9877585980935075, 0.9862640376717176, 0.002120997930476812, 0.01060498965238406, 0.39596402213597215, 0.5988955834806579, 0.006184566721785109, 0.9895306754856175, 0.0053140221975692264, 0.00797103329635384, 0.28961420976752283, 0.6987939189803533, 0.9898139294360139, 0.0013148372045343009, 0.04996381377230343, 0.10781665077181267, 0.3273944639290409, 0.06574186022671505, 0.1327985576579644, 0.027611581295220317, 0.2879493477930119, 0.6054838548975632, 0.09380735780103092, 0.13218309508327084, 0.005685294412183692, 0.12791912427413307, 0.021319854045688848, 0.012791912427413307, 0.002100161517354584, 0.49983844113039105, 0.01575121138015938, 0.161712436836303, 0.03885298807105981, 0.040953149588414395, 0.09975767207434275, 0.0052504037933864605, 0.06195476476196023, 0.07350565310741045, 0.01827450615477056, 0.11979954034794034, 0.8609322899580797, 0.13941350681543926, 0.8589670903789968, 0.011215316291103991, 0.48225860051747166, 0.044861265164415964, 0.44861265164415964, 0.1766458184126165, 0.7949061828567743, 0.018926337687066055, 0.31195580813304313, 0.011141278861894397, 0.6573354528517694, 0.02256659944768876, 0.015044399631792507, 0.6995645828783515, 0.18053279558151009, 0.06769979834306628, 0.007522199815896254, 0.780263368805077, 0.144791140603004, 0.072395570301502, 0.9963882984545417, 0.07180437475787808, 0.10770656213681712, 0.02393479158595936, 0.7898481223366589, 0.3713092702166314, 0.6188487836943857, 0.004151953958899775, 0.7169040502366943, 0.26987700732848535, 0.0027679693059331828, 0.004151953958899775, 0.9892484744045529, 0.04824548192271805, 0.018953582183924948, 0.02067663510973631, 0.30842647372023324, 0.13956728699072007, 0.04307632314528397, 0.04135327021947262, 0.07753738166151115, 0.14645949869396552, 0.15679781624883365, 0.01672071535722222, 0.9698014907188888, 0.00761887896826519, 0.9752165079379443, 0.01523775793653038, 0.001968745363769227, 0.9961851540672289, 0.035045351262306804, 0.6301154156962764, 0.07639886575182883, 0.03925079341378362, 0.034344444237060665, 0.0855106570800286, 0.02312993183312249, 0.009111791328199769, 0.06728707442362906, 0.0007009070252461361, 0.5293375327019966, 0.004659030416469841, 0.11808850170975482, 0.053041269356733575, 0.07096061711238681, 0.02186160426189695, 0.14263800813499974, 0.03225482596017582, 0.00985564126560928, 0.017202573845427106, 0.05121285551624215, 0.07041767633483295, 0.06721687286506782, 0.019204820818590807, 0.09922490756271916, 0.6881727459995038, 0.0032008034697651342, 0.021942444542015705, 0.9727817080293629, 0.06820845190254929, 0.006820845190254929, 0.9242245232795429, 0.09269469785628028, 0.0968452067155167, 0.051189609263915976, 0.2919191230996289, 0.024903053155418582, 0.11898125396477767, 0.00830101771847286, 0.11621424805862005, 0.07885966832549217, 0.11898125396477767, 0.9912479495400959, 0.05152838698863712, 0.39014350148539534, 0.007361198141233875, 0.5484092615219236, 0.10315346521373789, 0.009731458982428103, 0.0019462917964856205, 0.6344911256543123, 0.0019462917964856205, 0.18684401246261959, 0.005838875389456862, 0.056442462098083, 0.22467696062855158, 0.03510577509821119, 0.681052036905297, 0.038616352608032305, 0.014042310039284474, 0.0035105775098211184, 0.9235253703104095, 0.05468242324206372, 0.01822747441402124, 0.774649625514628, 0.00632367041236431, 0.2134238764172955, 0.003161835206182155, 0.8879204995032146, 0.035281609251783363, 0.06468295029493616, 0.00588026820863056, 0.013152157255808466, 0.005260862902323386, 0.9785204998321498, 0.003109003841593993, 0.0015545019207969964, 0.9933267273892807, 0.0021725576720288734, 0.3736799195889662, 0.004345115344057747, 0.5322766296470739, 0.08472974920912606, 0.0021725576720288734, 0.0864885819585568, 0.6702865101788152, 0.24505098221591093, 0.011941809494703918, 0.2579430850856046, 0.025077799938878226, 0.26630235173189737, 0.08001012361451625, 0.21972929470255206, 0.0011941809494703916, 0.026271980888348617, 0.10269956165445368, 0.008359266646292742, 0.7497211844589502, 0.008717688191383141, 0.23537758116734483, 0.29605350363129085, 0.12185128960434227, 0.04874051584173691, 0.15163716039651484, 0.1254616981852117, 0.20218288052868644, 0.009026021452173502, 0.005415612871304101, 0.04061709653478076, 0.19967089122880521, 0.09549477406595032, 0.0032555036613392155, 0.024958861403600652, 0.046662219145862084, 0.5566911260890058, 0.0032555036613392155, 0.06836557688812352, 0.08245254416817888, 0.7924605633941637, 0.03664557518585728, 0.05038766588055376, 0.03664557518585728, 0.9607477060571701, 0.027190972812938776, 0.9900699389030286, 0.24332038384925098, 0.02703559820547233, 0.007373344965128818, 0.562831999004833, 0.11551573778701814, 0.04178228813572996, 0.07129533363544505, 0.03752385980812897, 0.01313335093284514, 0.01500954392325159, 0.5703626690835604, 0.29080991351299956, 0.36842237170106723, 0.0673608067662399, 0.00962297239517713, 0.4742750680480156, 0.05223899300239013, 0.0013747103421681613, 0.027494206843363227, 0.9045273956147625, 0.0667837816523386, 0.001964228872127606, 0.008839029924574226, 0.007856915488510424, 0.008839029924574226, 0.000982114436063803, 0.701338625573655, 0.12428785769659709, 0.0011097130151481883, 0.1420432659389681, 0.0011097130151481883, 0.005548565075740941, 0.024413686333260142, 0.12575837558748562, 0.027946305686107918, 0.7335905242603329, 0.0349328821076349, 0.07685234063679677, 0.010073064901175932, 0.07051145430823152, 0.27197275233175017, 0.6346030887740838, 0.9929849718884909, 0.026872634223140138, 0.7255611240247837, 0.008957544741046713, 0.23289616326721452, 0.3350414791759917, 0.0874021250024326, 0.28972185880435997, 0.04855673611246256, 0.10035058796575597, 0.053412409723708816, 0.04531962037163172, 0.03884538888997005, 0.9936504050592566, 0.9702187536070623, 0.011025213109171162, 0.011025213109171162, 0.9859538685907369, 0.004076997389072654, 0.009512993907836192, 0.8589703872115265, 0.032211389520432246, 0.10737129840144082, 0.17989594145852855, 0.0949450802142234, 0.024985547424795633, 0.2248699268231607, 0.4797225105560762, 0.9895503565297741, 0.3143756082260235, 0.6650253250935113, 0.006045684773577375, 0.01209136954715475, 0.4176643314766332, 0.131592871561131, 0.005721429198310044, 0.01716428759493013, 0.4233857606749432, 0.0959023655244241, 0.7149085430002524, 0.1394943498537078, 0.03487358746342695, 0.6549093311705737, 0.09696473107137134, 0.05722508718966177, 0.05563550143439339, 0.0460979869027831, 0.0031791715105367654, 0.03497088661590442, 0.01748544330795221, 0.03338130086063604, 0.05508658608807215, 0.061972409349081166, 0.027543293044036073, 0.006885823261009018, 0.8400704378431002, 0.7766004999067341, 0.0042985267522512955, 0.037253898519511226, 0.020059791510506044, 0.14614990957654406, 0.012895580256753886, 0.0014328422507504318, 0.0014328422507504318, 0.7209880328394617, 0.03383698349066065, 0.03643982837455763, 0.13795077884653958, 0.0026028448838969734, 0.0676739669813213, 0.7778448847902532, 0.005197181858286769, 0.11780278878783344, 0.017323939527622567, 0.0017323939527622565, 0.07969012182706381, 0.8710511023018461, 0.0759853089242036, 0.04633250544158756, 0.005559900652990508, 0.808291582568768, 0.18251745412843148, 0.7772370446190929, 0.06894844750653244, 0.08148452887135652, 0.00940206102361806, 0.02977319324145719, 0.01723711187663311, 0.01410309153542709, 0.0031133848719318144, 0.9931697741462487, 0.06735259769627021, 0.9285036682414393, 0.019842096049910003, 0.019842096049910003, 0.9424995623707252, 0.1971965882981072, 0.01593507784227129, 0.09561046705362775, 0.10756177543533121, 0.0019918847302839112, 0.5457764160977917, 0.033862040414826496, 0.0019918847302839112, 0.823388180591981, 0.0031307535383725514, 0.010644562030466674, 0.008139959199768634, 0.04195209741419219, 0.01941067193790982, 0.010644562030466674, 0.08327804412070987, 0.07083113875927785, 0.8912918293875796, 0.029512974483032438, 0.019492470398356207, 0.9551310495194542, 0.019492470398356207, 0.021420024495685502, 0.2784603184439115, 0.6854407838619361, 0.010710012247842751, 0.9910776157911569, 0.06365062875770622, 0.872302935020383, 0.015912657189426554, 0.017359262388465333, 0.028932103980775555, 0.995154063923561, 0.024409576527895094, 0.03603318439832133, 0.04068262754649183, 0.0011623607870426237, 0.13832093365807221, 0.0023247215740852473, 0.4079886362519609, 0.006974164722255742, 0.34057171060348873, 0.07304314301817703, 0.00442685715261679, 0.08189685732341062, 0.01328057145785037, 0.08189685732341062, 0.20252871473221815, 0.1826078575454426, 0.009960428593387778, 0.34972171505672645, 0.6330033393676436, 0.002090845051585941, 0.13172323824991428, 0.03449894335116802, 0.11499647783722675, 0.03554436587696099, 0.007317957680550793, 0.006795246417654307, 0.03397623208827154, 0.05563795922948062, 0.07630348694328772, 0.2638828923455367, 0.28136910810337346, 0.007948279889925804, 0.007948279889925804, 0.09220004672313932, 0.21619321300598185, 0.2792323873632425, 0.22742909378001655, 0.03727310148061382, 0.40621363139041844, 0.009476212240834022, 0.02084766692983485, 0.005685727344500413, 0.012634949654445364, 0.9929247374737805, 0.13769617201015869, 0.8628960112636611, 0.7895945377249982, 0.0736954901876665, 0.026319817924166607, 0.09475134452699978, 0.010527927169666643, 0.9429125130528253, 0.011225148964914586, 0.03367544689474376, 0.9973700954939565, 0.04129752513963284, 0.5722657055063408, 0.009832744080864963, 0.06686265974988174, 0.05506336685284379, 0.10029398962482261, 0.09832744080864962, 0.02163203697790292, 0.03343132987494087, 0.10363786976740412, 0.3943996710592879, 0.005757659431522451, 0.07772840232555309, 0.4145514790696165, 0.022098772324317863, 0.022098772324317863, 0.9391978237835092, 0.3068836574071781, 0.6819636831270625, 0.017283785031919383, 0.042249252300247384, 0.19780331758752184, 0.29382434554262954, 0.03840841118204308, 0.0019204205591021538, 0.40136789685235014, 0.007681682236408615, 0.31674114333913483, 0.1807703795732035, 0.1328269310777017, 0.12182351666889801, 0.16505121613205537, 0.04322769946315736, 0.03929790860287032, 0.24992983036410954, 0.06344372616935089, 0.0076901486265879854, 0.34028907672651837, 0.14611282390517172, 0.0038450743132939927, 0.13265506380864275, 0.046140891759527916, 0.0076901486265879854, 0.0986335689292774, 0.3823572918986803, 0.007306190291057585, 0.021918570873172755, 0.0036530951455287924, 0.11568134627507844, 0.006088491909214655, 0.0828034899653193, 0.2130972168225129, 0.07062650614688999, 0.0550500358013463, 0.00917500596689105, 0.935850608622887, 0.2833573426338368, 0.00223997899315286, 0.30127717457905967, 0.01791983194522288, 0.16575844549331165, 0.09407911771242013, 0.000559994748288215, 0.07447930152233259, 0.06047943281512722, 0.9904097843423547, 0.008253414869519622, 0.0085696739472786, 0.0228524638594096, 0.6884304737647142, 0.014282789912131, 0.0028565579824262, 0.0028565579824262, 0.257090218418358, 0.5271124861418117, 0.08231983410778923, 0.15667323265676017, 0.058420527431334295, 0.027882524455864095, 0.07302565928916788, 0.03983217779409157, 0.027882524455864095, 0.005310957039212209, 0.013822275783085581, 0.9399147532498194, 0.04146682734925674, 0.9814930287453322, 0.019087861300751945, 0.9734809263383493, 0.04457327205486444, 0.01910283088065619, 0.9360387131521534, 0.7956062924892998, 0.15627980745325531, 0.035518138057558025, 0.0642667941438386, 0.0952100653982794, 0.07616805231862353, 0.6307666832636011, 0.10949157520802132, 0.019042013079655883, 0.004760503269913971, 0.9964221406429775, 0.8381203349265226, 0.026466957945048084, 0.05293391589009617, 0.07940087383514424, 0.00945378476423134, 0.01890756952846268, 0.016544123337404845, 0.002363446191057835, 0.8579309673539941, 0.00472689238211567, 0.06144960096750371, 0.028361354292694017, 0.05292333090858416, 0.008142050909012948, 0.3989604945416344, 0.541446385449361, 0.22613558737495137, 0.7688609970748347, 0.0262414385897957, 0.33239155547074556, 0.6297945261550968, 0.04178103207975595, 0.14884492678413055, 0.010445258019938987, 0.010445258019938987, 0.14884492678413055, 0.11489783821932885, 0.033947088564801706, 0.04961497559471019, 0.020890516039877974, 0.4178103207975595, 0.10947890005322546, 0.03184840728821104, 0.24284410557260921, 0.039810509110263806, 0.5613281784547196, 0.009952627277565951, 0.02084406193802764, 0.9588268491492713, 0.01042203096901382, 0.9976861486649138, 0.9985159380833304, 0.1502172157018129, 0.15324172340050712, 0.20062567734671657, 0.0897270617279285, 0.11493129255038034, 0.030245076986942194, 0.034277753918534486, 0.040326769315922925, 0.15324172340050712, 0.033269584685636414, 0.9948698402911902, 0.14619037151684444, 0.2310329978435845, 0.10246378717921685, 0.1879590490930857, 0.10703223628911825, 0.0411160419891125, 0.07244255017129345, 0.06069510960297559, 0.03002123700792341, 0.020884338788120634, 0.997452437656889, 0.9986203060618428, 0.03567394246746724, 0.0025481387476762314, 0.005096277495352463, 0.914781810415767, 0.0407702199628197, 0.3889038504729629, 0.011920424535569744, 0.10728382082012769, 0.02235079600419327, 0.037251326673655445, 0.026820955205031922, 0.31440119712565195, 0.04321153894144032, 0.03874137974060166, 0.010430371468623524, 0.024384445415471755, 0.5109121896575034, 0.05225238303315376, 0.03831841422431276, 0.04876889083094351, 0.03367375795469909, 0.2670677355027859, 0.01857862507845467, 0.002322328134806834, 0.0034834922022102506, 0.020644773810540414, 0.006881591270180139, 0.20988853374049424, 0.0034407956350900695, 0.7569750397198153, 0.1731424371780068, 0.03725849913957108, 0.4120351669552566, 0.008766705679899078, 0.18190914285790585, 0.039450175559545844, 0.1293089087785114, 0.008766705679899078, 0.008766705679899078, 0.12107135597126839, 0.05697475575118512, 0.811890269454388, 0.08597925560374835, 0.007164937966979029, 0.9027821838393577, 0.02632377518647373, 0.02632377518647373, 0.9476559067130543, 0.040788699436770055, 0.862389645234567, 0.040788699436770055, 0.052442613561561505, 0.13659188482863246, 0.18847562402710524, 0.08682584927091366, 0.25730099235160997, 0.09529666383392962, 0.0995320711154376, 0.02329474004829391, 0.021177036407539915, 0.09317896019317563, 0.001058851820376996, 0.9970790908067622, 0.03786420266027097, 0.011359260798081292, 0.9466050665067743, 0.9947003548312092, 0.004891879707229183, 0.9294571443735448, 0.0587025564867502, 0.004891879707229183, 0.05257795048112786, 0.005007423855345511, 0.5132609451729149, 0.0951410532515647, 0.3204751267421127, 0.012518559638363777, 0.9631949004580506, 0.028895847013741517, 0.1599112409199467, 0.3776627179173209, 0.0034023668280839725, 0.04763313559317561, 0.08505917070209931, 0.08392504842607132, 0.004536489104111963, 0.02835305690069977, 0.1621794854720027, 0.04763313559317561, 0.10776607115675421, 0.08336620598918723, 0.054899696627025735, 0.012199932583783496, 0.44936418350269214, 0.04879973033513398, 0.09963278276756522, 0.09353281647567348, 0.05083305243243123, 0.638079787715764, 0.09549493421596468, 0.21703394139991974, 0.04340678827998395, 0.07481784548664185, 0.0340081115848372, 0.8774092788887997, 0.01360324463393488, 0.06904952369576277, 0.007672169299529197, 0.9206603159435036, 0.01032928467028328, 0.8470013429632289, 0.04131713868113312, 0.09296356203254952, 0.021020933186966958, 0.7987954611047444, 0.16816746549573566, 0.010510466593483479, 0.16353238606740386, 0.701606688611765, 0.12660571824573202, 0.9884718153513443, 0.03923692685244899, 0.07847385370489798, 0.8730216224669901, 0.9453098869266446, 0.045635649713700085, 0.03691721262692036, 0.1590956067969663, 0.18634402564064562, 0.1590956067969663, 0.11338858163982682, 0.08789812530219132, 0.11426756289284873, 0.029006381349723137, 0.10108284409752002, 0.014063700048350612, 0.050446249104917226, 0.945867170717198, 0.08584057211579191, 0.9099100644273942, 0.9852078776326546, 0.12148624982101588, 0.639401314847452, 0.08951618407864328, 0.01278802629694904, 0.08312217093016876, 0.00639401314847452, 0.04475809203932164, 0.002256548364308869, 0.0722095476578838, 0.5280323172482753, 0.39715251211836095, 0.02555615175729843, 0.178893062301089, 0.013218699184809533, 0.04934981028995559, 0.10751208670311753, 0.22471788614176205, 0.24234281838817479, 0.10310585364151435, 0.028199891594260337, 0.02555615175729843, 0.9935115013852172, 0.9894731186982845, 0.9956748971009127, 0.4622156457610971, 0.08746721639717986, 0.09699142440487277, 0.035181258150865674, 0.03926306158273407, 0.06511448331790055, 0.15685787473894255, 0.017299071687442238, 0.02390770581522916, 0.015744098951492373, 0.1342823077114628, 0.2745180182278673, 0.08400912846973288, 0.0833476392691838, 0.09194699887632182, 0.12369848050267755, 0.04828871164008268, 0.044319776436788215, 0.09459295567851812, 0.02050616521702141, 0.8675235295595645, 0.12940958728236596, 0.1828277901180775, 0.02559589061653085, 0.04570694752951938, 0.001828277901180775, 0.05302005913424248, 0.11335322987320806, 0.0950704508614003, 0.24864579456058541, 0.23584784925232, 0.07495444790538364, 0.8057603149828741, 0.01873861197634591, 0.09369305988172955, 0.9867497518777061, 0.9898098811057877], \"Term\": [\"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"access\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"aerospatiale\", \"air\", \"air\", \"air\", \"air\", \"air\", \"airbus\", \"aircraft\", \"aircraft\", \"aircraft\", \"airline\", \"airline\", \"airline\", \"airline\", \"airlines\", \"airlines\", \"airlines\", \"airlines\", \"alcatel\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"america\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"american\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analyst\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"analysts\", \"aol\", \"apec\", \"apple\", \"apple\", \"apple\", \"applications\", \"applications\", \"applications\", \"applications\", \"asset\", \"asset\", \"asset\", \"asset\", \"asset\", \"atlantic\", \"atlantic\", \"attorney\", \"attorney\", \"auction\", \"auction\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australia\", \"australian\", \"australian\", \"australian\", \"australian\", \"australian\", \"australian\", \"auto\", \"automaker\", \"automotive\", \"aviation\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bankers\", \"bankers\", \"banking\", \"banking\", \"banks\", \"banks\", \"barrick\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"beijing\", \"bell\", \"bell\", \"bell\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bid\", \"bids\", \"bids\", \"bids\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"billion\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"board\", \"boeing\", \"bond\", \"bond\", \"bre\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"britain\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"british\", \"broadcasting\", \"broadcasting\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokerage\", \"brokers\", \"brokers\", \"brokers\", \"brokers\", \"brokers\", \"brokers\", \"brokers\", \"bt\", \"bt\", \"bt\", \"budget\", \"budget\", \"budget\", \"budget\", \"busang\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"business\", \"bzw\", \"bzw\", \"bzw\", \"cable\", \"cable\", \"cable\", \"calling\", \"calling\", \"calling\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"calls\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canada\", \"canadian\", \"canadian\", \"canadian\", \"canadian\", \"canadian\", \"canadian\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"car\", \"car\", \"car\", \"carrier\", \"carrier\", \"carrier\", \"carrier\", \"carriers\", \"carriers\", \"carriers\", \"cars\", \"cars\", \"cars\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cathay\", \"cathay\", \"ce\", \"ce\", \"cents\", \"cents\", \"cents\", \"cents\", \"cents\", \"cents\", \"cents\", \"cereal\", \"cereal\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"chairman\", \"channels\", \"channels\", \"channels\", \"channels\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"china\", \"china\", \"china\", \"chinese\", \"chinese\", \"chrysler\", \"cigarette\", \"civil\", \"civil\", \"civil\", \"civil\", \"class\", \"class\", \"class\", \"class\", \"class\", \"coast\", \"coast\", \"cocoa\", \"coffee\", \"coffee\", \"colonial\", \"colony\", \"committee\", \"committee\", \"committee\", \"committee\", \"communications\", \"communications\", \"communications\", \"communications\", \"communist\", \"communist\", \"communist\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"companies\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"compensation\", \"compensation\", \"compensation\", \"computer\", \"computer\", \"computer\", \"computer\", \"computers\", \"computers\", \"concert\", \"concert\", \"conrail\", \"conrail\", \"consensus\", \"consensus\", \"consensus\", \"copper\", \"copper\", \"corn\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"corp\", \"costs\", \"costs\", \"costs\", \"costs\", \"costs\", \"costs\", \"costs\", \"costs\", \"costs\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"country\", \"court\", \"court\", \"court\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"credit\", \"crop\", \"crown\", \"crown\", \"crown\", \"crowns\", \"crowns\", \"crowns\", \"csf\", \"csx\", \"csx\", \"customers\", \"customers\", \"customers\", \"customers\", \"customers\", \"customers\", \"customers\", \"customers\", \"czech\", \"czech\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"death\", \"death\", \"death\", \"debate\", \"debate\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decline\", \"decline\", \"decline\", \"decline\", \"decline\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"democracy\", \"deng\", \"deposit\", \"deposit\", \"deposit\", \"deposit\", \"deposits\", \"deposits\", \"deutsche\", \"deutsche\", \"deutsche\", \"deutsche\", \"devices\", \"digital\", \"digital\", \"digital\", \"digital\", \"diplomat\", \"diplomatic\", \"diplomats\", \"distance\", \"distance\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"domestic\", \"drug\", \"drug\", \"earned\", \"earned\", \"earned\", \"earned\", \"earned\", \"earnings\", \"earnings\", \"earnings\", \"earnings\", \"earnings\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"economic\", \"election\", \"election\", \"election\", \"election\", \"electric\", \"electric\", \"electric\", \"electric\", \"electricity\", \"electricity\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronic\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"electronics\", \"encryption\", \"encryption\", \"encryption\", \"england\", \"england\", \"england\", \"england\", \"england\", \"estate\", \"estate\", \"estate\", \"estate\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"estimates\", \"european\", \"european\", \"european\", \"european\", \"european\", \"european\", \"european\", \"european\", \"european\", \"european\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"executive\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"expected\", \"export\", \"export\", \"export\", \"exporters\", \"exports\", \"fcc\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"fell\", \"figures\", \"figures\", \"figures\", \"figures\", \"figures\", \"figures\", \"filed\", \"filed\", \"filed\", \"filed\", \"film\", \"film\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"finance\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"financial\", \"firms\", \"firms\", \"firms\", \"firms\", \"firms\", \"firms\", \"firms\", \"fiscal\", \"fiscal\", \"fiscal\", \"fiscal\", \"fiscal\", \"florida\", \"ford\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"foreign\", \"fourth\", \"fourth\", \"fourth\", \"fourth\", \"france\", \"france\", \"france\", \"france\", \"france\", \"francs\", \"francs\", \"francs\", \"francs\", \"freeport\", \"french\", \"french\", \"french\", \"french\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"friday\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"gains\", \"gains\", \"gains\", \"gains\", \"game\", \"game\", \"game\", \"game\", \"gas\", \"gas\", \"gas\", \"gazprom\", \"gec\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"gm\", \"gm\", \"gold\", \"gold\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"government\", \"grain\", \"grew\", \"grew\", \"grew\", \"grew\", \"grew\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"growth\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"half\", \"handover\", \"hilton\", \"hong\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hotels\", \"hotels\", \"hotels\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hp\", \"human\", \"human\", \"hwang\", \"ibm\", \"ibm\", \"import\", \"import\", \"imports\", \"imports\", \"income\", \"income\", \"income\", \"income\", \"income\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"increase\", \"index\", \"index\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesia\", \"indonesian\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"industry\", \"inflation\", \"inflation\", \"inflation\", \"inflation\", \"inflation\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"institutions\", \"institutions\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"intel\", \"intel\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"international\", \"internet\", \"internet\", \"internet\", \"investigation\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investment\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"items\", \"items\", \"items\", \"items\", \"itt\", \"ivory\", \"ivory\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"japan\", \"jiang\", \"jones\", \"jones\", \"jones\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"july\", \"klaus\", \"klaus\", \"klaus\", \"kong\", \"korea\", \"korea\", \"korea\", \"korean\", \"korean\", \"labor\", \"labor\", \"labor\", \"labour\", \"labour\", \"labour\", \"labour\", \"lagardere\", \"lawsuit\", \"lawsuits\", \"lawsuits\", \"lawyers\", \"lawyers\", \"lawyers\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"leader\", \"legislature\", \"lending\", \"li\", \"licenses\", \"licenses\", \"licenses\", \"licenses\", \"licenses\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"life\", \"listed\", \"listed\", \"listed\", \"listed\", \"listed\", \"listed\", \"lloyd\", \"lme\", \"loan\", \"loan\", \"loans\", \"loans\", \"loans\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"london\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"machines\", \"machines\", \"machines\", \"mainframe\", \"mainframe\", \"mainland\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"major\", \"margin\", \"margin\", \"margin\", \"margin\", \"margins\", \"margins\", \"margins\", \"margins\", \"margins\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"market\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"markets\", \"mci\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"mercury\", \"mercury\", \"mercury\", \"mercury\", \"mercury\", \"merger\", \"merger\", \"merger\", \"merger\", \"merger\", \"merger\", \"merger\", \"metal\", \"metal\", \"metals\", \"metals\", \"mgam\", \"microsoft\", \"midnight\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"million\", \"minerals\", \"mining\", \"mining\", \"mining\", \"minister\", \"minister\", \"minister\", \"ministry\", \"ministry\", \"ministry\", \"ministry\", \"ministry\", \"morris\", \"morris\", \"morris\", \"mortgage\", \"motor\", \"music\", \"music\", \"music\", \"mutual\", \"mutual\", \"mutual\", \"names\", \"names\", \"names\", \"names\", \"names\", \"names\", \"names\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"national\", \"natwest\", \"natwest\", \"natwest\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"netscape\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"newspapers\", \"newspapers\", \"newspapers\", \"newspapers\", \"newspapers\", \"newspapers\", \"nickel\", \"nickel\", \"nomura\", \"nomura\", \"nomura\", \"norfolk\", \"norfolk\", \"norilsk\", \"norilsk\", \"northern\", \"northern\", \"northern\", \"northern\", \"nynex\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"offer\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"official\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"officials\", \"oil\", \"oil\", \"oil\", \"online\", \"online\", \"operators\", \"operators\", \"operators\", \"operators\", \"opposition\", \"opposition\", \"opposition\", \"optus\", \"optus\", \"optus\", \"original\", \"original\", \"original\", \"original\", \"original\", \"original\", \"outlook\", \"outlook\", \"outlook\", \"output\", \"paris\", \"paris\", \"paris\", \"paris\", \"parliament\", \"parliament\", \"party\", \"party\", \"party\", \"party\", \"party\", \"patten\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"pc\", \"pc\", \"pcs\", \"pcs\", \"pcs\", \"pence\", \"pence\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"personal\", \"philip\", \"philip\", \"phone\", \"phone\", \"phone\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plane\", \"plant\", \"plant\", \"plant\", \"plant\", \"plc\", \"plc\", \"plc\", \"plc\", \"plc\", \"plc\", \"plc\", \"plc\", \"points\", \"points\", \"points\", \"points\", \"points\", \"points\", \"police\", \"police\", \"police\", \"political\", \"political\", \"political\", \"political\", \"posted\", \"posted\", \"posted\", \"posted\", \"pound\", \"pound\", \"pound\", \"pounds\", \"pounds\", \"pounds\", \"power\", \"power\", \"power\", \"power\", \"power\", \"power\", \"prague\", \"prague\", \"prague\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"president\", \"pretax\", \"pretax\", \"pretax\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prices\", \"prices\", \"prices\", \"prices\", \"prices\", \"prices\", \"prices\", \"prices\", \"prime\", \"prime\", \"prime\", \"prime\", \"prime\", \"probe\", \"probe\", \"producers\", \"product\", \"product\", \"product\", \"product\", \"product\", \"product\", \"production\", \"production\", \"production\", \"production\", \"production\", \"production\", \"products\", \"products\", \"products\", \"products\", \"products\", \"products\", \"products\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profit\", \"profits\", \"profits\", \"profits\", \"profits\", \"profits\", \"profits\", \"profits\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"proposal\", \"provision\", \"provision\", \"provision\", \"provision\", \"provisional\", \"prudential\", \"prudential\", \"prudential\", \"prudential\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"public\", \"pyongyang\", \"quaker\", \"quaker\", \"quaker\", \"quarter\", \"quarter\", \"quarter\", \"quarters\", \"quarters\", \"quarters\", \"radio\", \"radio\", \"radio\", \"radio\", \"radio\", \"rains\", \"reform\", \"reform\", \"reform\", \"reform\", \"regulators\", \"regulators\", \"regulators\", \"regulators\", \"regulators\", \"rejected\", \"rejected\", \"rejected\", \"rejected\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reported\", \"reserves\", \"reserves\", \"reserves\", \"reserves\", \"reserves\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenues\", \"revenues\", \"revenues\", \"revenues\", \"revenues\", \"revenues\", \"rights\", \"rights\", \"rights\", \"rights\", \"robotics\", \"robotics\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"russia\", \"russia\", \"russian\", \"russian\", \"sa\", \"sa\", \"sa\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sale\", \"sales\", \"sales\", \"sales\", \"sales\", \"sales\", \"sales\", \"sales\", \"sales\", \"san\", \"san\", \"san\", \"satellite\", \"satellite\", \"satellite\", \"scandal\", \"scandal\", \"scandal\", \"scandal\", \"sears\", \"securities\", \"securities\", \"securities\", \"securities\", \"securities\", \"seoul\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"services\", \"services\", \"services\", \"services\", \"services\", \"services\", \"services\", \"services\", \"services\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"share\", \"shareholders\", \"shareholders\", \"shareholders\", \"shareholders\", \"shareholders\", \"shareholders\", \"shareholders\", \"shareholders\", \"shares\", \"shares\", \"shares\", \"shares\", \"shares\", \"shares\", \"shares\", \"shares\", \"sino\", \"skoda\", \"skoda\", \"slightly\", \"slightly\", \"slightly\", \"slightly\", \"slightly\", \"snapple\", \"snapple\", \"snapple\", \"software\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"south\", \"southern\", \"southern\", \"southern\", \"southern\", \"southern\", \"sport\", \"sport\", \"sport\", \"sprint\", \"sprint\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"stake\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"statement\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"states\", \"steel\", \"steel\", \"steel\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"stock\", \"strathcona\", \"strathcona\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strike\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"strong\", \"subscribers\", \"subscribers\", \"subscribers\", \"suez\", \"sugar\", \"sugar\", \"sun\", \"sun\", \"sun\", \"suspended\", \"suspended\", \"suspended\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"taiwan\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"telecom\", \"telecom\", \"telecom\", \"telecom\", \"telecommunications\", \"telecommunications\", \"telecoms\", \"telecoms\", \"telecoms\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"telephone\", \"television\", \"television\", \"television\", \"television\", \"television\", \"television\", \"tender\", \"tender\", \"tender\", \"territory\", \"thomson\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"thursday\", \"tobacco\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"tonne\", \"tonnes\", \"toronto\", \"toronto\", \"toronto\", \"toronto\", \"toronto\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"trade\", \"traders\", \"traders\", \"traders\", \"traders\", \"traders\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trading\", \"trial\", \"trial\", \"trial\", \"truck\", \"truck\", \"truck\", \"trucks\", \"trucks\", \"trucks\", \"trust\", \"trust\", \"trust\", \"trust\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tuesday\", \"tung\", \"tv\", \"tv\", \"tv\", \"uaw\", \"uk\", \"uk\", \"uk\", \"uk\", \"union\", \"union\", \"union\", \"union\", \"union\", \"union\", \"unions\", \"unions\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"united\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"users\", \"users\", \"users\", \"users\", \"vehicle\", \"vehicle\", \"vehicle\", \"vehicle\", \"vehicles\", \"vehicles\", \"vehicles\", \"version\", \"version\", \"version\", \"version\", \"video\", \"video\", \"video\", \"video\", \"vote\", \"vote\", \"vote\", \"walsh\", \"waste\", \"waste\", \"waste\", \"web\", \"web\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"windows\", \"windows\", \"wireless\", \"wireless\", \"wmx\", \"won\", \"won\", \"won\", \"won\", \"won\", \"won\", \"won\", \"workers\", \"workers\", \"workers\", \"workers\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"xiaoping\", \"xinhua\", \"xinjiang\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yen\", \"yen\", \"york\", \"york\", \"york\", \"york\", \"york\", \"york\", \"york\", \"york\", \"york\", \"zealand\", \"zealand\", \"zealand\", \"zealand\", \"zemin\", \"zinc\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 10, 9, 6, 7, 8, 5, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el9843381322139795939368380334490\", ldavis_el9843381322139795939368380334490_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el9843381322139795939368380334490\", ldavis_el9843381322139795939368380334490_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el9843381322139795939368380334490\", ldavis_el9843381322139795939368380334490_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.195590 -0.001257       1        1  16.585694\n",
       "1     -0.186213 -0.161415       2        1  15.989690\n",
       "3      0.090098 -0.033537       3        1  13.097525\n",
       "9     -0.132089 -0.082174       4        1  10.528012\n",
       "8      0.000344  0.025032       5        1   9.442926\n",
       "5      0.012645  0.078475       6        1   9.412560\n",
       "6      0.153766 -0.165063       7        1   8.086043\n",
       "7     -0.061683  0.130637       8        1   6.364485\n",
       "4     -0.050851  0.023379       9        1   5.929084\n",
       "0     -0.021607  0.185922      10        1   4.563982, topic_info=              Term         Freq        Total Category  logprob  loglift\n",
       "226          china  3383.000000  3383.000000  Default  30.0000  30.0000\n",
       "722           kong  2288.000000  2288.000000  Default  29.0000  29.0000\n",
       "621           hong  2272.000000  2272.000000  Default  28.0000  28.0000\n",
       "110           bank  1772.000000  1772.000000  Default  27.0000  27.0000\n",
       "1022        pounds  1286.000000  1286.000000  Default  26.0000  26.0000\n",
       "...            ...          ...          ...      ...      ...      ...\n",
       "1230  shareholders   136.341908   629.066926  Topic10  -4.9871   1.5579\n",
       "474       expected   157.141784  1739.140506  Topic10  -4.8451   0.6830\n",
       "668       industry   147.986234  1594.999347  Topic10  -4.9052   0.7095\n",
       "4           access   129.417425   460.530927  Topic10  -5.0392   1.8176\n",
       "273        company   133.978540  3714.255646  Topic10  -5.0046  -0.2353\n",
       "\n",
       "[558 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "4         1  0.002171   access\n",
       "4         2  0.169370   access\n",
       "4         3  0.056457   access\n",
       "4         6  0.243198   access\n",
       "4         7  0.043428   access\n",
       "...     ...       ...      ...\n",
       "1497      3  0.805760  zealand\n",
       "1497      6  0.018739  zealand\n",
       "1497      8  0.093693  zealand\n",
       "1498      2  0.986750    zemin\n",
       "1499      7  0.989810     zinc\n",
       "\n",
       "[2007 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 4, 10, 9, 6, 7, 8, 5, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.lda_model.prepare(lda, tf_large, vectorizer, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** there is a small bug that when you show the `pyLDAvis` visualization it will hide some of the icons of JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"text-decoration: underline;\">Neural Networks</span><a id='nn_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested? Check out the Stanford course CS224n ([Page](http://web.stanford.edu/class/cs224n/index.html#schedule))!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
