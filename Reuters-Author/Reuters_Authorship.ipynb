{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqtE9qCqztTi"
   },
   "source": [
    "# Week 2: Python Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGotMBbXz1Pl"
   },
   "source": [
    "See the Repository for Future Work: https://github.com/firmai/python-business-analytics or\n",
    "\n",
    "Sign up to the mailing list: https://mailchi.mp/ec4942d52cc5/firmai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki1UWVLczVCC"
   },
   "source": [
    "# Text Mining NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeorQOsbzVCQ"
   },
   "source": [
    "# *Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeKqM74hzVCR"
   },
   "source": [
    "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7XU4fuOzVCT"
   },
   "source": [
    "## Note: companion slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W8FwKUgzVCZ"
   },
   "source": [
    "# *Elements / topics that are discussed in this notebook: *\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD7WDs30zVCa"
   },
   "source": [
    "# *Table of Contents*  <a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqFVyXnDzVCc"
   },
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem)\n",
    "    * [Language modeling](#lang_model)\n",
    "        * [Part-of-Speech tagging](#pos_tagging)\n",
    "        * [Uni-Gram & N-Grams](#n_grams)\n",
    "        * [Stop words](#stop_words)\n",
    "* [Direct feature extraction](#feature_extract)\n",
    "    * [Feature search](#feature_search)\n",
    "        * [Entity recognition](#entity_recognition)\n",
    "        * [Pattern search](#pattern_search)\n",
    "    * [Text evaluation](#text_eval)\n",
    "        * [Language](#language)\n",
    "        * [Dictionary counting](#dict_counting)\n",
    "        * [Readability](#readability)\n",
    "* [Represent text numerically](#text_numerical)\n",
    "    * [Bag of Words](#bows)\n",
    "        * [TF-IDF](#tfidf)\n",
    "    * [Word Embeddings](#word_embed)\n",
    "        * [Word2Vec](#Word2Vec)\n",
    "* [Statistical models](#stat_models)\n",
    "    * [\"Traditional\" machine learning](#trad_ml)\n",
    "        * [Supervised](#trad_ml_supervised)\n",
    "            * [Na√Øve Bayes](#trad_ml_supervised_nb)\n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm)\n",
    "        * [Unsupervised](#trad_ml_unsupervised)\n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda)\n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis)\n",
    "* [Model Selection and Evaluation](#trad_ml_eval)\n",
    "* [Neural Networks](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zJnQENtazVCf",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbPFc05MzVCh"
   },
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` and the higher-level wrapper `Textacy`\n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `textstat` to calculate readability statistics\n",
    "5. `Gensim` for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Jj3UkiLSzVCj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po-N0OZKzVCm"
   },
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9axpUNqqzVCp"
   },
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments.\n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_EQ6LkxzVCr"
   },
   "source": [
    "### Download and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fPhFIZozVCt"
   },
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p0ptU3H_7hdD",
    "outputId": "d7a449c4-6c2b-48a0-edce-4fb8f20f81ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m55 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m53 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m57 packages\u001b[0m \u001b[2min 439ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1.73s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangdetect\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 459ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 7.92s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgensim\u001b[0m\u001b[2m==4.4.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m59 packages\u001b[0m \u001b[2min 304ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfuzzywuzzy\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m65 packages\u001b[0m \u001b[2min 639ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 534ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 74ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfuncy\u001b[0m\u001b[2m==2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.14.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyldavis\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv add textacy\n",
    "# !uv add langdetect\n",
    "# !uv add gensim\n",
    "# !uv add fuzzywuzzy\n",
    "# !uv add pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kEwYopmNzVCv"
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-8KFZ4qzVC-"
   },
   "source": [
    "*Download and extract the zip file with the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XZjfB-bUzVDB"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVRLr8EbzVDR"
   },
   "source": [
    "*Load the data into memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rzwaWd8HzVDT"
   },
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test'}\n",
    "text_dict = {'test' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "USqLZVb4zVDg"
   },
   "outputs": [],
   "source": [
    "for label, folder in folder_dict.items():\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pbB8sAqzVEA"
   },
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-jZczhS4zVEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\\n Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\\n Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group\\'s shares up 11 pence to 248-1/2p by 1415 gmt.\\n \"It\\'s all quite encouraging. The way they are analysing and managing the business is very much more in line with what the market demands,\" said Richard Workman, an analyst at ABN-AMRO Hoare Govett.\\n The company said its recovery was strongly led by its British operations, while in mainland Europe, it started to realise the potential of businesses acquired earlier in the 1990\\'s.\\n UB\\'s British business increased profits before exceptional items by 12 percent to 113.7 million pounds as renewed consumer confidence encouraged shoppers to step up brands.\\n In an interview UB\\'s managing director Eric Nicoli said UB Has now entered a phase of consolidation after a hectic disposal programme.\\n \"The emphasis is on organic growth as we are now in a consolidation phase,\" said Nicoli.\\n Over the last two years UB has sold its American snack operation Keebler, pulled out of Portugal and exited from Spanish snacks. It has also sold one of its Italian snack businesses, withdrawn from Turkey and Brazil and sold Ross Vegetables Products business in Britain.\\n UB expects its margins in Europe to remain firm and to move up over the next two to three years, said Nicoli.\\n \"We expect margins to move to 10 percent in all our businesses over a two or three year period,\" he said.\\n (Corrects to make clear that company isn\\'t expecting European markets alone to move by 10 percent.)\\n UB margins are currently at around 10 percent in Britain and four percent in mainland Europe.\\n In 1997 the company plans to build on the turnaround \"all our markets continue to be very competitive but we expect another year of good progress,\" said Nicoli.\\n The total dividend payout was increased by 10 pence per share from 9.8p.\\n The company\\'s leading biscuit brands include McVities in Britain, Verkade in Holland and Oxford in Denmark. Biscuit sales growth came from the launch of Go Ahead! a range of low fat products. UB also owns snacks KP Nuts, Skips and Hula Hoops.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hcDfreU4zVE4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\"> 8.0. Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-wjbBW-zVE5"
   },
   "source": [
    "## Convert the text into a NLP representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDY9_DR8zVFA"
   },
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP_gh4-XzVFC"
   },
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6lbCCb4zVFE"
   },
   "source": [
    "**Note:** depending on the way that you installed the language models you will need to import it differently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z7q_0kpzVFG"
   },
   "source": [
    "```\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "```\n",
    "OR\n",
    "```\n",
    "import en_core_web_sm\n",
    "parser = en_core_web_sm.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pj4GrH-1zVFI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE4wAqeszVFW"
   },
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v2XCZmFbzVFY"
   },
   "outputs": [],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    spacy_text[author] = [nlp(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CUP_HWzxzVFk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gR79Z6jzVFy"
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Im0zp4IKzVF0"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_BSfJKezVF7"
   },
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzbRGbaazVGD"
   },
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fVYhYEDUzVGI"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h9cyqnjzVGT"
   },
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "miCnaBPCzVGY"
   },
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_STJ4QWizVGr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "s3eNuaFczVG0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHlPtBa6zVG4"
   },
   "source": [
    "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
    "\n",
    "This can imply many things, I will show a couple of things below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Gui1Vbh-zVG9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJqf_-unzVG_"
   },
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nI528gkQzVHD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ot36oM-YzVHM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
      " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48pFgqarzVHU"
   },
   "source": [
    "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
    "\n",
    "**So how do we remove them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foq49UGGzVHX"
   },
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wjf30lSZzVHZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products. United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyl9zCsAzVHn"
   },
   "source": [
    "Sometimes, however, the problem arises because of encoding/decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8RGwL1nKzVHp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is some  text that has to be cleaned! it's annoying!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hO0E7tL9zVH0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B7YG1mczVH-"
   },
   "source": [
    "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0ufvwboczVIA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products',\n",
       " '\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U',\n",
       " 'S',\n",
       " ' Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84',\n",
       " '7 million pounds in 1996 compared with 150',\n",
       " '3 million in 1995',\n",
       " '\\n Sales rose by three percent to 1',\n",
       " '887 billion and trading profits grew four pe']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtXXcCidzVIq"
   },
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Xhqki3D5zVIr"
   },
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kpMnK7ZdzVIx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       "  ,\n",
       " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\n",
       "  ,\n",
       " Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\n",
       "  ,\n",
       " Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group's shares up 11 pence to 248-1/2p by 1415 gmt.\n",
       "  ,\n",
       " \"It's all quite encouraging.]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMEdmpOozVI_"
   },
   "source": [
    "Notice that the returned object is still a `spacy` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "K-t-CTMLzVJD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hubnl9KzVJN"
   },
   "source": [
    "Apply to all texts (for use later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "D4ldqRWPzVJP"
   },
   "outputs": [],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in spacy_text.items():\n",
    "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "uK4F1nTCzVJd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       "  ,\n",
       " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\n",
       "  ,\n",
       " Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\n",
       "  ]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "pnFio7DDzVJm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHvcdrOfzVJp"
   },
   "source": [
    "Word tokenization means to split the sentence (or text) up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "M7yhJWfJzVJr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       " "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ubhNWlnzVKj"
   },
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5NyYQtZLzVKk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United,\n",
       " Biscuits,\n",
       " (,\n",
       " Holdings,\n",
       " ),\n",
       " Plc,\n",
       " more,\n",
       " than,\n",
       " doubled,\n",
       " its,\n",
       " profits,\n",
       " in,\n",
       " 1996,\n",
       " to,\n",
       " 109]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "289WZS32zVKv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4Sqgia2zVK0"
   },
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0R7wbGxzVK5"
   },
   "source": [
    "**Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWo_LBdNzVLB"
   },
   "source": [
    "Space offers build-in functionality for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NjeVMKvDzVLC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United',\n",
       " 'Biscuits',\n",
       " '(',\n",
       " 'Holdings',\n",
       " ')',\n",
       " 'Plc',\n",
       " 'more',\n",
       " 'than',\n",
       " 'double',\n",
       " 'its',\n",
       " 'profit',\n",
       " 'in',\n",
       " '1996',\n",
       " 'to',\n",
       " '109']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUnihWkKzVLa"
   },
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWRNB4e7zVLd"
   },
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UEqxYGvszVLe"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5yMJq8eKzVLj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unit',\n",
       " 'biscuit',\n",
       " '(',\n",
       " 'hold',\n",
       " ')',\n",
       " 'plc',\n",
       " 'more',\n",
       " 'than',\n",
       " 'doubl',\n",
       " 'it',\n",
       " 'profit',\n",
       " 'in',\n",
       " '1996',\n",
       " 'to',\n",
       " '109']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOw9KWKbzVLz"
   },
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zWzVOxHMzVL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United  |  United  |  unit\n",
      "Biscuits  |  Biscuits  |  biscuit\n",
      "(  |  (  |  (\n",
      "Holdings  |  Holdings  |  hold\n",
      ")  |  )  |  )\n",
      "Plc  |  Plc  |  plc\n",
      "more  |  more  |  more\n",
      "than  |  than  |  than\n",
      "doubled  |  double  |  doubl\n",
      "its  |  its  |  it\n",
      "profits  |  profit  |  profit\n",
      "in  |  in  |  in\n",
      "1996  |  1996  |  1996\n",
      "to  |  to  |  to\n",
      "109  |  109  |  109\n"
     ]
    }
   ],
   "source": [
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(original, ' | ', lemma, ' | ', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WST1UsmzVMA"
   },
   "source": [
    "In my experience it is usually best to use lemmatization instead of a stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "k1n7zXYRzVMC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOTi2TTEzVMD"
   },
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "OEiCmMoVzVMH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJvGsa0DzVMI"
   },
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONuqyRPvzVMQ"
   },
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_4B7AUIxzVMR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(United, 'PROPN'),\n",
       " (Biscuits, 'PROPN'),\n",
       " ((, 'PUNCT'),\n",
       " (Holdings, 'PROPN'),\n",
       " (), 'PUNCT'),\n",
       " (Plc, 'PROPN'),\n",
       " (more, 'ADV'),\n",
       " (than, 'ADP'),\n",
       " (doubled, 'VERB'),\n",
       " (its, 'PRON')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "D152zwLUzVMX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-gWZCBzVMY"
   },
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars  \n",
    "\n",
    "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Pc-zTsdzVMZ"
   },
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "JNY2U2VyzVMa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profits-in', 'in-1996', '1996-to', 'to-109', '109-million']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "jgy4iAQpzVMj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bif0cWA9zVMl"
   },
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9m4Rq7QzVMo"
   },
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "J7w9yH9hzVMp"
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "V1sYIDe-zVMu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, (, Holdings, ), Plc, doubled, profits, 1996, 109]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jpmZjNLXzVM-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, (, Holdings, ), Plc, more, than, doubled, its]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfyZoy-6zVNM"
   },
   "source": [
    "*Note* we can also remove punctuation in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8kd_rGgTzVNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, Holdings, Plc, more]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in example_sentence if not token.is_punct][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnGJNZJWzVNV"
   },
   "source": [
    "## Wrap everything into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN5Ti3nIzVNW"
   },
   "source": [
    "Below I will primarily use `SpaCy` directly. However, I also recommend to check out the high-level wrapper `Textacy`.\n",
    "\n",
    "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZKHceWXzVNX"
   },
   "source": [
    "### Quick `Textacy` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ikof63OBzVNY"
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hwjvM34vzVNe"
   },
   "outputs": [],
   "source": [
    "example_text = text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YDUV2X1LzVNj"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'textacy' has no attribute 'preprocess_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cleaned_text = \u001b[43mtextacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_text\u001b[49m(example_text, lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, fix_unicode=\u001b[38;5;28;01mTrue\u001b[39;00m, no_punct=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'textacy' has no attribute 'preprocess_text'"
     ]
    }
   ],
   "source": [
    "cleaned_text = textacy.preprocess_text(example_text, lowercase=True, fix_unicode=True, no_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M16-lmobzVNo"
   },
   "source": [
    "** Basic SpaCy text processing function **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R8Q6c5rzVNq"
   },
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer and remove top words\n",
    "3. Clean up the sentence using `textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "gk2lMON0zVNs"
   },
   "outputs": [],
   "source": [
    "def process_text_custom(text):\n",
    "    sentences = list(nlp(text).sents)\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop | token.is_punct | token.is_space])\n",
    "    return [nlp(' '.join(sentence)) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HwX9R_YzVNw"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "spacy_text_clean = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        lst.append(process_text_custom(text))\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OK3ybv1zVN5"
   },
   "source": [
    "Note that there are quite a lot of sentences (~52K) so this takes a bit of time (~ 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yLy5CSlzVN6"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for author, texts in spacy_text_clean.items():\n",
    "    for text in texts:\n",
    "        count += len(text)\n",
    "print('Number of sentences:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov_YJypWzVOO"
   },
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABxccKHezVOP"
   },
   "outputs": [],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Uo89NizNzVOY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJMPsyLwzVOZ"
   },
   "source": [
    "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "vefZY_WbzVOa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zvB_TQqwzVOb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkHbQjBWzVOc"
   },
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
    "\n",
    "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
    "\n",
    "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-57j90XDzVOd"
   },
   "outputs": [],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uUFq1a10zVOp"
   },
   "outputs": [],
   "source": [
    "[(i, i.label_) for i in parser(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFlwjICAzVOv"
   },
   "outputs": [],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTM4MjgTzVO5"
   },
   "outputs": [],
   "source": [
    "[(i, i.label_) for i in parser(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Phyb4rbpzVPM",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Odw0uoyzVPN"
   },
   "source": [
    "Using the build-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZs3HpaAzVPO"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0PW3WB9zVPU"
   },
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFiKKAZrzVPX"
   },
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5_ek_NkzVPY"
   },
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywReAZ9IzVPd"
   },
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXyhq2CIzVPh"
   },
   "outputs": [],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_vzcmowzVQL"
   },
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOjI6qqkzVQM"
   },
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhtvMohizVQP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    contains = True if re.search('million', sen.text) else False\n",
    "    if contains:\n",
    "        print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "_kAH64jTzVQc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGXV6jb_zVQd"
   },
   "source": [
    "Besides feature search there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ1waxmozVQf"
   },
   "outputs": [],
   "source": [
    "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "7CbA_Uj4zVQv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-h5gNZHzVQw"
   },
   "source": [
    "Using the `langdetect` package it is easy to detect the language of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NbTq0U3_IVA"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAEVJFYyzVQx"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlqALfz7zVQ0"
   },
   "outputs": [],
   "source": [
    "detect(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "frmtGDTPzVRE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDKalz3yzVRN"
   },
   "source": [
    "Using the `textstat` package we can compute various readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWC-8mwTzVRO"
   },
   "source": [
    "https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPo_GBg7C5CM"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVTjORoHzVRO"
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc9LPUVHzVRS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(textstat.flesch_reading_ease(example_paragraph))\n",
    "print(textstat.smog_index(example_paragraph))\n",
    "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
    "print(textstat.coleman_liau_index(example_paragraph))\n",
    "print(textstat.automated_readability_index(example_paragraph))\n",
    "print(textstat.dale_chall_readability_score(example_paragraph))\n",
    "print(textstat.difficult_words(example_paragraph))\n",
    "print(textstat.linsear_write_formula(example_paragraph))\n",
    "print(textstat.gunning_fog(example_paragraph))\n",
    "print(textstat.text_standard(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPO_VVHrzVRZ"
   },
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07d0tYdGDCn-"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install fuzzywuzzy\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m81Jm4m4zVRf"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "k2vblIgozVRi",
    "outputId": "db8a2323-fc05-4840-88d0-ad0d2580e133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "a6c5W0NVzVRn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTNiB1uqzVRo"
   },
   "source": [
    "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
    "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
    "\n",
    "In essence this technique is very simple to program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ_Gm7v-zVRp"
   },
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WboJdEXxzVRs"
   },
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "DZFDFOIezVRv",
    "outputId": "c3230d09-7438-475b-a3d5-9879880fae75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 0\n",
      "first 0\n",
      "most 0\n",
      "be 1\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGv6r4tAzVR9"
   },
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "45YS4EmDzVR-",
    "outputId": "abdb881c-f759-4d0f-f626-a702615d424a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ['great', 'increase']\n",
    "neg = ['bad', 'decrease']\n",
    "\n",
    "sentence = '''According to Trump everything is great, great,\n",
    "and great even though his popularity is seeing a decrease.'''\n",
    "\n",
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)\n",
    "\n",
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)\n",
    "\n",
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TybOMjAQzVSH"
   },
   "outputs": [],
   "source": [
    "sentence = '''According to Trump everything is great, great,\n",
    "and great even though his popularity is seeing a decrease.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CCBJuvipzVSK",
    "outputId": "9121dd43-79f5-448f-98fe-dae23503b390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "by03-OIVzVSR",
    "outputId": "e223d80b-d636-4a9f-c537-d9ab5850cd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EMiSUvOYzVSX",
    "outputId": "652a7451-3cce-4167-d3e2-5a3276a10f41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCmhJIbszVSc"
   },
   "source": [
    "Getting the total number of words is also easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cj1GiPsxzVSd",
    "outputId": "492bbe68-1976-4c9b-f4bf-7c02901c68ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parser(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NCVqhxKzVTo"
   },
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYMkGWDtzVTo"
   },
   "source": [
    "We can also save the count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV75kU7nzVTq"
   },
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ctUf44xzVTt",
    "outputId": "5eaca5e7-9ac4-4019-9d5f-6f870a6f7c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'increase': 0}"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "HV_eHMHxzVUu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Or4hmJKyzVVC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_cQOeC2zVVH"
   },
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Note 1: these functions also already includes a lot of preprocessing options (e.g. ngrames, remove stop words, accent stripper).\n",
    "\n",
    "Note 2: example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo5GO5o-zVVY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR6YA_aUzVVe"
   },
   "source": [
    "### Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpy0oD0NzVVf"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"The sky is blue.\"\n",
    "doc_2 = \"The sun is bright today.\"\n",
    "doc_3 = \"The sun in the sky is bright.\"\n",
    "doc_4 = \"We can see the shining sun, the bright sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVTX0FTezVVp"
   },
   "source": [
    "Calculate term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wMnrgmlzVVq"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "0Lm2LlYNzVVy",
    "outputId": "a274f7b1-4d5f-4c39-dddb-8b6f0d5c0f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue', 'bright', 'shining', 'sky', 'sun', 'today']\n",
      "[1 0 0 1 0 0]\n",
      "[0 1 0 0 1 1]\n",
      "[0 1 0 1 1 0]\n",
      "[0 1 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "for doc_tf_vector in tf.toarray():\n",
    "    print(doc_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GPRbzfvJzVWT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQR-cppDzVWU"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "DP7xLrCKzVWY",
    "outputId": "114f3664-b792-404d-c230-cc3cd4107ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
      "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
      "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
      "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRCeQpwVzVWd"
   },
   "source": [
    "### More elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLQYa1iFzVWe"
   },
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x.text for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XueJvbeZzVWh",
    "outputId": "a408eada-ea06-496a-c7c5-6cdbab7a401b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99XHAHCFzVWl"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "W5oOQ889zVWp",
    "outputId": "1d1acfd4-66f1-4f3a-f2ef-3392878b26f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 24130\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Akku50qvzVWt",
    "outputId": "b239fb66-e499-4465-d6a3-935a1cd056ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x24130 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 444178 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "89OXigz8zVWy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VxwA3sptzVWz",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnpmYO8izVW0"
   },
   "source": [
    "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Y8RousiXzVW1",
    "outputId": "a3c6220a-e17d-43a4-d64f-2582eac73515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPBPtxWqzVW_"
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXtzmP__zVXE"
   },
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "fP_TJbkwzVXI",
    "outputId": "1f285964-91d3-4ce0-a503-c975d13cbf88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI-mzHzkzVXT"
   },
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "G-BUV7FPzVXW",
    "outputId": "fb21fdd6-bd26-4318-fd44-18b63572c0d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ81a8jPzVXd"
   },
   "source": [
    "Find words most similar to 'mother':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "h7Dk7RBgzVXd",
    "outputId": "50dba550-4078-4c3a-9536-e72c3c977815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9841663837432861), ('husband', 0.9670721292495728), ('wife', 0.9487313032150269), ('friend', 0.9323579668998718), ('son', 0.9275298714637756), ('nickname', 0.9200363159179688), ('eagle', 0.9182674288749695), ('addiction', 0.9054847955703735), ('voice', 0.9040984511375427), ('patient', 0.8997060060501099)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmI5IFgmzVXh"
   },
   "source": [
    "Find the odd one out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "V-mbhBujzVXi",
    "outputId": "30d71cdb-66a8-4829-a592-d47b9ccb4663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "2nq_qc0AzVXx",
    "outputId": "9c69724d-8635-4b1d-8ba6-bac69e3cd5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8NJT85TzVX4"
   },
   "source": [
    "Retrieve vector representation of the word \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "84jajA9wzVX5",
    "outputId": "55fbab38-0c14-4362-f705-0253589551e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2541619 ,  2.2159684 ,  1.047784  , -0.71238947, -0.61424005,\n",
       "       -0.2838337 ,  0.22731484,  0.64372134, -0.5807566 , -0.2922792 ,\n",
       "       -0.4993856 ,  0.19945972,  0.5109125 , -0.44036752, -0.31287178,\n",
       "       -0.05132972,  1.0744963 ,  0.49401098, -0.7575154 , -0.7520932 ,\n",
       "       -0.47252244,  0.15777719,  0.01892141, -0.13521086,  0.13225318,\n",
       "        0.01569277,  0.8201043 , -0.18367213, -0.5115785 ,  0.0950046 ,\n",
       "       -0.901529  , -0.00576999, -0.18296817, -0.7227348 , -0.36876544,\n",
       "        0.12904815, -0.49410135, -0.16567625,  0.25800195, -0.9476387 ,\n",
       "        0.6677448 , -0.66520613,  0.15521108, -0.05746429, -0.66669667,\n",
       "        1.1473489 , -0.30393326,  0.27609795,  0.03071395,  0.21279913,\n",
       "       -0.15011618,  0.06648927,  0.4653522 , -0.06295931, -0.59686804,\n",
       "        0.22332567,  0.52038115,  0.08707199, -0.03864726, -1.1777682 ,\n",
       "       -0.6586736 ,  0.7845623 ,  0.54146487, -1.0455779 , -0.5684107 ,\n",
       "        0.0466442 ,  0.44047025, -0.28766677, -0.17281069, -0.18058509,\n",
       "        0.21136013,  0.95359594, -0.66299623, -0.28144175,  0.01736428,\n",
       "        0.9233736 ,  0.533335  ,  0.47873688,  0.30168334, -1.0639472 ,\n",
       "        0.03699052,  0.32431152,  0.9761932 ,  0.6572108 , -0.32395357,\n",
       "       -0.21990493,  0.82805634,  0.27187476, -0.50445306,  0.06266934,\n",
       "       -0.05255144,  0.45138708,  0.34316576, -0.5600966 , -0.52912354,\n",
       "        0.18519725, -0.36004648, -0.4918508 , -0.04166657,  0.17255953],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "6Eht6I1JzVYG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iQTGfmcHzVYH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paklyzdrzVYI"
   },
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "mY1gvLnozVYI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vyvdgww1zVYJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47LgZmhMzVYS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyNejarAzVYr"
   },
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwRfyuMazVYv"
   },
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x.text for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjMTWkorzVYz"
   },
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "Ukjf2aY0zVY2",
    "outputId": "47e30415-9e0d-40d5-ee99-17a54813804c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>WilliamKazer</td>\n",
       "      <td>United States China verge breakthrough testy t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>SimonCowell</td>\n",
       "      <td>Lloyd London announce Tuesday backer time meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>DarrenSchuettler</td>\n",
       "      <td>Canada security regulator say Thursday probe B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>TheresePoletti</td>\n",
       "      <td>Microsoft Corp. big corporate foe include Inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>JonathanBirt</td>\n",
       "      <td>drug discovery company Chiroscience Group Plc ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                               text\n",
       "925       WilliamKazer  United States China verge breakthrough testy t...\n",
       "584        SimonCowell  Lloyd London announce Tuesday backer time meet...\n",
       "1000  DarrenSchuettler  Canada security regulator say Thursday probe B...\n",
       "968     TheresePoletti  Microsoft Corp. big corporate foe include Inte...\n",
       "1600      JonathanBirt  drug discovery company Chiroscience Group Plc ..."
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aZZcgY0zVY_"
   },
   "source": [
    "### Split the sample into a training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8Gr-Y1fzVZA"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0_m0XbFXzVZD",
    "outputId": "bd2c77ee-c944-4db4-9d51-9c7c2c803e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VuusccEzVZG"
   },
   "source": [
    "### Train and evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKkWHuvZzVZI"
   },
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASW0B6sjzVZI"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "uSwaI8sdzVZW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Na√Øve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3l7pDZpzVZX"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXsEg9LXzVZZ"
   },
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O29a1MF5zVZZ"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "\n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdfKwR-3zVZc"
   },
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t5R-dmEqzVZq",
    "outputId": "e63b90dd-0c4d-43f6-d0a7-4dab398141b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.8405\n",
      "Accuracy on testing set:\n",
      "0.696\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.88      0.70      0.78        10\n",
      "       AlanCrosby       0.55      0.67      0.60         9\n",
      "   AlexanderSmith       0.71      0.56      0.63         9\n",
      "  BenjaminKangLim       0.67      0.18      0.29        11\n",
      "    BernardHickey       0.75      0.67      0.71         9\n",
      "      BradDorfman       0.90      0.90      0.90        10\n",
      " DarrenSchuettler       0.73      0.62      0.67        13\n",
      "      DavidLawder       0.89      0.62      0.73        13\n",
      "    EdnaFernandes       1.00      0.41      0.58        17\n",
      "      EricAuchard       0.80      0.44      0.57         9\n",
      "   FumikoFujisaki       0.86      0.86      0.86         7\n",
      "   GrahamEarnshaw       0.60      1.00      0.75         9\n",
      " HeatherScoffield       1.00      0.58      0.74        12\n",
      "       JanLopatka       0.62      0.45      0.53        11\n",
      "    JaneMacartney       0.40      0.50      0.44         8\n",
      "     JimGilchrist       0.92      1.00      0.96        12\n",
      "   JoWinterbottom       0.64      0.90      0.75        10\n",
      "         JoeOrtiz       0.70      0.88      0.78         8\n",
      "     JohnMastrini       0.58      0.78      0.67         9\n",
      "     JonathanBirt       0.83      0.71      0.77        14\n",
      "      KarlPenhaul       1.00      1.00      1.00        10\n",
      "        KeithWeir       0.45      0.83      0.59         6\n",
      "   KevinDrawbaugh       0.20      0.40      0.27         5\n",
      "    KevinMorrison       0.71      0.91      0.80        11\n",
      "    KirstinRidley       0.64      0.64      0.64        14\n",
      "KouroshKarimkhany       0.90      0.90      0.90        10\n",
      "        LydiaZajc       0.58      0.78      0.67         9\n",
      "   LynneO'Donnell       0.86      0.86      0.86        14\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       0.67      0.80      0.73        10\n",
      "     MarkBendeich       0.86      0.67      0.75         9\n",
      "       MartinWolk       0.67      0.20      0.31        10\n",
      "     MatthewBunce       1.00      1.00      1.00        10\n",
      "    MichaelConnor       0.67      0.60      0.63        10\n",
      "       MureDickie       0.67      0.46      0.55        13\n",
      "        NickLouth       0.64      0.90      0.75        10\n",
      "  PatriciaCommins       0.57      0.89      0.70         9\n",
      "    PeterHumphrey       0.44      0.70      0.54        10\n",
      "       PierreTran       1.00      0.62      0.76        13\n",
      "       RobinSidel       1.00      0.78      0.88         9\n",
      "     RogerFillion       1.00      0.88      0.93         8\n",
      "      SamuelPerry       0.54      0.64      0.58        11\n",
      "     SarahDavison       0.83      0.42      0.56        12\n",
      "      ScottHillis       0.22      0.67      0.33         3\n",
      "      SimonCowell       0.71      0.50      0.59        10\n",
      "         TanEeLyn       0.56      0.62      0.59         8\n",
      "   TheresePoletti       0.83      1.00      0.91        10\n",
      "       TimFarrand       0.60      1.00      0.75         9\n",
      "       ToddNissen       0.53      0.89      0.67         9\n",
      "     WilliamKazer       0.20      0.12      0.15         8\n",
      "\n",
      "        micro avg       0.70      0.70      0.70       500\n",
      "        macro avg       0.71      0.70      0.68       500\n",
      "     weighted avg       0.74      0.70      0.69       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ximtzrGXzVaJ"
   },
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gDZnSjVczVaJ",
    "outputId": "9357f7ab-b449-4534-e0e4-d6eecd962104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWUzMTi0zVaN"
   },
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDAtLlu8zVaO"
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "OE1Kcc5HzVaU",
    "outputId": "9bcfe54c-9e47-4f7f-b764-ff0b95b64a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: EricAuchard\n",
      "Predicted author: EricAuchard\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zEih7o-RzVaa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3tWA3FizVab"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk2xP8ZdzVal"
   },
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtWDmCKFzVal"
   },
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "\n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUnKn1VfzVao"
   },
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCMbZQU7zVas"
   },
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NYAILjS5zVat",
    "outputId": "980ffe75-c52e-4d2b-8b93-af020b1046ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.998\n",
      "Accuracy on testing set:\n",
      "0.828\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.82      0.90      0.86        10\n",
      "       AlanCrosby       0.78      0.78      0.78         9\n",
      "   AlexanderSmith       1.00      0.78      0.88         9\n",
      "  BenjaminKangLim       0.78      0.64      0.70        11\n",
      "    BernardHickey       1.00      0.89      0.94         9\n",
      "      BradDorfman       0.91      1.00      0.95        10\n",
      " DarrenSchuettler       1.00      1.00      1.00        13\n",
      "      DavidLawder       0.82      0.69      0.75        13\n",
      "    EdnaFernandes       1.00      0.82      0.90        17\n",
      "      EricAuchard       0.62      0.89      0.73         9\n",
      "   FumikoFujisaki       1.00      0.86      0.92         7\n",
      "   GrahamEarnshaw       0.73      0.89      0.80         9\n",
      " HeatherScoffield       0.92      1.00      0.96        12\n",
      "       JanLopatka       0.78      0.64      0.70        11\n",
      "    JaneMacartney       0.50      0.38      0.43         8\n",
      "     JimGilchrist       1.00      0.92      0.96        12\n",
      "   JoWinterbottom       0.75      0.90      0.82        10\n",
      "         JoeOrtiz       0.78      0.88      0.82         8\n",
      "     JohnMastrini       0.67      0.89      0.76         9\n",
      "     JonathanBirt       0.93      0.93      0.93        14\n",
      "      KarlPenhaul       1.00      1.00      1.00        10\n",
      "        KeithWeir       1.00      0.67      0.80         6\n",
      "   KevinDrawbaugh       0.56      1.00      0.71         5\n",
      "    KevinMorrison       0.85      1.00      0.92        11\n",
      "    KirstinRidley       0.92      0.79      0.85        14\n",
      "KouroshKarimkhany       0.89      0.80      0.84        10\n",
      "        LydiaZajc       1.00      0.78      0.88         9\n",
      "   LynneO'Donnell       0.93      0.93      0.93        14\n",
      "  LynnleyBrowning       1.00      1.00      1.00        10\n",
      "  MarcelMichelson       0.83      1.00      0.91        10\n",
      "     MarkBendeich       0.90      1.00      0.95         9\n",
      "       MartinWolk       0.70      0.70      0.70        10\n",
      "     MatthewBunce       1.00      1.00      1.00        10\n",
      "    MichaelConnor       0.82      0.90      0.86        10\n",
      "       MureDickie       0.78      0.54      0.64        13\n",
      "        NickLouth       0.90      0.90      0.90        10\n",
      "  PatriciaCommins       1.00      1.00      1.00         9\n",
      "    PeterHumphrey       0.45      0.50      0.48        10\n",
      "       PierreTran       1.00      0.85      0.92        13\n",
      "       RobinSidel       0.90      1.00      0.95         9\n",
      "     RogerFillion       1.00      0.88      0.93         8\n",
      "      SamuelPerry       0.88      0.64      0.74        11\n",
      "     SarahDavison       0.56      0.42      0.48        12\n",
      "      ScottHillis       0.43      1.00      0.60         3\n",
      "      SimonCowell       1.00      0.70      0.82        10\n",
      "         TanEeLyn       0.67      0.75      0.71         8\n",
      "   TheresePoletti       1.00      1.00      1.00        10\n",
      "       TimFarrand       0.64      1.00      0.78         9\n",
      "       ToddNissen       0.62      0.56      0.59         9\n",
      "     WilliamKazer       0.45      0.62      0.53         8\n",
      "\n",
      "        micro avg       0.83      0.83      0.83       500\n",
      "        macro avg       0.83      0.83      0.82       500\n",
      "     weighted avg       0.84      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX6wlU59zVaw"
   },
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JJQSupEhzVax",
    "outputId": "92ef1d98-f274-4766-a775-e6a9c0594572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWGWAhmSzVa6"
   },
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b17BsOqOzVa7"
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "hn2k9wDHzVa-",
    "outputId": "26437c3b-869d-4787-c177-666bde1c8b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: EricAuchard\n",
      "Predicted author: EricAuchard\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "vpRZqHSNzVbB",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQ52YFLzVbC"
   },
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4wtBtFEzVbC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wJgG3nczVbH"
   },
   "source": [
    "First we define the options that should be tried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBSoERHDzVbJ"
   },
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = { 'vect__stop_words': ['english'],\n",
    "                'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "             'clf__gamma' : [0.2, 0.3, 0.4],\n",
    "             'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tp-NLigBzVbL"
   },
   "source": [
    "Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "F1VQ-VGjzVbM",
    "outputId": "9a509d22-165c-42c1-8d05-ecff30cd59b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(clf_search, param_grid=parameters, scoring=make_scorer(f1_score, average='micro'), n_jobs=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt2CvCxVzVbQ"
   },
   "source": [
    "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZg7EZTBzVbQ"
   },
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "NvIi77ZczVbW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "yGyPcY1JzVbh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeVuhs6ZzVbh"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEfe9WZ-zVbp"
   },
   "source": [
    "Vectorizer (using countvectorizer for the sake of example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cx4JnRCXzVbp"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english', max_df=0.8)\n",
    "tf_large = vectorizer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC05UAkVzVbs"
   },
   "source": [
    "Run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crHxv3OKzVbs"
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ECCbQnczVbx"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                n_jobs=1)\n",
    "lda_fitted = lda.fit_transform(tf_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WWs2PfDzVb0"
   },
   "source": [
    "Visualize top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHgccEPWzVb2"
   },
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words):\n",
    "    out_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUApo9RHzVb_"
   },
   "outputs": [],
   "source": [
    "result_df = save_top_words(lda, vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXjV_kHYzVcC"
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "rtrOd5e2zVci",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS_91lazzVck"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rxaaZ9fzVco"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda, tf_large, vectorizer, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWw0uej5wo-"
   },
   "source": [
    "\n",
    "Credit: [Ties de Kok](https://github.com/TiesdeKok)\n",
    "\n",
    "Repository: [Python NLP](https://github.com/TiesdeKok/Python_NLP_Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyoPQUDx5ylY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8NCVqhxKzVTo"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
