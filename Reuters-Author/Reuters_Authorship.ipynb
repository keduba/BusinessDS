{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqtE9qCqztTi"
   },
   "source": [
    "# Week 2: Python Business Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGotMBbXz1Pl"
   },
   "source": [
    "See the Repository for Future Work: https://github.com/firmai/python-business-analytics or\n",
    "\n",
    "Sign up to the mailing list: https://mailchi.mp/ec4942d52cc5/firmai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki1UWVLczVCC"
   },
   "source": [
    "# Text Mining NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeorQOsbzVCQ"
   },
   "source": [
    "# *Introduction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeKqM74hzVCR"
   },
   "source": [
    "This notebook contains code examples to get you started with Natural Language Processing (NLP) / Text Mining for Research and Data Science purposes.  \n",
    "\n",
    "In the large scheme of things there are roughly 4 steps:  \n",
    "\n",
    "1. Identify a data source  \n",
    "2. Gather the data  \n",
    "3. Process the data  \n",
    "4. Analyze the data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7XU4fuOzVCT"
   },
   "source": [
    "## Note: companion slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W8FwKUgzVCZ"
   },
   "source": [
    "# *Elements / topics that are discussed in this notebook: *\n",
    "\n",
    "\n",
    "<img style=\"float: left\" src=\"https://i.imgur.com/c3aCZLA.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD7WDs30zVCa"
   },
   "source": [
    "# *Table of Contents*  <a id='toc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqFVyXnDzVCc"
   },
   "source": [
    "* [Primer on NLP tools](#tool_primer)     \n",
    "* [Process + Clean text](#proc_clean)   \n",
    "    * [Normalization](#normalization)\n",
    "        * [Deal with unwanted characters](#unwanted_char)\n",
    "        * [Sentence segmentation](#sentence_seg)   \n",
    "        * [Word tokenization](#word_token)\n",
    "        * [Lemmatization & Stemming](#lem_and_stem)\n",
    "    * [Language modeling](#lang_model)\n",
    "        * [Part-of-Speech tagging](#pos_tagging)\n",
    "        * [Uni-Gram & N-Grams](#n_grams)\n",
    "        * [Stop words](#stop_words)\n",
    "* [Direct feature extraction](#feature_extract)\n",
    "    * [Feature search](#feature_search)\n",
    "        * [Entity recognition](#entity_recognition)\n",
    "        * [Pattern search](#pattern_search)\n",
    "    * [Text evaluation](#text_eval)\n",
    "        * [Language](#language)\n",
    "        * [Dictionary counting](#dict_counting)\n",
    "        * [Readability](#readability)\n",
    "* [Represent text numerically](#text_numerical)\n",
    "    * [Bag of Words](#bows)\n",
    "        * [TF-IDF](#tfidf)\n",
    "    * [Word Embeddings](#word_embed)\n",
    "        * [Word2Vec](#Word2Vec)\n",
    "* [Statistical models](#stat_models)\n",
    "    * [\"Traditional\" machine learning](#trad_ml)\n",
    "        * [Supervised](#trad_ml_supervised)\n",
    "            * [Na√Øve Bayes](#trad_ml_supervised_nb)\n",
    "            * [Support Vector Machines (SVM)](#trad_ml_supervised_svm)\n",
    "        * [Unsupervised](#trad_ml_unsupervised)\n",
    "            * [Latent Dirichilet Allocation (LDA)](#trad_ml_unsupervised_lda)\n",
    "            * [pyLDAvis](#trad_ml_unsupervised_pyLDAvis)\n",
    "* [Model Selection and Evaluation](#trad_ml_eval)\n",
    "* [Neural Networks](#nn_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zJnQENtazVCf",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Primer on NLP tools</span><a id='tool_primer'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbPFc05MzVCh"
   },
   "source": [
    "There are many tools available for NLP purposes.  \n",
    "The code examples below are based on what I personally like to use, it is not intended to be a comprehsnive overview.  \n",
    "\n",
    "Besides build-in Python functionality I will use / demonstrate the following packages:\n",
    "\n",
    "**Standard NLP libraries**:\n",
    "1. `Spacy` and the higher-level wrapper `Textacy`\n",
    "2. `NLTK` and the higher-level wrapper `TextBlob`\n",
    "\n",
    "*Note: besides installing the above packages you also often have to download (model) data . Make sure to check the documentation!*\n",
    "\n",
    "**Standard machine learning library**:\n",
    "\n",
    "1. `scikit learn`\n",
    "\n",
    "**Specific task libraries**:\n",
    "\n",
    "There are many, just a couple of examples:\n",
    "\n",
    "1. `pyLDAvis` for visualizing LDA)\n",
    "2. `langdetect` for detecting languages\n",
    "3. `fuzzywuzzy` for fuzzy text matching\n",
    "4. `textstat` to calculate readability statistics\n",
    "5. `Gensim` for topic modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Jj3UkiLSzVCj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Get some example data</span><a id='example_data'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po-N0OZKzVCm"
   },
   "source": [
    "There are many example datasets available to play around with, see for example this great repository:  \n",
    "https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=text&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9axpUNqqzVCp"
   },
   "source": [
    "The data that I will use for most of the examples is the \"Reuter_50_50 Data Set\" that is used for author identification experiments.\n",
    "\n",
    "See the details here: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_EQ6LkxzVCr"
   },
   "source": [
    "### Download and load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fPhFIZozVCt"
   },
   "source": [
    "Can't follow what I am doing here? Please see my [Python tutorial](https://github.com/TiesdeKok/LearnPythonforResearch) (although the `zipfile` and `io` operations are not very relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p0ptU3H_7hdD",
    "outputId": "d7a449c4-6c2b-48a0-edce-4fb8f20f81ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m55 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m53 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m57 packages\u001b[0m \u001b[2min 439ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 1.73s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangdetect\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 459ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 7.92s\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgensim\u001b[0m\u001b[2m==4.4.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m59 packages\u001b[0m \u001b[2min 304ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfuzzywuzzy\u001b[0m\u001b[2m==0.18.0\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/koollio/.cache/uv/builds-v0/.tmpBgEVsp` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m65 packages\u001b[0m \u001b[2min 639ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 534ms\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 74ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfuncy\u001b[0m\u001b[2m==2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumexpr\u001b[0m\u001b[2m==2.14.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyldavis\u001b[0m\u001b[2m==3.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv add textacy\n",
    "# !uv add langdetect\n",
    "# !uv add gensim\n",
    "# !uv add fuzzywuzzy\n",
    "# !uv add pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kEwYopmNzVCv"
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-8KFZ4qzVC-"
   },
   "source": [
    "*Download and extract the zip file with the data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XZjfB-bUzVDB"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('C50test'):\n",
    "    r = requests.get(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00217/C50.zip\")\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVRLr8EbzVDR"
   },
   "source": [
    "*Load the data into memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rzwaWd8HzVDT"
   },
   "outputs": [],
   "source": [
    "folder_dict = {'test' : 'C50test'}\n",
    "text_dict = {'test' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "USqLZVb4zVDg"
   },
   "outputs": [],
   "source": [
    "for label, folder in folder_dict.items():\n",
    "    authors = os.listdir(folder)\n",
    "    for author in authors:\n",
    "        text_files = os.listdir(os.path.join(folder, author))\n",
    "        for file in text_files:\n",
    "            with open(os.path.join(folder, author, file), 'r') as text_file:\n",
    "                text_dict[label].setdefault(author, []).append(' '.join(text_file.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pbB8sAqzVEA"
   },
   "source": [
    "*Note: the text comes pre-split per sentence, for the sake of example I undo this through `' '.join(text_file.readlines()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-jZczhS4zVEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\\n Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\\n Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group\\'s shares up 11 pence to 248-1/2p by 1415 gmt.\\n \"It\\'s all quite encouraging. The way they are analysing and managing the business is very much more in line with what the market demands,\" said Richard Workman, an analyst at ABN-AMRO Hoare Govett.\\n The company said its recovery was strongly led by its British operations, while in mainland Europe, it started to realise the potential of businesses acquired earlier in the 1990\\'s.\\n UB\\'s British business increased profits before exceptional items by 12 percent to 113.7 million pounds as renewed consumer confidence encouraged shoppers to step up brands.\\n In an interview UB\\'s managing director Eric Nicoli said UB Has now entered a phase of consolidation after a hectic disposal programme.\\n \"The emphasis is on organic growth as we are now in a consolidation phase,\" said Nicoli.\\n Over the last two years UB has sold its American snack operation Keebler, pulled out of Portugal and exited from Spanish snacks. It has also sold one of its Italian snack businesses, withdrawn from Turkey and Brazil and sold Ross Vegetables Products business in Britain.\\n UB expects its margins in Europe to remain firm and to move up over the next two to three years, said Nicoli.\\n \"We expect margins to move to 10 percent in all our businesses over a two or three year period,\" he said.\\n (Corrects to make clear that company isn\\'t expecting European markets alone to move by 10 percent.)\\n UB margins are currently at around 10 percent in Britain and four percent in mainland Europe.\\n In 1997 the company plans to build on the turnaround \"all our markets continue to be very competitive but we expect another year of good progress,\" said Nicoli.\\n The total dividend payout was increased by 10 pence per share from 9.8p.\\n The company\\'s leading biscuit brands include McVities in Britain, Verkade in Holland and Oxford in Denmark. Biscuit sales growth came from the launch of Go Ahead! a range of low fat products. UB also owns snacks KP Nuts, Skips and Hula Hoops.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hcDfreU4zVE4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\"> 8.0. Process + Clean text</span><a id='proc_clean'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-wjbBW-zVE5"
   },
   "source": [
    "## Convert the text into a NLP representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDY9_DR8zVFA"
   },
   "source": [
    "We can use the text directly, but if want to use packages like `spacy` and `textblob` we first have to convert the text into a corresponding object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP_gh4-XzVFC"
   },
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6lbCCb4zVFE"
   },
   "source": [
    "**Note:** depending on the way that you installed the language models you will need to import it differently:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z7q_0kpzVFG"
   },
   "source": [
    "```\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "```\n",
    "OR\n",
    "```\n",
    "import en_core_web_sm\n",
    "parser = en_core_web_sm.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pj4GrH-1zVFI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE4wAqeszVFW"
   },
   "source": [
    "Convert all text in the \"test\" sample to a `spacy` `doc` object using `parser()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v2XCZmFbzVFY"
   },
   "outputs": [],
   "source": [
    "spacy_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    spacy_text[author] = [nlp(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CUP_HWzxzVFk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gR79Z6jzVFy"
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Im0zp4IKzVF0"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_BSfJKezVF7"
   },
   "source": [
    "We can apply basic `nltk` operations directly to the text so we don't need to convert first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzbRGbaazVGD"
   },
   "source": [
    "### TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fVYhYEDUzVGI"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h9cyqnjzVGT"
   },
   "source": [
    "Convert all text in the \"test\" sample to a `TextBlob` object using `TextBlob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "miCnaBPCzVGY"
   },
   "outputs": [],
   "source": [
    "textblob_text = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    textblob_text[author] = [TextBlob(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_STJ4QWizVGr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textblob_text['TimFarrand'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "s3eNuaFczVG0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Normalization</span><a id='normalization'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHlPtBa6zVG4"
   },
   "source": [
    "**Text normalization** describes the task of transforming the text into a different (more comparable) form.  \n",
    "\n",
    "This can imply many things, I will show a couple of things below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Gui1Vbh-zVG9",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Deal with unwanted characters</span><a id='unwanted_char'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJqf_-unzVG_"
   },
   "source": [
    "You will often notice that there are characters that you don't want in your text.  \n",
    "\n",
    "Let's look at this sentence for example:\n",
    "\n",
    "> \"Shares in brewing-to-leisure group Bass Plc are likely to be held back until Britain\\'s Trade and Industry secretary Ian Lang decides whether to allow its proposed merge with brewer Carlsberg-Tetley, said analysts.\\n Earlier Lang announced the Bass deal would be referred to the Monoplies and Mergers\"\n",
    "\n",
    "You notice that there are some `\\` and `\\n` in there. These are used to define how a string should be displayed, if we print this text we get:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nI528gkQzVHD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ot36oM-YzVHM",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
      " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee\n"
     ]
    }
   ],
   "source": [
    "print(text_dict['test']['TimFarrand'][0][:298])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48pFgqarzVHU"
   },
   "source": [
    "If we want to analyze text we often don't care about the visual representation. They might actually cause problems!  \n",
    "\n",
    "**So how do we remove them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foq49UGGzVHX"
   },
   "source": [
    "In many cases it is sufficient to simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wjf30lSZzVHZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products. United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Kee'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:298].replace('\\n', '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyl9zCsAzVHn"
   },
   "source": [
    "Sometimes, however, the problem arises because of encoding/decoding problems.  \n",
    "\n",
    "In those cases you can usually do something like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8RGwL1nKzVHp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This is some  text that has to be cleaned! it's annoying!\"\n"
     ]
    }
   ],
   "source": [
    "problem_sentence = 'This is some \\\\u03c0 text that has to be cleaned\\\\u2026! it\\\\u0027s annoying!'\n",
    "print(problem_sentence.encode().decode('unicode_escape').encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hO0E7tL9zVH0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Sentence segmentation</span><a id='sentence_seg'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B7YG1mczVH-"
   },
   "source": [
    "Sentence segmentation means the task of splitting up the piece of text by sentence.  \n",
    "\n",
    "You could do this by splitting on the `.` symbol, but dots are used in many other cases as well so it is not very robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0ufvwboczVIA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products',\n",
       " '\\n United, which owns brands such as McVities biscuits and KP nuts but has exited from its U',\n",
       " 'S',\n",
       " ' Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84',\n",
       " '7 million pounds in 1996 compared with 150',\n",
       " '3 million in 1995',\n",
       " '\\n Sales rose by three percent to 1',\n",
       " '887 billion and trading profits grew four pe']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dict['test']['TimFarrand'][0][:550].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtXXcCidzVIq"
   },
   "source": [
    "It is better to use a more sophisticated implementation such as the one by `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Xhqki3D5zVIr"
   },
   "outputs": [],
   "source": [
    "example_paragraph = spacy_text['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "kpMnK7ZdzVIx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       "  ,\n",
       " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\n",
       "  ,\n",
       " Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\n",
       "  ,\n",
       " Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group's shares up 11 pence to 248-1/2p by 1415 gmt.\n",
       "  ,\n",
       " \"It's all quite encouraging.]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list = [s for s in example_paragraph.sents]\n",
    "sentence_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMEdmpOozVI_"
   },
   "source": [
    "Notice that the returned object is still a `spacy` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "K-t-CTMLzVJD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hubnl9KzVJN"
   },
   "source": [
    "Apply to all texts (for use later on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "D4ldqRWPzVJP"
   },
   "outputs": [],
   "source": [
    "spacy_sentences = {}\n",
    "for author, text_list in spacy_text.items():\n",
    "    spacy_sentences[author] = [list(text.sents) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "uK4F1nTCzVJd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       "  ,\n",
       " United, which owns brands such as McVities biscuits and KP nuts but has exited from its U.S. Keebler subsidiary, said total exceptional charges, mainly from the loss on disposal of businesses, amounted to 84.7 million pounds in 1996 compared with 150.3 million in 1995.\n",
       "  ,\n",
       " Sales rose by three percent to 1.887 billion and trading profits grew four percent to 129.2 million.\n",
       "  ]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentences['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "pnFio7DDzVJm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word tokenization</span><a id='word_token'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHvcdrOfzVJp"
   },
   "source": [
    "Word tokenization means to split the sentence (or text) up into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "M7yhJWfJzVJr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "United Biscuits (Holdings) Plc more than doubled its profits in 1996 to 109 million pounds ($174 million) before tax and exceptional items, reflecting a simpler and slimmed-down portfolio of products.\n",
       " "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ubhNWlnzVKj"
   },
   "source": [
    "A word is called a `token` in this context (hence `tokenization`), using `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5NyYQtZLzVKk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United,\n",
       " Biscuits,\n",
       " (,\n",
       " Holdings,\n",
       " ),\n",
       " Plc,\n",
       " more,\n",
       " than,\n",
       " doubled,\n",
       " its,\n",
       " profits,\n",
       " in,\n",
       " 1996,\n",
       " to,\n",
       " 109]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [token for token in example_sentence]\n",
    "token_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "289WZS32zVKv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Lemmatization & Stemming</span><a id='lem_and_stem'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4Sqgia2zVK0"
   },
   "source": [
    "In some cases you want to convert a word (i.e. token) into a more general representation.  \n",
    "\n",
    "For example: convert \"car\", \"cars\", \"car's\", \"cars'\" all into the word `car`.\n",
    "\n",
    "This is generally done through lemmatization / stemming (different approaches trying to achieve a similar goal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0R7wbGxzVK5"
   },
   "source": [
    "**Spacy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWo_LBdNzVLB"
   },
   "source": [
    "Space offers build-in functionality for lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NjeVMKvDzVLC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United',\n",
       " 'Biscuits',\n",
       " '(',\n",
       " 'Holdings',\n",
       " ')',\n",
       " 'Plc',\n",
       " 'more',\n",
       " 'than',\n",
       " 'double',\n",
       " 'its',\n",
       " 'profit',\n",
       " 'in',\n",
       " '1996',\n",
       " 'to',\n",
       " '109']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = [token.lemma_ for token in example_sentence]\n",
    "lemmatized[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUnihWkKzVLa"
   },
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWRNB4e7zVLd"
   },
   "source": [
    "Using the NLTK libary we can also use the more aggressive Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UEqxYGvszVLe"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "5yMJq8eKzVLj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unit',\n",
       " 'biscuit',\n",
       " '(',\n",
       " 'hold',\n",
       " ')',\n",
       " 'plc',\n",
       " 'more',\n",
       " 'than',\n",
       " 'doubl',\n",
       " 'it',\n",
       " 'profit',\n",
       " 'in',\n",
       " '1996',\n",
       " 'to',\n",
       " '109']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed = [stemmer.stem(token.text) for token in example_sentence]\n",
    "stemmed[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOw9KWKbzVLz"
   },
   "source": [
    "**Compare**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zWzVOxHMzVL0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United  |  United  |  unit\n",
      "Biscuits  |  Biscuits  |  biscuit\n",
      "(  |  (  |  (\n",
      "Holdings  |  Holdings  |  hold\n",
      ")  |  )  |  )\n",
      "Plc  |  Plc  |  plc\n",
      "more  |  more  |  more\n",
      "than  |  than  |  than\n",
      "doubled  |  double  |  doubl\n",
      "its  |  its  |  it\n",
      "profits  |  profit  |  profit\n",
      "in  |  in  |  in\n",
      "1996  |  1996  |  1996\n",
      "to  |  to  |  to\n",
      "109  |  109  |  109\n"
     ]
    }
   ],
   "source": [
    "for original, lemma, stem in zip(token_list[:15], lemmatized[:15], stemmed[:15]):\n",
    "    print(original, ' | ', lemma, ' | ', stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WST1UsmzVMA"
   },
   "source": [
    "In my experience it is usually best to use lemmatization instead of a stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "k1n7zXYRzVMC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Language modeling</span><a id='lang_model'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOTi2TTEzVMD"
   },
   "source": [
    "Text is inherently structured in complex ways, we can often use some of this underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "OEiCmMoVzVMH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Part-of-Speech tagging</span><a id='pos_tagging'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJvGsa0DzVMI"
   },
   "source": [
    "Part of speech tagging refers to the identification of words as nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONuqyRPvzVMQ"
   },
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_4B7AUIxzVMR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(United, 'PROPN'),\n",
       " (Biscuits, 'PROPN'),\n",
       " ((, 'PUNCT'),\n",
       " (Holdings, 'PROPN'),\n",
       " (), 'PUNCT'),\n",
       " (Plc, 'PROPN'),\n",
       " (more, 'ADV'),\n",
       " (than, 'ADP'),\n",
       " (doubled, 'VERB'),\n",
       " (its, 'PRON')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = [(token, token.pos_) for token in example_sentence]\n",
    "pos_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "D152zwLUzVMX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Uni-Gram & N-Grams</span><a id='n_grams'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-gWZCBzVMY"
   },
   "source": [
    "Obviously a sentence is not a random collection of words, the sequence of words has information value.  \n",
    "\n",
    "A simple way to incorporate some of this sequence is by using what is called `n-grams`.  \n",
    "An `n-gram` is nothing more than a a combination of `N` words into one token (a uni-gram token is just one word).  \n",
    "\n",
    "So we can convert `\"Sentence about flying cars\"` into a list of bigrams:\n",
    "\n",
    "> Sentence-about, about-flying, flying-cars  \n",
    "\n",
    "See my slide on N-Grams for a more comprehensive example: [click here](http://www.tiesdekok.com/AccountingNLP_Slides/#14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Pc-zTsdzVMZ"
   },
   "source": [
    "Using `NLTK`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "JNY2U2VyzVMa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profits-in', 'in-1996', '1996-to', 'to-109', '109-million']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_list = ['-'.join(x) for x in nltk.bigrams([token.text for token in example_sentence])]\n",
    "bigram_list[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "jgy4iAQpzVMj",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Stop words</span><a id='stop_words'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bif0cWA9zVMl"
   },
   "source": [
    "Depending on what you are trying to do it is possible that there are many words that don't add any information value to the sentence.  \n",
    "\n",
    "The primary example are stop words.  \n",
    "\n",
    "Sometimes you can improve the accuracy of your model by removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9m4Rq7QzVMo"
   },
   "source": [
    "Using `Spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "J7w9yH9hzVMp"
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in example_sentence if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "V1sYIDe-zVMu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, (, Holdings, ), Plc, doubled, profits, 1996, 109]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jpmZjNLXzVM-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, (, Holdings, ), Plc, more, than, doubled, its]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfyZoy-6zVNM"
   },
   "source": [
    "*Note* we can also remove punctuation in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8kd_rGgTzVNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United, Biscuits, Holdings, Plc, more]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in example_sentence if not token.is_punct][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnGJNZJWzVNV"
   },
   "source": [
    "## Wrap everything into one function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN5Ti3nIzVNW"
   },
   "source": [
    "Below I will primarily use `SpaCy` directly. However, I also recommend to check out the high-level wrapper `Textacy`.\n",
    "\n",
    "See their GitHub page for details: https://github.com/chartbeat-labs/textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZKHceWXzVNX"
   },
   "source": [
    "### Quick `Textacy` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ikof63OBzVNY"
   },
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "hwjvM34vzVNe"
   },
   "outputs": [],
   "source": [
    "example_text = text_dict['test']['TimFarrand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YDUV2X1LzVNj"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'textacy' has no attribute 'preprocess_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cleaned_text = \u001b[43mtextacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_text\u001b[49m(example_text, lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m, fix_unicode=\u001b[38;5;28;01mTrue\u001b[39;00m, no_punct=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'textacy' has no attribute 'preprocess_text'"
     ]
    }
   ],
   "source": [
    "cleaned_text = textacy.preprocess_text(example_text, lowercase=True, fix_unicode=True, no_punct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M16-lmobzVNo"
   },
   "source": [
    "**Basic SpaCy text processing function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1R8Q6c5rzVNq"
   },
   "source": [
    "1. Split into sentences\n",
    "2. Apply lemmatizer and remove top words\n",
    "3. Clean up the sentence using `textacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "gk2lMON0zVNs"
   },
   "outputs": [],
   "source": [
    "def process_text_custom(text):\n",
    "    sentences = list(nlp(text).sents)\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        lemmatized_sentences.append([token.lemma_ for token in sentence if not token.is_stop | token.is_punct | token.is_space])\n",
    "    return [nlp(' '.join(sentence)) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0HwX9R_YzVNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 5s, sys: 9.75 s, total: 16min 15s\n",
      "Wall time: 1h 12min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_text_clean = {}\n",
    "for author, text_list in text_dict['test'].items():\n",
    "    lst = []\n",
    "    for text in text_list:\n",
    "        lst.append(process_text_custom(text))\n",
    "    spacy_text_clean[author] = lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OK3ybv1zVN5"
   },
   "source": [
    "Note that there are quite a lot of sentences (~52K) so this takes a bit of time (~ 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5yLy5CSlzVN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 53986\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for author, texts in spacy_text_clean.items():\n",
    "    for text in texts:\n",
    "        count += len(text)\n",
    "print('Number of sentences:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov_YJypWzVOO"
   },
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ABxccKHezVOP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[United Biscuits Holdings Plc double profit 1996 109 million pound $ 174 million tax exceptional item reflect simple slimme portfolio product,\n",
       " United own brand McVities biscuit KP nut exit U.S. Keebler subsidiary say total exceptional charge mainly loss disposal business amount 84.7 million pound 1996 compare 150.3 million 1995,\n",
       " sale rise percent 1.887 billion trading profit grow percent 129.2 million]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_text_clean['TimFarrand'][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Uo89NizNzVOY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Direct feature extraction</span><a id='feature_extract'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJMPsyLwzVOZ"
   },
   "source": [
    "We now have pre-processed our text into something that we can use for direct feature extraction or to convert it to a numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "vefZY_WbzVOa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Feature search</span><a id='feature_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zvB_TQqwzVOb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Entity recognition</span><a id='entity_recognition'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkHbQjBWzVOc"
   },
   "source": [
    "It is often useful / relevant to extract entities that are mentioned in a piece of text.   \n",
    "\n",
    "SpaCy is quite powerful in extracting entities, however, it doesn't work very well on lowercase text.  \n",
    "\n",
    "Given that \"token.lemma\\_\" removes capitalization I will use `spacy_sentences` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-57j90XDzVOd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Underlying profits growth was in line with stock brokers forecasts, but a presentation by management to analysts was greeted positively, sending the group's shares up 11 pence to 248-1/2p by 1415 gmt.\n",
       " "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][0][3]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uUFq1a10zVOp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11 pence, 'MONEY'), (248, 'CARDINAL'), (1415, 'DATE')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "BFlwjICAzVOv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Brewer to leisure group Whitbread Plc has turned in a \"sound\" business performance in the last three months, said chief executive Peter Jarvis in an interview on Friday.\n",
       " "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = spacy_sentences['TimFarrand'][4][0]\n",
    "example_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "BTM4MjgTzVO5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Whitbread Plc, 'ORG'),\n",
       " (the last three months, 'DATE'),\n",
       " (Peter Jarvis, 'PERSON'),\n",
       " (Friday, 'DATE')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, i.label_) for i in nlp(example_sentence.text).ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Phyb4rbpzVPM",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Pattern search</span><a id='pattern_search'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Odw0uoyzVPN"
   },
   "source": [
    "Using the built-in `re` (regular expression) library you can pattern match nearly anything you want.  \n",
    "\n",
    "I will not go into details about regular expressions but see here for a tutorial:  \n",
    "https://regexone.com/references/python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "IZs3HpaAzVPO"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0PW3WB9zVPU"
   },
   "source": [
    "**TIP**: Use [Pythex.org](https://pythex.org/) to try out your regular expression\n",
    "\n",
    "Example on Pythex: <a href=\"https://pythex.org/?regex=IDNUMBER: (\\d\\d\\d-\\w\\w)&test_string=Ties de Kok (IDNUMBER: 123-AZ). Rest of Text.\" target='_blank'>click here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFiKKAZrzVPX"
   },
   "source": [
    "**Example 1:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "V5_ek_NkzVPY"
   },
   "outputs": [],
   "source": [
    "string_1 = 'Ties de Kok (#IDNUMBER: 123-AZ). Rest of text...'\n",
    "string_2 = 'Philip Joos (#IDNUMBER: 663-BY). Rest of text...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ywReAZ9IzVPd"
   },
   "outputs": [],
   "source": [
    "pattern = r'#IDNUMBER: (\\d\\d\\d-\\w\\w)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "PXyhq2CIzVPh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-AZ\n",
      "663-BY\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, string_1)[0])\n",
    "print(re.findall(pattern, string_2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_vzcmowzVQL"
   },
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOjI6qqkzVQM"
   },
   "source": [
    "If a sentence contains the word 'million' return True, otherwise return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "RhtvMohizVQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company hotel exclude Metropole London acquire November 327 million stg $ 543 million Lonrho show occupancy level 71.8 percent 13 week 1997 touch 72 percent time\n",
      "group hotel occupancy pull slightly period temporary closure Stakis Tyneside work underway 3.5 million stg refurbishment\n",
      "turnover casino rise 20 percent 14.2 million stg drive 17 percent increase attendance\n"
     ]
    }
   ],
   "source": [
    "for sen in spacy_text_clean['TimFarrand'][2]:\n",
    "    TERM = 'million'\n",
    "    contains = True if re.search('million', sen.text) else False\n",
    "    if contains:\n",
    "        print(sen)\n",
    "        # print(sen.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "_kAH64jTzVQc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Text evaluation</span><a id='text_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGXV6jb_zVQd"
   },
   "source": [
    "Besides feature search there are also many ways to analyze the text as a whole.  \n",
    "\n",
    "Let's, for example, evaluate the following paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "PQ1waxmozVQf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scottish base Stakis Plc Wednesday report surge visitor casino sharp rise hotel room rate chief executive David Michel confident mood future trend real term room rate late 1980 room rate reach pre recession level province Michels tell Reuters company hotel exclude Metropole London acquire November 327 million stg $ 543 million Lonrho show occupancy level 71.8 percent 13 week 1997 touch 72 percent time average room rate rise 50.10 stg period 45.58 1996 quarter 10 percent think average 7.5 percent'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = ' '.join([x.text for x in spacy_text_clean['TimFarrand'][2]])\n",
    "example_paragraph[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "7CbA_Uj4zVQv",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Language</span><a id='language'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-h5gNZHzVQw"
   },
   "source": [
    "Using the `langdetect` package it is easy to detect the language of a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NbTq0U3_IVA"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "QAEVJFYyzVQx"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "qlqALfz7zVQ0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "frmtGDTPzVRE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Readability</span><a id='readability'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDKalz3yzVRN"
   },
   "source": [
    "Using the `textstat` package we can compute various readability metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWC-8mwTzVRO"
   },
   "source": [
    "https://github.com/shivam5992/textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPo_GBg7C5CM"
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "BVTjORoHzVRO"
   },
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "gc9LPUVHzVRS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.07917624521073\n",
      "16.32212239822248\n",
      "14.909885057471268\n",
      "15.376724137931038\n",
      "17.826837606837607\n",
      "14.103224329501916\n",
      "54\n",
      "13.4\n",
      "17.724904214559388\n",
      "14th and 15th grade\n"
     ]
    }
   ],
   "source": [
    "print(textstat.flesch_reading_ease(example_paragraph))\n",
    "print(textstat.smog_index(example_paragraph))\n",
    "print(textstat.flesch_kincaid_grade(example_paragraph))\n",
    "print(textstat.coleman_liau_index(example_paragraph))\n",
    "print(textstat.automated_readability_index(example_paragraph))\n",
    "print(textstat.dale_chall_readability_score(example_paragraph))\n",
    "print(textstat.difficult_words(example_paragraph))\n",
    "print(textstat.linsear_write_formula(example_paragraph))\n",
    "print(textstat.gunning_fog(example_paragraph))\n",
    "print(textstat.text_standard(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPO_VVHrzVRZ"
   },
   "source": [
    "## Text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07d0tYdGDCn-"
   },
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install fuzzywuzzy\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "m81Jm4m4zVRf"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "k2vblIgozVRi",
    "outputId": "db8a2323-fc05-4840-88d0-ad0d2580e133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "a6c5W0NVzVRn",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Term (dictionary) counting</span><a id='dict_counting'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTNiB1uqzVRo"
   },
   "source": [
    "One of the most common techniques that researchers currently use (at least in Accounting research) are simple metrics based on counting words in a dictionary.  \n",
    "This technique is, for example, very prevalent in sentiment analysis (counting positive and negative words).  \n",
    "\n",
    "In essence this technique is very simple to program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ_Gm7v-zVRp"
   },
   "source": [
    "### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "WboJdEXxzVRs"
   },
   "outputs": [],
   "source": [
    "word_dictionary = ['soft', 'first', 'most', 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "DZFDFOIezVRv",
    "outputId": "c3230d09-7438-475b-a3d5-9879880fae75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft 0\n",
      "first 0\n",
      "most 0\n",
      "be 3\n"
     ]
    }
   ],
   "source": [
    "for word in word_dictionary:\n",
    "    print(word, example_paragraph.count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGv6r4tAzVR9"
   },
   "source": [
    "### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "45YS4EmDzVR-",
    "outputId": "abdb881c-f759-4d0f-f626-a702615d424a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = ['great', 'increase']\n",
    "neg = ['bad', 'decrease']\n",
    "\n",
    "sentence = '''According to Trump everything is great, great,\n",
    "and great even though his popularity is seeing a decrease.'''\n",
    "\n",
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)\n",
    "\n",
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)\n",
    "\n",
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TybOMjAQzVSH"
   },
   "outputs": [],
   "source": [
    "sentence = '''According to Trump everything is great, great,\n",
    "and great even though his popularity is seeing a decrease.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CCBJuvipzVSK",
    "outputId": "9121dd43-79f5-448f-98fe-dae23503b390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "pos_count = 0\n",
    "for word in pos:\n",
    "    pos_count += sentence.lower().count(word)\n",
    "print(pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "by03-OIVzVSR",
    "outputId": "e223d80b-d636-4a9f-c537-d9ab5850cd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "neg_count = 0\n",
    "for word in neg:\n",
    "    neg_count += sentence.lower().count(word)\n",
    "print(neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "EMiSUvOYzVSX",
    "outputId": "652a7451-3cce-4167-d3e2-5a3276a10f41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count / (neg_count + pos_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCmhJIbszVSc"
   },
   "source": [
    "Getting the total number of words is also easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cj1GiPsxzVSd",
    "outputId": "492bbe68-1976-4c9b-f4bf-7c02901c68ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp(example_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NCVqhxKzVTo"
   },
   "source": [
    "#### Example 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYMkGWDtzVTo"
   },
   "source": [
    "We can also save the count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "qV75kU7nzVTq"
   },
   "outputs": [],
   "source": [
    "pos_count_dict = {}\n",
    "for word in pos:\n",
    "    pos_count_dict[word] = sentence.lower().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ctUf44xzVTt",
    "outputId": "5eaca5e7-9ac4-4019-9d5f-6f870a6f7c65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'great': 3, 'increase': 0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "HV_eHMHxzVUu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Represent text numerically</span><a id='text_numerical'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Or4hmJKyzVVC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Bag of Words</span><a id='bows'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_cQOeC2zVVH"
   },
   "source": [
    "Sklearn includes the `CountVectorizer` and `TfidfVectorizer` function.  \n",
    "\n",
    "For details, see the documentation:  \n",
    "[TF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)  \n",
    "[TFIDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)\n",
    "\n",
    "Note 1: These functions also already include a lot of preprocessing options (e.g. ngrams, remove stop words, accent stripper).\n",
    "\n",
    "Note 2: Example based on the following website [click here](http://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Wo5GO5o-zVVY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR6YA_aUzVVe"
   },
   "source": [
    "### Simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "bpy0oD0NzVVf"
   },
   "outputs": [],
   "source": [
    "doc_1 = \"The sky is blue.\"\n",
    "doc_2 = \"The sun is bright today.\"\n",
    "doc_3 = \"The sun in the sky is bright.\"\n",
    "doc_4 = \"We can see the shining sun, the bright sun.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVTX0FTezVVp"
   },
   "source": [
    "Calculate term frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "5wMnrgmlzVVq"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf = vectorizer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "0Lm2LlYNzVVy",
    "outputId": "a274f7b1-4d5f-4c39-dddb-8b6f0d5c0f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue' 'bright' 'shining' 'sky' 'sun' 'today']\n",
      "[1 0 0 1 0 0]\n",
      "[0 1 0 0 1 1]\n",
      "[0 1 0 1 1 0]\n",
      "[0 1 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "for doc_tf_vector in tf.toarray():\n",
    "    print(doc_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GPRbzfvJzVWT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">TF-IDF</span><a id='tfidf'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "LQR-cppDzVWU"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = transformer.fit_transform([doc_1, doc_2, doc_3, doc_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "DP7xLrCKzVWY",
    "outputId": "114f3664-b792-404d-c230-cc3cd4107ba8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78528828 0.         0.         0.6191303  0.         0.        ]\n",
      "[0.         0.47380449 0.         0.         0.47380449 0.74230628]\n",
      "[0.         0.53256952 0.         0.65782931 0.53256952 0.        ]\n",
      "[0.         0.36626037 0.57381765 0.         0.73252075 0.        ]\n"
     ]
    }
   ],
   "source": [
    "for doc_vector in tfidf.toarray():\n",
    "    print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRCeQpwVzVWd"
   },
   "source": [
    "### More elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "CLQYa1iFzVWe"
   },
   "outputs": [],
   "source": [
    "clean_paragraphs = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        clean_paragraphs.append(' '.join([x.text for x in article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XueJvbeZzVWh",
    "outputId": "a408eada-ea06-496a-c7c5-6cdbab7a401b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "99XHAHCFzVWl"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_large = transformer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "W5oOQ889zVWp",
    "outputId": "1d1acfd4-66f1-4f3a-f2ef-3392878b26f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors: 2500\n",
      "Number of words in dictionary: 24036\n"
     ]
    }
   ],
   "source": [
    "print('Number of vectors:', len(tfidf_large.toarray()))\n",
    "print('Number of words in dictionary:', len(tfidf_large.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Akku50qvzVWt",
    "outputId": "b239fb66-e499-4465-d6a3-935a1cd056ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 443919 stored elements and shape (2500, 24036)>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "89OXigz8zVWy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">Word Embeddings</span><a id='word_embed'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "VxwA3sptzVWz",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span style=\"text-decoration: underline;\">Word2Vec</span><a id='Word2Vec'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnpmYO8izVW0"
   },
   "source": [
    "Simple example below is from:  https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "Y8RousiXzVW1",
    "outputId": "a3c6220a-e17d-43a4-d64f-2582eac73515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/koollio/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "nltk.download('brown', download_dir=\"/opt/share/nltk_data\")\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "MPBPtxWqzVW_"
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents()\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXtzmP__zVXE"
   },
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "fP_TJbkwzVXI",
    "outputId": "1f285964-91d3-4ce0-a503-c975d13cbf88"
   },
   "outputs": [],
   "source": [
    "model.save('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI-mzHzkzVXT"
   },
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "G-BUV7FPzVXW",
    "outputId": "fb21fdd6-bd26-4318-fd44-18b63572c0d7"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ81a8jPzVXd"
   },
   "source": [
    "Find words most similar to 'mother':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9790230393409729), ('husband', 0.9679327011108398), ('wife', 0.946841835975647), ('son', 0.9338570237159729), ('friend', 0.9207152128219604), ('nickname', 0.9092258810997009), ('voice', 0.9054335951805115), ('brother', 0.8999156355857849), ('patient', 0.8900373578071594), ('uncle', 0.8772851228713989)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmI5IFgmzVXh"
   },
   "source": [
    "Find the odd one out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "V-mbhBujzVXi",
    "outputId": "30d71cdb-66a8-4829-a592-d47b9ccb4663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "2nq_qc0AzVXx",
    "outputId": "9c69724d-8635-4b1d-8ba6-bac69e3cd5eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garden\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"pizza pasta garden fries\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8NJT85TzVX4"
   },
   "source": [
    "Retrieve vector representation of the word \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "id": "84jajA9wzVX5",
    "outputId": "55fbab38-0c14-4362-f705-0253589551e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.59822446,  0.37523517,  0.7429167 ,  0.18978556, -0.27945676,\n",
       "       -0.63060504,  1.4232993 ,  1.3837723 , -0.5692845 , -0.35793665,\n",
       "       -0.09688178, -0.7069113 ,  0.71324366, -0.4941389 ,  0.09916251,\n",
       "       -0.6406467 ,  0.48585665, -0.26917398, -0.48565945, -1.0296743 ,\n",
       "        0.59314114,  0.04635499,  0.5858246 ,  0.7504386 , -0.36290467,\n",
       "       -0.35890818,  0.21142045,  0.17639156, -0.73307604, -0.10112016,\n",
       "        0.43756485, -0.6939162 ,  0.78580564, -0.3232209 ,  0.31219828,\n",
       "        0.88461053, -0.37566754, -0.47876412, -0.59902835,  0.01704816,\n",
       "       -0.08662194, -0.3377133 ,  0.5304672 ,  0.00568901,  0.6407281 ,\n",
       "        0.14313847, -0.32486907, -0.16631149,  0.07767224,  0.21260379,\n",
       "        1.1854287 , -0.60895926, -0.4149272 , -0.09742101, -0.824041  ,\n",
       "       -0.49826938,  0.9745278 ,  0.14413634, -0.67737544, -0.11342227,\n",
       "        0.06062152,  0.27667984,  0.01661909, -0.66811484, -1.0064873 ,\n",
       "        1.0263972 ,  0.06277847,  0.3018849 , -0.8364451 ,  0.17967385,\n",
       "        0.14203374,  0.1532228 ,  0.7875735 ,  0.22940065,  1.288669  ,\n",
       "        0.36276057,  1.5101249 ,  0.57316905, -0.14563093, -0.6989914 ,\n",
       "       -1.1195713 , -0.0848947 , -0.41708896,  0.29283205, -0.396991  ,\n",
       "        0.05872308,  0.42374784, -0.0970023 ,  0.20464493,  0.15659001,\n",
       "        0.617291  ,  0.12912068,  0.08119339,  0.02159359,  0.45567054,\n",
       "       -0.21329428,  0.1933668 , -1.1398536 , -0.1436757 , -0.31174198],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['human']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "6Eht6I1JzVYG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# <span style=\"text-decoration: underline;\">Statistical models</span><a id='stat_models'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "iQTGfmcHzVYH",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span style=\"text-decoration: underline;\">\"Traditional\" machine learning</span><a id='trad_ml'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paklyzdrzVYI"
   },
   "source": [
    "The library to use for machine learning is scikit-learn ([\"sklearn\"](http://scikit-learn.org/stable/index.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "mY1gvLnozVYI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Supervised</span><a id='trad_ml_supervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Vyvdgww1zVYJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "47LgZmhMzVYS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyNejarAzVYr"
   },
   "source": [
    "### Convert the data into a pandas dataframe (so that we can input it easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "pwRfyuMazVYv"
   },
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for author, value in spacy_text_clean.items():\n",
    "    for article in value:\n",
    "        article_list.append((author, ' '.join([x.text for x in article])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "bjMTWkorzVYz"
   },
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame(article_list, columns=['author', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "Ukjf2aY0zVY2",
    "outputId": "47e30415-9e0d-40d5-ee99-17a54813804c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>SamuelPerry</td>\n",
       "      <td>Intel Corp. world big maker computer chip say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>BernardHickey</td>\n",
       "      <td>New Zealand base investment group Brierley Inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>KeithWeir</td>\n",
       "      <td>anglo dutch publishing group Reed Elsevier Plc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>EdnaFernandes</td>\n",
       "      <td>Northern England glitter record inward investm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2125</th>\n",
       "      <td>DarrenSchuettler</td>\n",
       "      <td>saga Bre X Minerals Ltd. rich indonesian gold ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                               text\n",
       "2483       SamuelPerry  Intel Corp. world big maker computer chip say ...\n",
       "2238     BernardHickey  New Zealand base investment group Brierley Inv...\n",
       "2258         KeithWeir  anglo dutch publishing group Reed Elsevier Plc...\n",
       "1151     EdnaFernandes  Northern England glitter record inward investm...\n",
       "2125  DarrenSchuettler  saga Bre X Minerals Ltd. rich indonesian gold ..."
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aZZcgY0zVY_"
   },
   "source": [
    "### Split the sample into a training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "T8Gr-Y1fzVZA"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(article_df.text, article_df.author, test_size=0.20, random_state=3561)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0_m0XbFXzVZD",
    "outputId": "bd2c77ee-c944-4db4-9d51-9c7c2c803e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VuusccEzVZG"
   },
   "source": [
    "### Train and evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKkWHuvZzVZI"
   },
   "source": [
    "Simple function to train (i.e. fit) and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "ASW0B6sjzVZI"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "uSwaI8sdzVZW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Na√Øve Bayes estimator</span><a id='trad_ml_supervised_nb'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "k3l7pDZpzVZX"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXsEg9LXzVZZ"
   },
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "O29a1MF5zVZZ"
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "\n",
    "    ('clf', MultinomialNB(alpha = 1,\n",
    "                          fit_prior = True\n",
    "                          )\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdfKwR-3zVZc"
   },
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t5R-dmEqzVZq",
    "outputId": "e63b90dd-0c4d-43f6-d0a7-4dab398141b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.848\n",
      "Accuracy on testing set:\n",
      "0.708\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.64      0.88      0.74         8\n",
      "       AlanCrosby       0.50      1.00      0.67         8\n",
      "   AlexanderSmith       0.82      0.82      0.82        11\n",
      "  BenjaminKangLim       0.60      0.27      0.38        11\n",
      "    BernardHickey       0.67      0.60      0.63        10\n",
      "      BradDorfman       0.86      0.67      0.75         9\n",
      " DarrenSchuettler       0.75      0.64      0.69        14\n",
      "      DavidLawder       0.83      0.50      0.62        10\n",
      "    EdnaFernandes       0.43      1.00      0.60         3\n",
      "      EricAuchard       0.71      0.56      0.62         9\n",
      "   FumikoFujisaki       0.93      1.00      0.96        13\n",
      "   GrahamEarnshaw       0.59      0.91      0.71        11\n",
      " HeatherScoffield       0.83      0.50      0.62        10\n",
      "       JanLopatka       0.83      0.50      0.62        10\n",
      "    JaneMacartney       0.40      0.40      0.40        10\n",
      "     JimGilchrist       0.77      1.00      0.87        10\n",
      "   JoWinterbottom       1.00      1.00      1.00         8\n",
      "         JoeOrtiz       1.00      0.70      0.82        10\n",
      "     JohnMastrini       0.75      0.50      0.60         6\n",
      "     JonathanBirt       0.75      0.50      0.60        12\n",
      "      KarlPenhaul       0.93      1.00      0.97        14\n",
      "        KeithWeir       0.70      1.00      0.82         7\n",
      "   KevinDrawbaugh       0.80      0.40      0.53        10\n",
      "    KevinMorrison       0.71      0.71      0.71        17\n",
      "    KirstinRidley       0.60      0.60      0.60        10\n",
      "KouroshKarimkhany       0.89      0.89      0.89         9\n",
      "        LydiaZajc       0.64      1.00      0.78         9\n",
      "   LynneO'Donnell       1.00      0.78      0.88         9\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       1.00      0.67      0.80         9\n",
      "     MarkBendeich       0.83      0.45      0.59        11\n",
      "       MartinWolk       1.00      0.60      0.75        10\n",
      "     MatthewBunce       1.00      0.92      0.96        13\n",
      "    MichaelConnor       0.69      1.00      0.82         9\n",
      "       MureDickie       0.42      0.36      0.38        14\n",
      "        NickLouth       0.67      0.89      0.76         9\n",
      "  PatriciaCommins       0.44      0.88      0.58         8\n",
      "    PeterHumphrey       0.43      0.77      0.56        13\n",
      "       PierreTran       0.88      0.78      0.82         9\n",
      "       RobinSidel       1.00      0.75      0.86        12\n",
      "     RogerFillion       1.00      0.92      0.96        12\n",
      "      SamuelPerry       0.75      0.60      0.67        10\n",
      "     SarahDavison       1.00      0.38      0.55         8\n",
      "      ScottHillis       0.33      0.44      0.38         9\n",
      "      SimonCowell       1.00      0.62      0.76        13\n",
      "         TanEeLyn       0.62      0.56      0.59         9\n",
      "   TheresePoletti       0.75      1.00      0.86         9\n",
      "       TimFarrand       0.47      0.90      0.62        10\n",
      "       ToddNissen       0.40      0.80      0.53         5\n",
      "     WilliamKazer       0.50      0.20      0.29        10\n",
      "\n",
      "         accuracy                           0.71       500\n",
      "        macro avg       0.74      0.72      0.70       500\n",
      "     weighted avg       0.75      0.71      0.70       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ximtzrGXzVaJ"
   },
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gDZnSjVczVaJ",
    "outputId": "9357f7ab-b449-4534-e0e4-d6eecd962104"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_bayes_results.pkl']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'naive_bayes_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWUzMTi0zVaN"
   },
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "nDAtLlu8zVaO"
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "OE1Kcc5HzVaU",
    "outputId": "9bcfe54c-9e47-4f7f-b764-ff0b95b64a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: MarcelMichelson\n",
      "Predicted author: MarcelMichelson\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "zEih7o-RzVaa",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Support Vector Machines (SVM)</span><a id='trad_ml_supervised_svm'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "r3tWA3FizVab"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk2xP8ZdzVal"
   },
   "source": [
    "Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "YtWDmCKFzVal"
   },
   "outputs": [],
   "source": [
    "clf_svm = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english'\n",
    "                            )),\n",
    "\n",
    "    ('clf', SVC(kernel='rbf' ,\n",
    "                C=10, gamma=0.3)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUnKn1VfzVao"
   },
   "source": [
    "*Note:* The SVC estimator is very sensitive to the hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCMbZQU7zVas"
   },
   "source": [
    "Train and show evaluation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NYAILjS5zVat",
    "outputId": "980ffe75-c52e-4d2b-8b93-af020b1046ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.998\n",
      "Accuracy on testing set:\n",
      "0.846\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    AaronPressman       0.80      1.00      0.89         8\n",
      "       AlanCrosby       0.89      1.00      0.94         8\n",
      "   AlexanderSmith       0.85      1.00      0.92        11\n",
      "  BenjaminKangLim       0.64      0.82      0.72        11\n",
      "    BernardHickey       0.80      0.80      0.80        10\n",
      "      BradDorfman       0.80      0.89      0.84         9\n",
      " DarrenSchuettler       1.00      0.93      0.96        14\n",
      "      DavidLawder       0.89      0.80      0.84        10\n",
      "    EdnaFernandes       0.60      1.00      0.75         3\n",
      "      EricAuchard       0.58      0.78      0.67         9\n",
      "   FumikoFujisaki       1.00      1.00      1.00        13\n",
      "   GrahamEarnshaw       0.83      0.91      0.87        11\n",
      " HeatherScoffield       1.00      1.00      1.00        10\n",
      "       JanLopatka       1.00      0.70      0.82        10\n",
      "    JaneMacartney       0.50      0.40      0.44        10\n",
      "     JimGilchrist       0.91      1.00      0.95        10\n",
      "   JoWinterbottom       1.00      1.00      1.00         8\n",
      "         JoeOrtiz       1.00      0.90      0.95        10\n",
      "     JohnMastrini       0.75      1.00      0.86         6\n",
      "     JonathanBirt       0.91      0.83      0.87        12\n",
      "      KarlPenhaul       1.00      1.00      1.00        14\n",
      "        KeithWeir       0.88      1.00      0.93         7\n",
      "   KevinDrawbaugh       0.73      0.80      0.76        10\n",
      "    KevinMorrison       0.88      0.82      0.85        17\n",
      "    KirstinRidley       1.00      0.50      0.67        10\n",
      "KouroshKarimkhany       1.00      0.89      0.94         9\n",
      "        LydiaZajc       0.90      1.00      0.95         9\n",
      "   LynneO'Donnell       1.00      0.78      0.88         9\n",
      "  LynnleyBrowning       0.91      1.00      0.95        10\n",
      "  MarcelMichelson       0.88      0.78      0.82         9\n",
      "     MarkBendeich       1.00      0.91      0.95        11\n",
      "       MartinWolk       0.88      0.70      0.78        10\n",
      "     MatthewBunce       1.00      1.00      1.00        13\n",
      "    MichaelConnor       0.90      1.00      0.95         9\n",
      "       MureDickie       0.62      0.57      0.59        14\n",
      "        NickLouth       0.89      0.89      0.89         9\n",
      "  PatriciaCommins       0.89      1.00      0.94         8\n",
      "    PeterHumphrey       0.67      0.77      0.71        13\n",
      "       PierreTran       0.78      0.78      0.78         9\n",
      "       RobinSidel       1.00      0.92      0.96        12\n",
      "     RogerFillion       0.92      1.00      0.96        12\n",
      "      SamuelPerry       0.75      0.60      0.67        10\n",
      "     SarahDavison       1.00      0.50      0.67         8\n",
      "      ScottHillis       0.55      0.67      0.60         9\n",
      "      SimonCowell       1.00      1.00      1.00        13\n",
      "         TanEeLyn       0.70      0.78      0.74         9\n",
      "   TheresePoletti       0.90      1.00      0.95         9\n",
      "       TimFarrand       0.89      0.80      0.84        10\n",
      "       ToddNissen       0.71      1.00      0.83         5\n",
      "     WilliamKazer       0.38      0.30      0.33        10\n",
      "\n",
      "         accuracy                           0.85       500\n",
      "        macro avg       0.85      0.85      0.84       500\n",
      "     weighted avg       0.85      0.85      0.84       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX6wlU59zVaw"
   },
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JJQSupEhzVax",
    "outputId": "92ef1d98-f274-4766-a775-e6a9c0594572"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_results.pkl']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, 'svm_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWGWAhmSzVa6"
   },
   "source": [
    "Predict out of sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "b17BsOqOzVa7"
   },
   "outputs": [],
   "source": [
    "example_y, example_X = y_train[33], X_train[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "hn2k9wDHzVa-",
    "outputId": "26437c3b-869d-4787-c177-666bde1c8b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual author: MarcelMichelson\n",
      "Predicted author: MarcelMichelson\n"
     ]
    }
   ],
   "source": [
    "print('Actual author:', example_y)\n",
    "print('Predicted author:', clf_svm.predict([example_X])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "vpRZqHSNzVbB",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Model Selection and Evaluation</span><a id='trad_ml_eval'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQ52YFLzVbC"
   },
   "source": [
    "Both the `TfidfVectorizer` and `SVC()` estimator take a lot of hyperparameters.  \n",
    "\n",
    "It can be difficult to figure out what the best parameters are.\n",
    "\n",
    "We can use `GridSearchCV` to help figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "p4wtBtFEzVbC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wJgG3nczVbH"
   },
   "source": [
    "First we define the options that should be tried out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "pBSoERHDzVbJ"
   },
   "outputs": [],
   "source": [
    "clf_search = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "parameters = {'vect__stop_words': ['english'],\n",
    "              'vect__strip_accents': ['unicode'],\n",
    "              'vect__max_features' : [1500],\n",
    "              'vect__ngram_range': [(1,1), (2,2) ],\n",
    "              'clf__gamma' : [0.2, 0.3, 0.4],\n",
    "              'clf__C' : [8, 10, 12],\n",
    "              'clf__kernel' : ['rbf']\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tp-NLigBzVbL"
   },
   "source": [
    "Run everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "F1VQ-VGjzVbM",
    "outputId": "9a509d22-165c-42c1-8d05-ecff30cd59b5"
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf_search, param_grid=parameters, cv=6, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt2CvCxVzVbQ"
   },
   "source": [
    "*Note:* if you are on a powerful unix system you can set n_jobs to the number of available threads to speed up the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZg7EZTBzVbQ"
   },
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "y_true, y_pred = y_test, grid.predict(X_test)\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "NvIi77ZczVbW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <span>Unsupervised</span><a id='trad_ml_unsupervised'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "yGyPcY1JzVbh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>Latent Dirichilet Allocation (LDA)</span><a id='trad_ml_unsupervised_lda'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "WeVuhs6ZzVbh"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEfe9WZ-zVbp"
   },
   "source": [
    "Vectorizer (using countvectorizer for the sake of example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "Cx4JnRCXzVbp"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                             lowercase = True,\n",
    "                            max_features = 1500,\n",
    "                            stop_words='english', max_df=0.8)\n",
    "tf_large = vectorizer.fit_transform(clean_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC05UAkVzVbs"
   },
   "source": [
    "Run the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "crHxv3OKzVbs"
   },
   "outputs": [],
   "source": [
    "n_topics = 10\n",
    "n_top_words = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "0ECCbQnczVbx"
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=10,\n",
    "                                learning_method='online',\n",
    "                                n_jobs=1)\n",
    "lda_fitted = lda.fit_transform(tf_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WWs2PfDzVb0"
   },
   "source": [
    "Visualize top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "SHgccEPWzVb2"
   },
   "outputs": [],
   "source": [
    "def save_top_words(model, feature_names, n_top_words):\n",
    "    out_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        out_list.append((topic_idx+1, \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])))\n",
    "    out_df = pd.DataFrame(out_list, columns=['topic_id', 'top_words'])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "oUApo9RHzVb_"
   },
   "outputs": [],
   "source": [
    "result_df = save_top_words(lda, vectorizer.get_feature_names_out(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "KXjV_kHYzVcC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>quarter analyst percent share million sale com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>government minister official union state plan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>service company new amp corp network customer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>percent million profit bank billion analyst ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>company internet computer microsoft software t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>000 crop air tobacco japanese trade industry j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>market tonne percent price export 000 trader o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>china kong hong chinese beijing people deng of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>bre gold stock share toronto busang canada con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>company bank share market financial business m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_id                                          top_words\n",
       "0         1  quarter analyst percent share million sale com...\n",
       "1         2  government minister official union state plan ...\n",
       "2         3  service company new amp corp network customer ...\n",
       "3         4  percent million profit bank billion analyst ma...\n",
       "4         5  company internet computer microsoft software t...\n",
       "5         6  000 crop air tobacco japanese trade industry j...\n",
       "6         7  market tonne percent price export 000 trader o...\n",
       "7         8  china kong hong chinese beijing people deng of...\n",
       "8         9  bre gold stock share toronto busang canada con...\n",
       "9        10  company bank share market financial business m..."
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "rtrOd5e2zVci",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### <span>pyLDAvis</span><a id='trad_ml_unsupervised_pyLDAvis'></a> [(to top)](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS_91lazzVck"
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import pyLDAvis.lda_model\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rxaaZ9fzVco"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.lda_model.prepare(lda, tf_large, vectorizer, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWw0uej5wo-"
   },
   "source": [
    "\n",
    "Credit: [Ties de Kok](https://github.com/TiesdeKok)\n",
    "\n",
    "Repository: [Python NLP](https://github.com/TiesdeKok/Python_NLP_Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyoPQUDx5ylY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8NCVqhxKzVTo"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
